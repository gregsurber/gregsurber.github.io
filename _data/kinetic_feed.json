[
  {
    "id": "http://arxiv.org/abs/2601.06022v1",
    "title": "AdaFuse: Adaptive Ensemble Decoding with Test-Time Scaling for LLMs",
    "authors": [
      "Chengming Cui",
      "Tianxin Wei",
      "Ziyi Chen",
      "Ruizhong Qiu",
      "Zhichen Zeng",
      "Zhining Liu",
      "Xuying Ning",
      "Duo Zhou",
      "Jingrui He"
    ],
    "summary": "Large language models (LLMs) exhibit complementary strengths arising from differences in pretraining data, model architectures, and decoding behaviors. Inference-time ensembling provides a practical way to combine these capabilities without retraining. However, existing ensemble approaches suffer from fundamental limitations. Most rely on fixed fusion granularity, which lacks the flexibility required for mid-generation adaptation and fails to adapt to different generation characteristics across tasks. To address these challenges, we propose AdaFuse, an adaptive ensemble decoding framework that dynamically selects semantically appropriate fusion units during generation. Rather than committing to a fixed granularity, AdaFuse adjusts fusion behavior on the fly based on the decoding context, with words serving as basic building blocks for alignment. To be specific, we introduce an uncertainty-based criterion to decide whether to apply ensembling at each decoding step. Under confident decoding states, the model continues generation directly. In less certain states, AdaFuse invokes a diversity-aware scaling strategy to explore alternative candidate continuations and inform ensemble decisions. This design establishes a synergistic interaction between adaptive ensembling and test-time scaling, where ensemble decisions guide targeted exploration, and the resulting diversity in turn strengthens ensemble quality. Experiments on open-domain question answering, arithmetic reasoning, and machine translation demonstrate that AdaFuse consistently outperforms strong ensemble baselines, achieving an average relative improvement of 6.88%. The code is available at https://github.com/CCM0111/AdaFuse.",
    "link": "http://arxiv.org/abs/2601.06022v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.06021v1",
    "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
    "authors": [
      "Jiajie Zhang",
      "Xin Lv",
      "Ling Feng",
      "Lei Hou",
      "Juanzi Li"
    ],
    "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose \\textbf{Citation-aware Rubric Rewards (CaRR)}, a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce \\textbf{Citation-aware Group Relative Policy Optimization (C-GRPO)}, which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
    "link": "http://arxiv.org/abs/2601.06021v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.06007v1",
    "title": "Don't Break the Cache: An Evaluation of Prompt Caching for Long-Horizon Agentic Tasks",
    "authors": [
      "Elias Lumer",
      "Faheem Nizar",
      "Akshaya Jangiti",
      "Kevin Frank",
      "Anmol Gulati",
      "Mandar Phadate",
      "Vamse Kumar Subbiah"
    ],
    "summary": "Recent advancements in Large Language Model (LLM) agents have enabled complex multi-turn agentic tasks requiring extensive tool calling, where conversations can span dozens of API calls with increasingly large context windows. However, although major LLM providers offer prompt caching to reduce cost and latency, its benefits for agentic workloads remain underexplored in the research literature. To our knowledge, no prior work quantifies these cost savings or compares caching strategies for multi-turn agentic tasks. We present a comprehensive evaluation of prompt caching across three major LLM providers (OpenAI, Anthropic, and Google) and compare three caching strategies, including full context caching, system prompt only caching, and caching that excludes dynamic tool results. We evaluate on DeepResearchBench, a multi-turn agentic benchmark where agents autonomously execute real-world web search tool calls to answer complex research questions, measuring both API cost and time to first token (TTFT) across over 500 agent sessions with 10,000-token system prompts. Our results demonstrate that prompt caching reduces API costs by 45-80% and improves time to first token by 13-31% across providers. We find that strategic prompt cache block control, such as placing dynamic content at the end of the system prompt, avoiding dynamic traditional function calling, and excluding dynamic tool results, provides more consistent benefits than naive full-context caching, which can paradoxically increase latency. Our analysis reveals nuanced variations in caching behavior across providers, and we provide practical guidance for implementing prompt caching in production agentic systems.",
    "link": "http://arxiv.org/abs/2601.06007v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.06002v1",
    "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
    "authors": [
      "Qiguang Chen",
      "Yantao Du",
      "Ziniu Li",
      "Jinhao Liu",
      "Songyao Duan",
      "Jiarui Guo",
      "Minghao Liu",
      "Jiaheng Liu",
      "Tong Yang",
      "Ge Zhang",
      "Libo Qin",
      "Wanxiang Che",
      "Wenhao Huang"
    ],
    "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
    "link": "http://arxiv.org/abs/2601.06002v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05991v1",
    "title": "Open-Vocabulary 3D Instruction Ambiguity Detection",
    "authors": [
      "Jiayu Ding",
      "Haoran Tang",
      "Ge Li"
    ],
    "summary": "In safety-critical domains, linguistic ambiguity can have severe consequences; a vague command like \"Pass me the vial\" in a surgical setting could lead to catastrophic errors. Yet, most embodied AI research overlooks this, assuming instructions are clear and focusing on execution rather than confirmation. To address this critical safety gap, we are the first to define Open-Vocabulary 3D Instruction Ambiguity Detection, a fundamental new task where a model must determine if a command has a single, unambiguous meaning within a given 3D scene. To support this research, we build Ambi3D, the large-scale benchmark for this task, featuring over 700 diverse 3D scenes and around 22k instructions. Our analysis reveals a surprising limitation: state-of-the-art 3D Large Language Models (LLMs) struggle to reliably determine if an instruction is ambiguous. To address this challenge, we propose AmbiVer, a two-stage framework that collects explicit visual evidence from multiple views and uses it to guide an vision-language model (VLM) in judging instruction ambiguity. Extensive experiments demonstrate the challenge of our task and the effectiveness of AmbiVer, paving the way for safer and more trustworthy embodied AI. Code and dataset available at https://jiayuding031020.github.io/ambi3d/.",
    "link": "http://arxiv.org/abs/2601.05991v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05960v1",
    "title": "Distilling Feedback into Memory-as-a-Tool",
    "authors": [
      "V\u00edctor Gallego"
    ],
    "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.",
    "link": "http://arxiv.org/abs/2601.05960v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05943v1",
    "title": "Global Optimization for Combinatorial Geometry Problems",
    "authors": [
      "Timo Berthold",
      "Dominik Kamp",
      "Gioni Mexi",
      "Sebastian Pokutta",
      "Imre P\u00f3lik"
    ],
    "summary": "Recent progress in LLM-driven algorithm discovery, exemplified by DeepMind's AlphaEvolve, has produced new best-known solutions for a range of hard geometric and combinatorial problems. This raises a natural question: to what extent can modern off-the-shelf global optimization solvers match such results when the problems are formulated directly as nonlinear optimization problems (NLPs)?   We revisit a subset of problems from the AlphaEvolve benchmark suite and evaluate straightforward NLP formulations with two state-of-the-art solvers, the commercial FICO Xpress and the open-source SCIP. Without any solver modifications, both solvers reproduce, and in several cases improve upon, the best solutions previously reported in the literature, including the recent LLM-driven discoveries. Our results not only highlight the maturity of generic NLP technology and its ability to tackle nonlinear mathematical problems that were out of reach for general-purpose solvers only a decade ago, but also position global NLP solvers as powerful tools that may be exploited within LLM-driven algorithm discovery.",
    "link": "http://arxiv.org/abs/2601.05943v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05930v1",
    "title": "Can We Predict Before Executing Machine Learning Agents?",
    "authors": [
      "Jingsheng Zheng",
      "Jintian Zhang",
      "Yujie Luo",
      "Yuren Mao",
      "Yunjun Gao",
      "Lun Du",
      "Huajun Chen",
      "Ningyu Zhang"
    ],
    "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
    "link": "http://arxiv.org/abs/2601.05930v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05918v1",
    "title": "Agentic LLMs as Powerful Deanonymizers: Re-identification of Participants in the Anthropic Interviewer Dataset",
    "authors": [
      "Tianshi Li"
    ],
    "summary": "On December 4, 2025, Anthropic released Anthropic Interviewer, an AI tool for running qualitative interviews at scale, along with a public dataset of 1,250 interviews with professionals, including 125 scientists, about their use of AI for research. Focusing on the scientist subset, I show that widely available LLMs with web search and agentic capabilities can link six out of twenty-four interviews to specific scientific works, recovering associated authors and, in some cases, uniquely identifying the interviewees. My contribution is to show that modern LLM-based agents make such re-identification attacks easy and low-effort: off-the-shelf tools can, with a few natural-language prompts, search the web, cross-reference details, and propose likely matches, effectively lowering the technical barrier. Existing safeguards can be bypassed by breaking down the re-identification into benign tasks. I outline the attack at a high level, discuss implications for releasing rich qualitative data in the age of LLM agents, and propose mitigation recommendations and open problems. I have notified Anthropic of my findings.",
    "link": "http://arxiv.org/abs/2601.05918v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05905v1",
    "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
    "authors": [
      "Haoming Xu",
      "Ningyuan Zhao",
      "Yunzhi Yao",
      "Weihong Xu",
      "Hongru Wang",
      "Xinle Deng",
      "Shumin Deng",
      "Jeff Z. Pan",
      "Huajun Chen",
      "Ningyu Zhang"
    ],
    "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
    "link": "http://arxiv.org/abs/2601.05905v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05904v1",
    "title": "Can AI mediation improve democratic deliberation?",
    "authors": [
      "Michael Henry Tessler",
      "Georgina Evans",
      "Michiel A. Bakker",
      "Iason Gabriel",
      "Sophie Bridgers",
      "Rishub Jain",
      "Raphael Koster",
      "Verena Rieser",
      "Anca Dragan",
      "Matthew Botvinick",
      "Christopher Summerfield"
    ],
    "summary": "The strength of democracy lies in the free and equal exchange of diverse viewpoints. Living up to this ideal at scale faces inherent tensions: broad participation, meaningful deliberation, and political equality often trade off with one another (Fishkin, 2011). We ask whether and how artificial intelligence (AI) could help navigate this \"trilemma\" by engaging with a recent example of a large language model (LLM)-based system designed to help people with diverse viewpoints find common ground (Tessler, Bakker, et al., 2024). Here, we explore the implications of the introduction of LLMs into deliberation augmentation tools, examining their potential to enhance participation through scalability, improve political equality via fair mediation, and foster meaningful deliberation by, for example, surfacing trustworthy information. We also point to key challenges that remain. Ultimately, a range of empirical, technical, and theoretical advancements are needed to fully realize the promise of AI-mediated deliberation for enhancing citizen engagement and strengthening democratic deliberation.",
    "link": "http://arxiv.org/abs/2601.05904v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05903v1",
    "title": "HAPS: Hierarchical LLM Routing with Joint Architecture and Parameter Search",
    "authors": [
      "Zihang Tian",
      "Rui Li",
      "Jingsen Zhang",
      "Xiaohe Bo",
      "Wei Huo",
      "Xu Chen"
    ],
    "summary": "Large language model (LLM) routing aims to exploit the specialized strengths of different LLMs for diverse tasks. However, existing approaches typically focus on selecting LLM architectures while overlooking parameter settings, which are critical for task performance. In this paper, we introduce HAPS, a hierarchical LLM routing framework that jointly searches over model architectures and parameters. Specifically, we use a high-level router to select among candidate LLM architectures, and then search for the optimal parameters for the selected architectures based on a low-level router. We design a parameter generation network to share parameters between the two routers to mutually enhance their capabilities. In the training process, we design a reward-augmented objective to effectively optimize our framework. Experiments on two commonly used benchmarks show that HAPS consistently outperforms strong routing baselines. We have released our code at https://github.com/zihangtian/HAPS.",
    "link": "http://arxiv.org/abs/2601.05903v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05899v1",
    "title": "TowerMind: A Tower Defence Game Learning Environment and Benchmark for LLM as Agents",
    "authors": [
      "Dawei Wang",
      "Chengming Zhou",
      "Di Zhao",
      "Xinyuan Liu",
      "Marci Chi Ma",
      "Gary Ushaw",
      "Richard Davison"
    ],
    "summary": "Recent breakthroughs in Large Language Models (LLMs) have positioned them as a promising paradigm for agents, with long-term planning and decision-making emerging as core general-purpose capabilities for adapting to diverse scenarios and tasks. Real-time strategy (RTS) games serve as an ideal testbed for evaluating these two capabilities, as their inherent gameplay requires both macro-level strategic planning and micro-level tactical adaptation and action execution. Existing RTS game-based environments either suffer from relatively high computational demands or lack support for textual observations, which has constrained the use of RTS games for LLM evaluation. Motivated by this, we present TowerMind, a novel environment grounded in the tower defense (TD) subgenre of RTS games. TowerMind preserves the key evaluation strengths of RTS games for assessing LLMs, while featuring low computational demands and a multimodal observation space, including pixel-based, textual, and structured game-state representations. In addition, TowerMind supports the evaluation of model hallucination and provides a high degree of customizability. We design five benchmark levels to evaluate several widely used LLMs under different multimodal input settings. The results reveal a clear performance gap between LLMs and human experts across both capability and hallucination dimensions. The experiments further highlight key limitations in LLM behavior, such as inadequate planning validation, a lack of multifinality in decision-making, and inefficient action use. We also evaluate two classic reinforcement learning algorithms: Ape-X DQN and PPO. By offering a lightweight and multimodal design, TowerMind complements the existing RTS game-based environment landscape and introduces a new benchmark for the AI agent field. The source code is publicly available on GitHub(https://github.com/tb6147877/TowerMind).",
    "link": "http://arxiv.org/abs/2601.05899v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05887v1",
    "title": "Cybersecurity AI: A Game-Theoretic AI for Guiding Attack and Defense",
    "authors": [
      "V\u00edctor Mayoral-Vilches",
      "Mar\u00eda Sanz-G\u00f3mez",
      "Francesco Balassone",
      "Stefan Rass",
      "Lidia Salas-Espejo",
      "Benjamin Jablonski",
      "Luis Javier Navarrete-Lozano",
      "Maite del Mundo de Torres",
      "Crist\u00f3bal R. J. Veas Chavez"
    ],
    "summary": "AI-driven penetration testing now executes thousands of actions per hour but still lacks the strategic intuition humans apply in competitive security. To build cybersecurity superintelligence --Cybersecurity AI exceeding best human capability-such strategic intuition must be embedded into agentic reasoning processes. We present Generative Cut-the-Rope (G-CTR), a game-theoretic guidance layer that extracts attack graphs from agent's context, computes Nash equilibria with effort-aware scoring, and feeds a concise digest back into the LLM loop \\emph{guiding} the agent's actions. Across five real-world exercises, G-CTR matches 70--90% of expert graph structure while running 60--245x faster and over 140x cheaper than manual analysis. In a 44-run cyber-range, adding the digest lifts success from 20.0% to 42.9%, cuts cost-per-success by 2.7x, and reduces behavioral variance by 5.2x. In Attack-and-Defense exercises, a shared digest produces the Purple agent, winning roughly 2:1 over the LLM-only baseline and 3.7:1 over independently guided teams. This closed-loop guidance is what produces the breakthrough: it reduces ambiguity, collapses the LLM's search space, suppresses hallucinations, and keeps the model anchored to the most relevant parts of the problem, yielding large gains in success rate, consistency, and reliability.",
    "link": "http://arxiv.org/abs/2601.05887v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05879v1",
    "title": "Gender Bias in LLMs: Preliminary Evidence from Shared Parenting Scenario in Czech Family Law",
    "authors": [
      "Jakub Harasta",
      "Matej Vasina",
      "Martin Kornel",
      "Tomas Foltynek"
    ],
    "summary": "Access to justice remains limited for many people, leading laypersons to increasingly rely on Large Language Models (LLMs) for legal self-help. Laypeople use these tools intuitively, which may lead them to form expectations based on incomplete, incorrect, or biased outputs. This study examines whether leading LLMs exhibit gender bias in their responses to a realistic family law scenario. We present an expert-designed divorce scenario grounded in Czech family law and evaluate four state-of-the-art LLMs GPT-5 nano, Claude Haiku 4.5, Gemini 2.5 Flash, and Llama 3.3 in a fully zero-shot interaction. We deploy two versions of the scenario, one with gendered names and one with neutral labels, to establish a baseline for comparison. We further introduce nine legally relevant factors that vary the factual circumstances of the case and test whether these variations influence the models' proposed shared-parenting ratios. Our preliminary results highlight differences across models and suggest gender-dependent patterns in the outcomes generated by some systems. The findings underscore both the risks associated with laypeople's reliance on LLMs for legal guidance and the need for more robust evaluation of model behavior in sensitive legal contexts. We present exploratory and descriptive evidence intended to identify systematic asymmetries rather than to establish causal effects.",
    "link": "http://arxiv.org/abs/2601.05879v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05874v1",
    "title": "Continual-learning for Modelling Low-Resource Languages from Large Language Models",
    "authors": [
      "Santosh Srinath K",
      "Mudit Somani",
      "Varun Reddy Padala",
      "Prajna Devi Upadhyay",
      "Abhijit Das"
    ],
    "summary": "Modelling a language model for a multi-lingual scenario includes several potential challenges, among which catastrophic forgetting is the major challenge. For example, small language models (SLM) built for low-resource languages by adapting large language models (LLMs) pose the challenge of catastrophic forgetting. This work proposes to employ a continual learning strategy using parts-of-speech (POS)-based code-switching along with a replay adapter strategy to mitigate the identified gap of catastrophic forgetting while training SLM from LLM. Experiments conducted on vision language tasks such as visual question answering and language modelling task exhibits the success of the proposed architecture.",
    "link": "http://arxiv.org/abs/2601.05874v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05870v1",
    "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
    "authors": [
      "Huilin Deng",
      "Hongchen Luo",
      "Yue Zhu",
      "Long Li",
      "Zhuoyue Chen",
      "Xinghao Zhao",
      "Ming Li",
      "Jihai Zhang",
      "Mengchang Wang",
      "Yang Cao",
      "Yu Kang"
    ],
    "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
    "link": "http://arxiv.org/abs/2601.05870v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05858v1",
    "title": "CLewR: Curriculum Learning with Restarts for Machine Translation Preference Learning",
    "authors": [
      "Alexandra Dragomir",
      "Florin Brad",
      "Radu Tudor Ionescu"
    ],
    "summary": "Large language models (LLMs) have demonstrated competitive performance in zero-shot multilingual machine translation (MT). Some follow-up works further improved MT performance via preference optimization, but they leave a key aspect largely underexplored: the order in which data samples are given during training. We address this topic by integrating curriculum learning into various state-of-the-art preference optimization algorithms to boost MT performance. We introduce a novel curriculum learning strategy with restarts (CLewR), which reiterates easy-to-hard curriculum multiple times during training to effectively mitigate the catastrophic forgetting of easy examples. We demonstrate consistent gains across several model families (Gemma2, Qwen2.5, Llama3.1) and preference optimization techniques. We publicly release our code at https://github.com/alexandra-dragomir/CLewR.",
    "link": "http://arxiv.org/abs/2601.05858v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05835v1",
    "title": "Left, Right, or Center? Evaluating LLM Framing in News Classification and Generation",
    "authors": [
      "Molly Kennedy",
      "Ali Parker",
      "Yihong Liu",
      "Hinrich Sch\u00fctze"
    ],
    "summary": "Large Language Model (LLM) based summarization and text generation are increasingly used for producing and rewriting text, raising concerns about political framing in journalism where subtle wording choices can shape interpretation. Across nine state-of-the-art LLMs, we study political framing by testing whether LLMs' classification-based bias signals align with framing behavior in their generated summaries. We first compare few-shot ideology predictions against LEFT/CENTER/RIGHT labels. We then generate \"steered\" summaries under FAITHFUL, CENTRIST, LEFT, and RIGHT prompts, and score all outputs using a single fixed ideology evaluator. We find pervasive ideological center-collapse in both article-level ratings and generated text, indicating a systematic tendency toward centrist framing. Among evaluated models, Grok 4 is by far the most ideologically expressive generator, while Claude Sonnet 4.5 and Llama 3.1 achieve the strongest bias-rating performance among commercial and open-weight models, respectively.",
    "link": "http://arxiv.org/abs/2601.05835v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05827v1",
    "title": "SSR: Safeguarding Staking Rewards by Defining and Detecting Logical Defects in DeFi Staking",
    "authors": [
      "Zewei Lin",
      "Jiachi Chen",
      "Jingwen Zhang",
      "Zexu Wang",
      "Yuming Feng",
      "Weizhe Zhang",
      "Zibin Zheng"
    ],
    "summary": "Decentralized Finance (DeFi) staking is one of the most prominent applications within the DeFi ecosystem, where DeFi projects enable users to stake tokens on the platform and reward participants with additional tokens. However, logical defects in DeFi staking could enable attackers to claim unwarranted rewards by manipulating reward amounts, repeatedly claiming rewards, or engaging in other malicious actions. To mitigate these threats, we conducted the first study focused on defining and detecting logical defects in DeFi staking. Through the analysis of 64 security incidents and 144 audit reports, we identified six distinct types of logical defects, each accompanied by detailed descriptions and code examples. Building on this empirical research, we developed SSR (Safeguarding Staking Reward), a static analysis tool designed to detect logical defects in DeFi staking contracts. SSR utilizes a large language model (LLM) to extract fundamental information about staking logic and constructs a DeFi staking model. It then identifies logical defects by analyzing the model and the associated semantic features. We constructed a ground truth dataset based on known security incidents and audit reports to evaluate the effectiveness of SSR. The results indicate that SSR achieves an overall precision of 92.31%, a recall of 87.92%, and an F1-score of 88.85%. Additionally, to assess the prevalence of logical defects in real-world smart contracts, we compiled a large-scale dataset of 15,992 DeFi staking contracts. SSR detected that 3,557 (22.24%) of these contracts contained at least one logical defect.",
    "link": "http://arxiv.org/abs/2601.05827v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05215v1",
    "title": "MineNPC-Task: Task Suite for Memory-Aware Minecraft Agents",
    "authors": [
      "Tamil Sudaravan Mohan Doss",
      "Michael Xu",
      "Sudha Rao",
      "Andrew D. Wilson",
      "Balasaravanan Thoravi Kumaravel"
    ],
    "summary": "We present \\textsc{MineNPC-Task}, a user-authored benchmark and evaluation harness for testing memory-aware, mixed-initiative LLM agents in open-world \\emph{Minecraft}. Rather than relying on synthetic prompts, tasks are elicited from formative and summative co-play with expert players, normalized into parametric templates with explicit preconditions and dependency structure, and paired with machine-checkable validators under a bounded-knowledge policy that forbids out-of-world shortcuts. The harness captures plan/act/memory events-including plan previews, targeted clarifications, memory reads and writes, precondition checks, and repair attempts and reports outcomes relative to the total number of attempted subtasks, derived from in-world evidence.   As an initial snapshot, we instantiate the framework with GPT-4o and evaluate \\textbf{216} subtasks across \\textbf{8} experienced players. We observe recurring breakdown patterns in code execution, inventory/tool handling, referencing, and navigation, alongside recoveries supported by mixed-initiative clarifications and lightweight memory. Participants rated interaction quality and interface usability positively, while highlighting the need for stronger memory persistence across tasks. We release the complete task suite, validators, logs, and harness to support transparent, reproducible evaluation of future memory-aware embodied agents.",
    "link": "http://arxiv.org/abs/2601.05215v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05214v1",
    "title": "Internal Representations as Indicators of Hallucinations in Agent Tool Selection",
    "authors": [
      "Kait Healy",
      "Bharathi Srinivasan",
      "Visakh Madathil",
      "Jing Wu"
    ],
    "summary": "Large Language Models (LLMs) have shown remarkable capabilities in tool calling and tool usage, but suffer from hallucinations where they choose incorrect tools, provide malformed parameters and exhibit 'tool bypass' behavior by performing simulations and generating outputs instead of invoking specialized tools or external systems. This undermines the reliability of LLM based agents in production systems as it leads to inconsistent results, and bypasses security and audit controls. Such hallucinations in agent tool selection require early detection and error handling. Unlike existing hallucination detection methods that require multiple forward passes or external validation, we present a computationally efficient framework that detects tool-calling hallucinations in real-time by leveraging LLMs' internal representations during the same forward pass used for generation. We evaluate this approach on reasoning tasks across multiple domains, demonstrating strong detection performance (up to 86.4\\% accuracy) while maintaining real-time inference capabilities with minimal computational overhead, particularly excelling at detecting parameter-level hallucinations and inappropriate tool selections, critical for reliable agent deployment.",
    "link": "http://arxiv.org/abs/2601.05214v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05192v1",
    "title": "LELA: an LLM-based Entity Linking Approach with Zero-Shot Domain Adaptation",
    "authors": [
      "Samy Haffoudhi",
      "Fabian M. Suchanek",
      "Nils Holzenberger"
    ],
    "summary": "Entity linking (mapping ambiguous mentions in text to entities in a knowledge base) is a foundational step in tasks such as knowledge graph construction, question-answering, and information extraction. Our method, LELA, is a modular coarse-to-fine approach that leverages the capabilities of large language models (LLMs), and works with different target domains, knowledge bases and LLMs, without any fine-tuning phase. Our experiments across various entity linking settings show that LELA is highly competitive with fine-tuned approaches, and substantially outperforms the non-fine-tuned ones.",
    "link": "http://arxiv.org/abs/2601.05192v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05187v1",
    "title": "SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning",
    "authors": [
      "Yanchang Liang",
      "Xiaowei Zhao"
    ],
    "summary": "Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.",
    "link": "http://arxiv.org/abs/2601.05187v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05184v1",
    "title": "Observations and Remedies for Large Language Model Bias in Self-Consuming Performative Loop",
    "authors": [
      "Yaxuan Wang",
      "Zhongteng Cai",
      "Yujia Bao",
      "Xueru Zhang",
      "Yang Liu"
    ],
    "summary": "The rapid advancement of large language models (LLMs) has led to growing interest in using synthetic data to train future models. However, this creates a self-consuming retraining loop, where models are trained on their own outputs and may cause performance drops and induce emerging biases. In real-world applications, previously deployed LLMs may influence the data they generate, leading to a dynamic system driven by user feedback. For example, if a model continues to underserve users from a group, less query data will be collected from this particular demographic of users. In this study, we introduce the concept of \\textbf{S}elf-\\textbf{C}onsuming \\textbf{P}erformative \\textbf{L}oop (\\textbf{SCPL}) and investigate the role of synthetic data in shaping bias during these dynamic iterative training processes under controlled performative feedback. This controlled setting is motivated by the inaccessibility of real-world user preference data from dynamic production systems, and enables us to isolate and analyze feedback-driven bias evolution in a principled manner. We focus on two types of loops, including the typical retraining setting and the incremental fine-tuning setting, which is largely underexplored. Through experiments on three real-world tasks, we find that the performative loop increases preference bias and decreases disparate bias. We design a reward-based rejection sampling strategy to mitigate the bias, moving towards more trustworthy self-improving systems.",
    "link": "http://arxiv.org/abs/2601.05184v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05172v1",
    "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
    "authors": [
      "Haoyu Zhao",
      "Akide Liu",
      "Zeyu Zhang",
      "Weijie Wang",
      "Feng Chen",
      "Ruihan Zhu",
      "Gholamreza Haffari",
      "Bohan Zhuang"
    ],
    "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.   We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
    "link": "http://arxiv.org/abs/2601.05172v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05170v1",
    "title": "Reverse-engineering NLI: A study of the meta-inferential properties of Natural Language Inference",
    "authors": [
      "Rasmus Blanck",
      "Bill Noble",
      "Stergios Chatzikyriakidis"
    ],
    "summary": "Natural Language Inference (NLI) has been an important task for evaluating language models for Natural Language Understanding, but the logical properties of the task are poorly understood and often mischaracterized. Understanding the notion of inference captured by NLI is key to interpreting model performance on the task. In this paper we formulate three possible readings of the NLI label set and perform a comprehensive analysis of the meta-inferential properties they entail. Focusing on the SNLI dataset, we exploit (1) NLI items with shared premises and (2) items generated by LLMs to evaluate models trained on SNLI for meta-inferential consistency and derive insights into which reading of the logical relations is encoded by the dataset.",
    "link": "http://arxiv.org/abs/2601.05170v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05167v1",
    "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
    "authors": [
      "Chengsong Huang",
      "Tong Zheng",
      "Langlin Huang",
      "Jinyuan Li",
      "Haolin Liu",
      "Jiaxin Huang"
    ],
    "summary": "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.",
    "link": "http://arxiv.org/abs/2601.05167v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05162v1",
    "title": "GenAI-DrawIO-Creator: A Framework for Automated Diagram Generation",
    "authors": [
      "Jinze Yu",
      "Dayuan Jiang"
    ],
    "summary": "Diagrams are crucial for communicating complex information, yet creating and modifying them remains a labor-intensive task. We present GenAI-DrawIO-Creator, a novel framework that leverages Large Language Models (LLMs) to automate diagram generation and manipulation in the structured XML format used by draw.io. Our system integrates Claude 3.7 to reason about structured visual data and produce valid diagram representations. Key contributions include a high-level system design enabling real-time diagram updates, specialized prompt engineering and error-checking to ensure well-formed XML outputs. We demonstrate a working prototype capable of generating accurate diagrams (such as network architectures and flowcharts) from natural language or code, and even replicating diagrams from images. Simulated evaluations show that our approach significantly reduces diagram creation time and produces outputs with high structural fidelity. Our results highlight the promise of Claude 3.7 in handling structured visual reasoning tasks and lay the groundwork for future research in AI-assisted diagramming applications.",
    "link": "http://arxiv.org/abs/2601.05162v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05144v1",
    "title": "Distilling the Thought, Watermarking the Answer: A Principle Semantic Guided Watermark for Large Reasoning Models",
    "authors": [
      "Shuliang Liu",
      "Xingyu Li",
      "Hongyi Liu",
      "Yibo Yan",
      "Bingchen Duan",
      "Qi Zheng",
      "Dong Fang",
      "Lingfeng Su",
      "Xuming Hu"
    ],
    "summary": "Reasoning Large Language Models (RLLMs) excelling in complex tasks present unique challenges for digital watermarking, as existing methods often disrupt logical coherence or incur high computational costs. Token-based watermarking techniques can corrupt the reasoning flow by applying pseudo-random biases, while semantic-aware approaches improve quality but introduce significant latency or require auxiliary models. This paper introduces ReasonMark, a novel watermarking framework specifically designed for reasoning-intensive LLMs. Our approach decouples generation into an undisturbed Thinking Phase and a watermarked Answering Phase. We propose a Criticality Score to identify semantically pivotal tokens from the reasoning trace, which are distilled into a Principal Semantic Vector (PSV). The PSV then guides a semantically-adaptive mechanism that modulates watermark strength based on token-PSV alignment, ensuring robustness without compromising logical integrity. Extensive experiments show ReasonMark surpasses state-of-the-art methods by reducing text Perplexity by 0.35, increasing translation BLEU score by 0.164, and raising mathematical accuracy by 0.67 points. These advancements are achieved alongside a 0.34% higher watermark detection AUC and stronger robustness to attacks, all with a negligible increase in latency. This work enables the traceable and trustworthy deployment of reasoning LLMs in real-world applications.",
    "link": "http://arxiv.org/abs/2601.05144v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05114v1",
    "title": "Evaluative Fingerprints: Stable and Systematic Differences in LLM Evaluator Behavior",
    "authors": [
      "Wajid Nasser"
    ],
    "summary": "LLM-as-judge systems promise scalable, consistent evaluation. We find the opposite: judges are consistent, but not with each other; they are consistent with themselves. Across 3,240 evaluations (9 judges x 120 unique video x pack items x 3 independent runs), inter-judge agreement is near-zero (Krippendorff's \u03b1 = 0.042). On two dimensions, judges disagree more than random noise would predict (\u03b1 < 0). Yet this disagreement isn't chaos; it's structured. A classifier identifies which judge produced an evaluation with 77.1% accuracy from rubric scores alone, rising to 89.9% with disposition features. Within model families, the signal is even stronger: GPT-4.1 and GPT-5.2 are distinguishable with 99.6% accuracy. We call this the reliability paradox: judges cannot agree on what constitutes quality, yet their disagreement patterns are so stable they function as fingerprints. Each judge implements a distinct, stable theory of quality: an \"evaluative disposition\" that shapes how it interprets any rubric. We characterize these dispositions along multiple axes: harshness/leniency, dimension emphasis, within-judge stability (ICC), and evidence behavior (receipt validity, semantic linkage via NLI, and shotgun index). The implication is stark: LLM judges are not interchangeable instruments measuring a shared construct. They are distinct measurement devices, each encoding its own implicit theory of quality. Averaging their scores produces a synthetic verdict that corresponds to no judge's actual values.",
    "link": "http://arxiv.org/abs/2601.05114v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05111v1",
    "title": "Agent-as-a-Judge",
    "authors": [
      "Runyang You",
      "Hongru Cai",
      "Caiqi Zhang",
      "Qiancheng Xu",
      "Meng Liu",
      "Tiezheng Yu",
      "Yongqi Li",
      "Wenjie Li"
    ],
    "summary": "LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.",
    "link": "http://arxiv.org/abs/2601.05111v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05109v1",
    "title": "Nalar: An agent serving framework",
    "authors": [
      "Marco Laju",
      "Donghyun Son",
      "Saurabh Agarwal",
      "Nitin Kedia",
      "Myungjin Lee",
      "Jayanth Srinivasa",
      "Aditya Akella"
    ],
    "summary": "LLM-driven agentic applications increasingly automate complex, multi-step tasks, but serving them efficiently remains challenging due to heterogeneous components, dynamic and model-driven control flow, long-running state, and unpredictable latencies. Nalar is a ground-up agent-serving framework that cleanly separates workflow specification from execution while providing the runtime visibility and control needed for robust performance. Nalar preserves full Python expressiveness, using lightweight auto-generated stubs that turn agent and tool invocations into futures carrying dependency and context metadata. A managed state layer decouples logical state from physical placement, enabling safe reuse, migration, and consistent retry behavior. A two-level control architecture combines global policy computation with local event-driven enforcement to support adaptive routing, scheduling, and resource management across evolving workflows. Together, these mechanisms allow Nalar to deliver scalable, efficient, and policy-driven serving of heterogeneous agentic applications without burdening developers with orchestration logic. Across three agentic workloads, Nalar cuts tail latency by 34--74\\%, achieves up to $2.9\\times$ speedups, sustains 80 RPS where baselines fail, and scales to 130K futures with sub-500 ms control overhead.",
    "link": "http://arxiv.org/abs/2601.05109v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05107v1",
    "title": "Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction",
    "authors": [
      "Muzhao Tian",
      "Zisu Huang",
      "Xiaohua Wang",
      "Jingwen Xu",
      "Zhengkang Guo",
      "Qi Qian",
      "Yuanzhe Shen",
      "Kaitao Song",
      "Jiakang Yuan",
      "Changze Lv",
      "Xiaoqing Zheng"
    ],
    "summary": "As LLM-based agents are increasingly used in long-term interactions, cumulative memory is critical for enabling personalization and maintaining stylistic consistency. However, most existing systems adopt an ``all-or-nothing'' approach to memory usage: incorporating all relevant past information can lead to \\textit{Memory Anchoring}, where the agent is trapped by past interactions, while excluding memory entirely results in under-utilization and the loss of important interaction history. We show that an agent's reliance on memory can be modeled as an explicit and user-controllable dimension. We first introduce a behavioral metric of memory dependence to quantify the influence of past interactions on current outputs. We then propose \\textbf{Stee}rable \\textbf{M}emory Agent, \\texttt{SteeM}, a framework that allows users to dynamically regulate memory reliance, ranging from a fresh-start mode that promotes innovation to a high-fidelity mode that closely follows interaction history. Experiments across different scenarios demonstrate that our approach consistently outperforms conventional prompting and rigid memory masking strategies, yielding a more nuanced and effective control for personalized human-agent collaboration.",
    "link": "http://arxiv.org/abs/2601.05107v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05106v1",
    "title": "Token-Level LLM Collaboration via FusionRoute",
    "authors": [
      "Nuoya Xiong",
      "Yuhang Zhou",
      "Hanqing Zeng",
      "Zhaorun Chen",
      "Furong Huang",
      "Shuchao Bi",
      "Lizhu Zhang",
      "Zhuokai Zhao"
    ],
    "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
    "link": "http://arxiv.org/abs/2601.05106v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05103v1",
    "title": "Semantically Orthogonal Framework for Citation Classification: Disentangling Intent and Content",
    "authors": [
      "Changxu Duan",
      "Zhiyin Tan"
    ],
    "summary": "Understanding the role of citations is essential for research assessment and citation-aware digital libraries. However, existing citation classification frameworks often conflate citation intent (why a work is cited) with cited content type (what part is cited), limiting their effectiveness in auto classification due to a dilemma between fine-grained type distinctions and practical classification reliability. We introduce SOFT, a Semantically Orthogonal Framework with Two dimensions that explicitly separates citation intent from cited content type, drawing inspiration from semantic role theory. We systematically re-annotate the ACL-ARC dataset using SOFT and release a cross-disciplinary test set sampled from ACT2. Evaluation with both zero-shot and fine-tuned Large Language Models demonstrates that SOFT enables higher agreement between human annotators and LLMs, and supports stronger classification performance and robust cross-domain generalization compared to ACL-ARC and SciCite annotation frameworks. These results confirm SOFT's value as a clear, reusable annotation standard, improving clarity, consistency, and generalizability for digital libraries and scholarly communication infrastructures. All code and data are publicly available on GitHub https://github.com/zhiyintan/SOFT.",
    "link": "http://arxiv.org/abs/2601.05103v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05101v1",
    "title": "Arabic Prompts with English Tools: A Benchmark",
    "authors": [
      "Konstantin Kubrak",
      "Ahmed El-Moselhy",
      "Ammar Alsulami",
      "Remaz Altuwaim",
      "Hassan Ismail Fawaz",
      "Faisal Alsaby"
    ],
    "summary": "Large Language Models (LLMs) are now integral to numerous industries, increasingly serving as the core reasoning engine for autonomous agents that perform complex tasks through tool-use. While the development of Arabic-native LLMs is accelerating, the benchmarks for evaluating their capabilities lag behind, with most existing frameworks focusing on English. A critical and overlooked area is tool-calling, where the performance of models prompted in non-English languages like Arabic is poorly understood, especially since these models are often pretrained on predominantly English data. This paper addresses this critical gap by introducing the first dedicated benchmark for evaluating the tool-calling and agentic capabilities of LLMs in the Arabic language. Our work provides a standardized framework to measure the functional accuracy and robustness of models in Arabic agentic workflows. Our findings reveal a huge performance gap: when users interact in Arabic, tool-calling accuracy drops by an average of 5-10\\%, regardless of whether the tool descriptions themselves are in Arabic or English. By shedding light on these critical challenges, this benchmark aims to foster the development of more reliable and linguistically equitable AI agents for Arabic-speaking users.",
    "link": "http://arxiv.org/abs/2601.05101v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05075v1",
    "title": "SemPA: Improving Sentence Embeddings of Large Language Models through Semantic Preference Alignment",
    "authors": [
      "Ziyang Chen",
      "Zhenxuan Huang",
      "Yile Wang",
      "Weiqin Wang",
      "Lu Yin",
      "Hui Huang"
    ],
    "summary": "Traditional sentence embedding methods employ token-level contrastive learning on non-generative pre-trained models. Recently, there have emerged embedding methods based on generative large language models (LLMs). These methods either rely on fixed prompt templates or involve modifications to the model architecture. The former lacks further optimization of the model and results in limited performance, while the latter alters the internal computational mechanisms of the model, thereby compromising its generative capabilities. We propose SemPA, a novel approach that boosts the sentence representations while preserving the generative ability of LLMs via semantic preference alignment. We leverage sentence-level Direct Preference Optimization (DPO) to efficiently optimize LLMs on a paraphrase generation task, where the model learns to discriminate semantically equivalent sentences while preserving inherent generative capacity. Theoretically, we establish a formal connection between DPO and contrastive learning under the Plackett-Luce model framework. Empirically, experimental results on both semantic textual similarity tasks and various benchmarks for LLMs show that SemPA achieves better semantic representations without sacrificing the inherent generation capability of LLMs.",
    "link": "http://arxiv.org/abs/2601.05075v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05062v1",
    "title": "Compositional Steering of Large Language Models with Steering Tokens",
    "authors": [
      "Gorjan Radevski",
      "Kiril Gashteovski",
      "Giwon Hong",
      "Carolin Lawrence",
      "Goran Glava\u0161"
    ],
    "summary": "Deploying LLMs in real-world applications requires controllable output that satisfies multiple desiderata at the same time. While existing work extensively addresses LLM steering for a single behavior, \\textit{compositional steering} -- i.e., steering LLMs simultaneously towards multiple behaviors -- remains an underexplored problem. In this work, we propose \\emph{compositional steering tokens} for multi-behavior steering. We first embed individual behaviors, expressed as natural language instructions, into dedicated tokens via self-distillation. Contrary to most prior work, which operates in the activation space, our behavior steers live in the space of input tokens, enabling more effective zero-shot composition. We then train a dedicated \\textit{composition token} on pairs of behaviors and show that it successfully captures the notion of composition: it generalizes well to \\textit{unseen} compositions, including those with unseen behaviors as well as those with an unseen \\textit{number} of behaviors. Our experiments across different LLM architectures show that steering tokens lead to superior multi-behavior control compared to competing approaches (instructions, activation steering, and LoRA merging). Moreover, we show that steering tokens complement natural language instructions, with their combination resulting in further gains.",
    "link": "http://arxiv.org/abs/2601.05062v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05053v1",
    "title": "Reinforced Efficient Reasoning via Semantically Diverse Exploration",
    "authors": [
      "Ziqi Zhao",
      "Zhaochun Ren",
      "Jiahong Zou",
      "Liu Yang",
      "Zhiwei Xu",
      "Xuri Ge",
      "Zhumin Chen",
      "Xinyu Ma",
      "Daiting Shi",
      "Shuaiqiang Wang",
      "Dawei Yin",
      "Xin Xin"
    ],
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has proven effective in enhancing the reasoning of large language models (LLMs). Monte Carlo Tree Search (MCTS)-based extensions improve upon vanilla RLVR (e.g., GRPO) by providing tree-based reasoning rollouts that enable fine-grained and segment-level credit assignment. However, existing methods still suffer from limited exploration diversity and inefficient reasoning. To address the above challenges, we propose reinforced efficient reasoning via semantically diverse explorations, i.e., ROSE, for LLMs. To encourage more diverse reasoning exploration, our method incorporates a semantic-entropy-based branching strategy and an $\\varepsilon$-exploration mechanism. The former operates on already sampled reasoning rollouts to capture semantic uncertainty and select branching points with high semantic divergence to generate new successive reasoning paths, whereas the latter stochastically initiates reasoning rollouts from the root, preventing the search process from becoming overly local. To improve efficiency, we design a length-aware segment-level advantage estimator that rewards concise and correct reasoning while penalizing unnecessarily long reasoning chains. Extensive experiments on various mathematical reasoning benchmarks with Qwen and Llama models validate the effectiveness and efficiency of ROSE. Codes are available at https://github.com/ZiqiZhao1/ROSE-rl.",
    "link": "http://arxiv.org/abs/2601.05053v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  }
]