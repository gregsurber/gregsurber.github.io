[
  {
    "id": "http://arxiv.org/abs/2601.11522v1",
    "title": "UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation",
    "authors": [
      "Ruiheng Zhang",
      "Jingfeng Yao",
      "Huangxuan Zhao",
      "Hao Yan",
      "Xiao He",
      "Lei Chen",
      "Zhou Wei",
      "Yong Luo",
      "Zengmao Wang",
      "Lefei Zhang",
      "Dacheng Tao",
      "Bo Du"
    ],
    "summary": "Despite recent progress, medical foundation models still struggle to unify visual understanding and generation, as these tasks have inherently conflicting goals: semantic abstraction versus pixel-level reconstruction. Existing approaches, typically based on parameter-shared autoregressive architectures, frequently lead to compromised performance in one or both tasks. To address this, we present UniX, a next-generation unified medical foundation model for chest X-ray understanding and generation. UniX decouples the two tasks into an autoregressive branch for understanding and a diffusion branch for high-fidelity generation. Crucially, a cross-modal self-attention mechanism is introduced to dynamically guide the generation process with understanding features. Coupled with a rigorous data cleaning pipeline and a multi-stage training strategy, this architecture enables synergistic collaboration between tasks while leveraging the strengths of diffusion models for superior generation. On two representative benchmarks, UniX achieves a 46.1% improvement in understanding performance (Micro-F1) and a 24.2% gain in generation quality (FD-RadDino), using only a quarter of the parameters of LLM-CXR. By achieving performance on par with task-specific models, our work establishes a scalable paradigm for synergistic medical image understanding and generation. Codes and models are available at https://github.com/ZrH42/UniX.",
    "link": "http://arxiv.org/abs/2601.11522v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11518v1",
    "title": "How Long Is a Piece of String? A Brief Empirical Analysis of Tokenizers",
    "authors": [
      "Jonathan Roberts",
      "Kai Han",
      "Samuel Albanie"
    ],
    "summary": "Frontier LLMs are increasingly utilised across academia, society and industry. A commonly used unit for comparing models, their inputs and outputs, and estimating inference pricing is the token. In general, tokens are used as a stable currency, assumed to be broadly consistent across tokenizers and contexts, enabling direct comparisons. However, tokenization varies significantly across models and domains of text, making naive interpretation of token counts problematic. We quantify this variation by providing a comprehensive empirical analysis of tokenization, exploring the compression of sequences to tokens across different distributions of textual data. Our analysis challenges commonly held heuristics about token lengths, finding them to be overly simplistic. We hope the insights of our study add clarity and intuition toward tokenization in contemporary LLMs.",
    "link": "http://arxiv.org/abs/2601.11518v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11488v1",
    "title": "CTest-Metric: A Unified Framework to Assess Clinical Validity of Metrics for CT Report Generation",
    "authors": [
      "Vanshali Sharma",
      "Andrea Mia Bejar",
      "Gorkem Durak",
      "Ulas Bagci"
    ],
    "summary": "In the generative AI era, where even critical medical tasks are increasingly automated, radiology report generation (RRG) continues to rely on suboptimal metrics for quality assessment. Developing domain-specific metrics has therefore been an active area of research, yet it remains challenging due to the lack of a unified, well-defined framework to assess their robustness and applicability in clinical contexts. To address this, we present CTest-Metric, a first unified metric assessment framework with three modules determining the clinical feasibility of metrics for CT RRG. The modules test: (i) Writing Style Generalizability (WSG) via LLM-based rephrasing; (ii) Synthetic Error Injection (SEI) at graded severities; and (iii) Metrics-vs-Expert correlation (MvE) using clinician ratings on 175 \"disagreement\" cases. Eight widely used metrics (BLEU, ROUGE, METEOR, BERTScore-F1, F1-RadGraph, RaTEScore, GREEN Score, CRG) are studied across seven LLMs built on a CT-CLIP encoder. Using our novel framework, we found that lexical NLG metrics are highly sensitive to stylistic variations; GREEN Score aligns best with expert judgments (Spearman~0.70), while CRG shows negative correlation; and BERTScore-F1 is least sensitive to factual error injection. We will release the framework, code, and allowable portion of the anonymized evaluation data (rephrased/error-injected CT reports), to facilitate reproducible benchmarking and future metric development.",
    "link": "http://arxiv.org/abs/2601.11488v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11479v1",
    "title": "Health Facility Location in Ethiopia: Leveraging LLMs to Integrate Expert Knowledge into Algorithmic Planning",
    "authors": [
      "Yohai Trabelsi",
      "Guojun Xiong",
      "Fentabil Getnet",
      "St\u00e9phane Verguet",
      "Milind Tambe"
    ],
    "summary": "Ethiopia's Ministry of Health is upgrading health posts to improve access to essential services, particularly in rural areas. Limited resources, however, require careful prioritization of which facilities to upgrade to maximize population coverage while accounting for diverse expert and stakeholder preferences. In collaboration with the Ethiopian Public Health Institute and Ministry of Health, we propose a hybrid framework that systematically integrates expert knowledge with optimization techniques. Classical optimization methods provide theoretical guarantees but require explicit, quantitative objectives, whereas stakeholder criteria are often articulated in natural language and difficult to formalize. To bridge these domains, we develop the Large language model and Extended Greedy (LEG) framework. Our framework combines a provable approximation algorithm for population coverage optimization with LLM-driven iterative refinement that incorporates human-AI alignment to ensure solutions reflect expert qualitative guidance while preserving coverage guarantees. Experiments on real-world data from three Ethiopian regions demonstrate the framework's effectiveness and its potential to inform equitable, data-driven health system planning.",
    "link": "http://arxiv.org/abs/2601.11479v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11468v1",
    "title": "Exploring LLM Features in Predictive Process Monitoring for Small-Scale Event-Logs",
    "authors": [
      "Alessandro Padella",
      "Massimiliano de Leoni",
      "Marlon Dumas"
    ],
    "summary": "Predictive Process Monitoring is a branch of process mining that aims to predict the outcome of an ongoing process. Recently, it leveraged machine-and-deep learning architectures. In this paper, we extend our prior LLM-based Predictive Process Monitoring framework, which was initially focused on total time prediction via prompting. The extension consists of comprehensively evaluating its generality, semantic leverage, and reasoning mechanisms, also across multiple Key Performance Indicators. Empirical evaluations conducted on three distinct event logs and across the Key Performance Indicators of Total Time and Activity Occurrence prediction indicate that, in data-scarce settings with only 100 traces, the LLM surpasses the benchmark methods. Furthermore, the experiments also show that the LLM exploits both its embodied prior knowledge and the internal correlations among training traces. Finally, we examine the reasoning strategies employed by the model, demonstrating that the LLM does not merely replicate existing predictive methods but performs higher-order reasoning to generate the predictions.",
    "link": "http://arxiv.org/abs/2601.11468v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11441v1",
    "title": "Hierarchical Orthogonal Residual Spread for Precise Massive Editing in Large Language Models",
    "authors": [
      "Xiaojie Gu",
      "Guangxu Chen",
      "Yuheng Yang",
      "Jingxin Han",
      "Andi Zhang"
    ],
    "summary": "Large language models (LLMs) exhibit exceptional performance across various domains, yet they face critical safety concerns. Model editing has emerged as an effective approach to mitigate these issues. Existing model editing methods often focus on optimizing an information matrix that blends new and old knowledge. While effective, these approaches can be computationally expensive and may cause conflicts. In contrast, we shift our attention to Hierarchical Orthogonal Residual SprEad of the information matrix, which reduces noisy gradients and enables more stable edits from a different perspective. We demonstrate the effectiveness of our method HORSE through a clear theoretical comparison with several popular methods and extensive experiments conducted on two datasets across multiple LLMs. The results show that HORSE maintains precise massive editing across diverse scenarios. The code is available at https://github.com/XiaojieGu/HORSE",
    "link": "http://arxiv.org/abs/2601.11441v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11432v1",
    "title": "The unreasonable effectiveness of pattern matching",
    "authors": [
      "Gary Lupyan",
      "Blaise Ag\u00fcera y Arcas"
    ],
    "summary": "We report on an astonishing ability of large language models (LLMs) to make sense of \"Jabberwocky\" language in which most or all content words have been randomly replaced by nonsense strings, e.g., translating \"He dwushed a ghanc zawk\" to \"He dragged a spare chair\". This result addresses ongoing controversies regarding how to best think of what LLMs are doing: are they a language mimic, a database, a blurry version of the Web? The ability of LLMs to recover meaning from structural patterns speaks to the unreasonable effectiveness of pattern-matching. Pattern-matching is not an alternative to \"real\" intelligence, but rather a key ingredient.",
    "link": "http://arxiv.org/abs/2601.11432v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11429v1",
    "title": "Relational Linearity is a Predictor of Hallucinations",
    "authors": [
      "Yuetian Lu",
      "Yihong Liu",
      "Hinrich Sch\u00fctze"
    ],
    "summary": "Hallucination is a central failure mode in large language models (LLMs). We focus on hallucinations of answers to questions like: \"Which instrument did Glenn Gould play?\", but we ask these questions for synthetic entities that are unknown to the model. Surprisingly, we find that medium-size models like Gemma-7B-IT frequently hallucinate, i.e., they have difficulty recognizing that the hallucinated fact is not part of their knowledge. We hypothesize that an important factor in causing these hallucinations is the linearity of the relation: linear relations tend to be stored more abstractly, making it difficult for the LLM to assess its knowledge; the facts of nonlinear relations tend to be stored more directly, making knowledge assessment easier. To investigate this hypothesis, we create SyntHal, a dataset of 6000 synthetic entities for six relations. In our experiments with four models, we determine, for each relation, the hallucination rate on SyntHal and also measure its linearity, using $\u0394\\cos$. We find a strong correlation ($r \\in [.78,.82]$) between relational linearity and hallucination rate, providing evidence for our hypothesis that the underlying storage of triples of a relation is a factor in how well a model can self-assess its knowledge. This finding has implications for how to manage hallucination behavior and suggests new research directions for improving the representation of factual knowledge in LLMs.",
    "link": "http://arxiv.org/abs/2601.11429v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11398v1",
    "title": "Understanding Help Seeking for Digital Privacy, Safety, and Security",
    "authors": [
      "Kurt Thomas",
      "Sai Teja Peddinti",
      "Sarah Meiklejohn",
      "Tara Matthews",
      "Amelia Hassoun",
      "Animesh Srivastava",
      "Jessica McClearn",
      "Patrick Gage Kelley",
      "Sunny Consolvo",
      "Nina Taft"
    ],
    "summary": "The complexity of navigating digital privacy, safety, and security threats often falls directly on users. This leads to users seeking help from family and peers, platforms and advice guides, dedicated communities, and even large language models (LLMs). As a precursor to improving resources across this ecosystem, our community needs to understand what help seeking looks like in the wild. To that end, we blend qualitative coding with LLM fine-tuning to sift through over one billion Reddit posts from the last four years to identify where and for what users seek digital privacy, safety, or security help. We isolate three million relevant posts with 93% precision and recall and automatically annotate each with the topics discussed (e.g., security tools, privacy configurations, scams, account compromise, content moderation, and more). We use this dataset to understand the scope and scale of help seeking, the communities that provide help, and the types of help sought. Our work informs the development of better resources for users (e.g., user guides or LLM help-giving agents) while underscoring the inherent challenges of supporting users through complex combinations of threats, platforms, mitigations, context, and emotions.",
    "link": "http://arxiv.org/abs/2601.11398v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11379v1",
    "title": "Evaluating LLM Behavior in Hiring: Implicit Weights, Fairness Across Groups, and Alignment with Human Preferences",
    "authors": [
      "Morgane Hoffmann",
      "Emma Jouffroy",
      "Warren Jouanneau",
      "Marc Palyart",
      "Charles Pebereau"
    ],
    "summary": "General-purpose Large Language Models (LLMs) show significant potential in recruitment applications, where decisions require reasoning over unstructured text, balancing multiple criteria, and inferring fit and competence from indirect productivity signals. Yet, it is still uncertain how LLMs assign importance to each attribute and whether such assignments are in line with economic principles, recruiter preferences or broader societal norms. We propose a framework to evaluate an LLM's decision logic in recruitment, by drawing on established economic methodologies for analyzing human hiring behavior. We build synthetic datasets from real freelancer profiles and project descriptions from a major European online freelance marketplace and apply a full factorial design to estimate how a LLM weighs different match-relevant criteria when evaluating freelancer-project fit. We identify which attributes the LLM prioritizes and analyze how these weights vary across project contexts and demographic subgroups. Finally, we explain how a comparable experimental setup could be implemented with human recruiters to assess alignment between model and human decisions. Our findings reveal that the LLM weighs core productivity signals, such as skills and experience, but interprets certain features beyond their explicit matching value. While showing minimal average discrimination against minority groups, intersectional effects reveal that productivity signals carry different weights between demographic groups.",
    "link": "http://arxiv.org/abs/2601.11379v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11374v1",
    "title": "Reward Modeling for Scientific Writing Evaluation",
    "authors": [
      "Furkan \u015eahinu\u00e7",
      "Subhabrata Dutta",
      "Iryna Gurevych"
    ],
    "summary": "Scientific writing is an expert-domain task that demands deep domain knowledge, task-specific requirements and reasoning capabilities that leverage the domain knowledge to satisfy the task specifications. While scientific text generation has been widely studied, its evaluation remains a challenging and open problem. It is critical to develop models that can be reliably deployed for evaluating diverse open-ended scientific writing tasks while adhering to their distinct requirements. However, existing LLM-based judges and reward models are primarily optimized for general-purpose benchmarks with fixed scoring rubrics and evaluation criteria. Consequently, they often fail to reason over sparse knowledge of scientific domains when interpreting task-dependent and multi-faceted criteria. Moreover, fine-tuning for each individual task is costly and impractical for low-resource settings. To bridge these gaps, we propose cost-efficient, open-source reward models tailored for scientific writing evaluation. We introduce a two-stage training framework that initially optimizes scientific evaluation preferences and then refines reasoning capabilities. Our multi-aspect evaluation design and joint training across diverse tasks enable fine-grained assessment and robustness to dynamic criteria and scoring rubrics. Experimental analysis shows that our training regime strongly improves LLM-based scientific writing evaluation. Our models generalize effectively across tasks and to previously unseen scientific writing evaluation settings, allowing a single trained evaluator to be reused without task-specific retraining.",
    "link": "http://arxiv.org/abs/2601.11374v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11369v1",
    "title": "Institutional AI: Governing LLM Collusion in Multi-Agent Cournot Markets via Public Governance Graphs",
    "authors": [
      "Marcantonio Bracale Syrnikov",
      "Federico Pierucci",
      "Marcello Galisai",
      "Matteo Prandi",
      "Piercosma Bisconti",
      "Francesco Giarrusso",
      "Olga Sorokoletova",
      "Vincenzo Suriani",
      "Daniele Nardi"
    ],
    "summary": "Multi-agent LLM ensembles can converge on coordinated, socially harmful equilibria. This paper advances an experimental framework for evaluating Institutional AI, our system-level approach to AI alignment that reframes alignment from preference engineering in agent-space to mechanism design in institution-space. Central to this approach is the governance graph, a public, immutable manifest that declares legal states, transitions, sanctions, and restorative paths; an Oracle/Controller runtime interprets this manifest, attaching enforceable consequences to evidence of coordination while recording a cryptographically keyed, append-only governance log for audit and provenance. We apply the Institutional AI framework to govern the Cournot collusion case documented by prior work and compare three regimes: Ungoverned (baseline incentives from the structure of the Cournot market), Constitutional (a prompt-only policy-as-prompt prohibition implemented as a fixed written anti-collusion constitution, and Institutional (governance-graph-based). Across six model configurations including cross-provider pairs (N=90 runs/condition), the Institutional regime produces large reductions in collusion: mean tier falls from 3.1 to 1.8 (Cohen's d=1.28), and severe-collusion incidence drops from 50% to 5.6%. The prompt-only Constitutional baseline yields no reliable improvement, illustrating that declarative prohibitions do not bind under optimisation pressure. These results suggest that multi-agent alignment may benefit from being framed as an institutional design problem, where governance graphs can provide a tractable abstraction for alignment-relevant collective behavior.",
    "link": "http://arxiv.org/abs/2601.11369v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11362v1",
    "title": "RITA: A Tool for Automated Requirements Classification and Specification from Online User Feedback",
    "authors": [
      "Manjeshwar Aniruddh Mallya",
      "Alessio Ferrari",
      "Mohammad Amin Zadenoori",
      "Jacek D\u0105browski"
    ],
    "summary": "Context and motivation. Online user feedback is a valuable resource for requirements engineering, but its volume and noise make analysis difficult. Existing tools support individual feedback analysis tasks, but their capabilities are rarely integrated into end-to-end support. Problem. The lack of end-to-end integration limits the practical adoption of existing RE tools and makes it difficult to assess their real-world usefulness. Solution. To address this challenge, we present RITA, a tool that integrates lightweight open-source large language models into a unified workflow for feedback-driven RE. RITA supports automated request classification, non-functional requirement identification, and natural-language requirements specification generation from online feedback via a user-friendly interface, and integrates with Jira for seamless transfer of requirements specifications to development tools. Results and conclusions. RITA exploits previously evaluated LLM-based RE techniques to efficiently transform raw user feedback into requirements artefacts, helping bridge the gap between research and practice. A demonstration is available at: https://youtu.be/8meCLpwQWV8.",
    "link": "http://arxiv.org/abs/2601.11362v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11354v1",
    "title": "AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems",
    "authors": [
      "Weiyi Wang",
      "Xinchi Chen",
      "Jingjing Gong",
      "Xuanjing Huang",
      "Xipeng Qiu"
    ],
    "summary": "Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.",
    "link": "http://arxiv.org/abs/2601.11354v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11344v1",
    "title": "How Much Would a Clinician Edit This Draft? Evaluating LLM Alignment for Patient Message Response Drafting",
    "authors": [
      "Parker Seegmiller",
      "Joseph Gatto",
      "Sarah E. Greer",
      "Ganza Belise Isingizwe",
      "Rohan Ray",
      "Timothy E. Burdick",
      "Sarah Masud Preum"
    ],
    "summary": "Large language models (LLMs) show promise in drafting responses to patient portal messages, yet their integration into clinical workflows raises various concerns, including whether they would actually save clinicians time and effort in their portal workload. We investigate LLM alignment with individual clinicians through a comprehensive evaluation of the patient message response drafting task. We develop a novel taxonomy of thematic elements in clinician responses and propose a novel evaluation framework for assessing clinician editing load of LLM-drafted responses at both content and theme levels. We release an expert-annotated dataset and conduct large-scale evaluations of local and commercial LLMs using various adaptation techniques including thematic prompting, retrieval-augmented generation, supervised fine-tuning, and direct preference optimization. Our results reveal substantial epistemic uncertainty in aligning LLM drafts with clinician responses. While LLMs demonstrate capability in drafting certain thematic elements, they struggle with clinician-aligned generation in other themes, particularly question asking to elicit further information from patients. Theme-driven adaptation strategies yield improvements across most themes. Our findings underscore the necessity of adapting LLMs to individual clinician preferences to enable reliable and responsible use in patient-clinician communication workflows.",
    "link": "http://arxiv.org/abs/2601.11344v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11342v1",
    "title": "Unlocking the Potentials of Retrieval-Augmented Generation for Diffusion Language Models",
    "authors": [
      "Chuanyue Yu",
      "Jiahui Wang",
      "Yuhan Li",
      "Heng Chang",
      "Ge Lan",
      "Qingyun Sun",
      "Jia Li",
      "Jianxin Li",
      "Ziwei Zhang"
    ],
    "summary": "Diffusion Language Models (DLMs) have recently demonstrated remarkable capabilities in natural language processing tasks. However, the potential of Retrieval-Augmented Generation (RAG), which shows great successes for enhancing large language models (LLMs), has not been well explored, due to the fundamental difference between LLM and DLM decoding. To fill this critical gap, we systematically test the performance of DLMs within the RAG framework. Our findings reveal that DLMs coupled with RAG show promising potentials with stronger dependency on contextual information, but suffer from limited generation precision. We identify a key underlying issue: Response Semantic Drift (RSD), where the generated answer progressively deviates from the query's original semantics, leading to low precision content. We trace this problem to the denoising strategies in DLMs, which fail to maintain semantic alignment with the query throughout the iterative denoising process. To address this, we propose Semantic-Preserving REtrieval-Augmented Diffusion (SPREAD), a novel framework that introduces a query-relevance-guided denoising strategy. By actively guiding the denoising trajectory, SPREAD ensures the generation remains anchored to the query's semantics and effectively suppresses drift. Experimental results demonstrate that SPREAD significantly enhances the precision and effectively mitigates RSD of generated answers within the RAG framework.",
    "link": "http://arxiv.org/abs/2601.11342v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11332v1",
    "title": "Idea First, Code Later: Disentangling Problem Solving from Code Generation in Evaluating LLMs for Competitive Programming",
    "authors": [
      "Sama Hadhoud",
      "Alaa Elsetohy",
      "Frederikus Hudi",
      "Jan Christian Blaise Cruz",
      "Steven Halim",
      "Alham Fikri Aji"
    ],
    "summary": "Large Language Models (LLMs) increasingly succeed on competitive programming problems, yet existing evaluations conflate algorithmic reasoning with code-level implementation. We argue that competitive programming is fundamentally a problem-solving task and propose centering natural-language editorials in both solution generation and evaluation. Generating an editorial prior to code improves solve rates for some LLMs, with substantially larger gains when using expertly written gold editorials. However, even with gold editorials, models continue to struggle with implementation, while the gap between generated and gold editorials reveals a persistent problem-solving bottleneck in specifying correct and complete algorithms. Beyond pass/fail metrics, we diagnose reasoning errors by comparing model-generated editorials to gold standards using expert annotations and validate an LLM-as-a-judge protocol for scalable evaluation. We introduce a dataset of 83 ICPC-style problems with gold editorials and full test suites, and evaluate 19 LLMs, arguing that future benchmarks should explicitly separate problem solving from implementation.",
    "link": "http://arxiv.org/abs/2601.11332v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11327v1",
    "title": "Can Small Agent Collaboration Beat a Single Big LLM?",
    "authors": [
      "Agata \u017bywot",
      "Xinyi Chen",
      "Maarten de Rijke"
    ],
    "summary": "This report studies whether small, tool-augmented agents can match or outperform larger monolithic models on the GAIA benchmark. Using Qwen3 models (4B-32B) within an adapted Agentic-Reasoning framework, we isolate the effects of model scale, explicit thinking (no thinking, planner-only, or full), and tool use (search, code, mind-map). Tool augmentation provides the largest and most consistent gains. Using tools, 4B models can outperform 32B models without tool access on GAIA in our experimental setup. In contrast, explicit thinking is highly configuration- and difficulty-dependent: planner-only thinking can improve decomposition and constraint tracking, while unrestricted full thinking often degrades performance by destabilizing tool orchestration, leading to skipped verification steps, excessive tool calls, non-termination, and output-format drift.",
    "link": "http://arxiv.org/abs/2601.11327v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11314v1",
    "title": "Membership Inference on LLMs in the Wild",
    "authors": [
      "Jiatong Yi",
      "Yanyang Li"
    ],
    "summary": "Membership Inference Attacks (MIAs) act as a crucial auditing tool for the opaque training data of Large Language Models (LLMs). However, existing techniques predominantly rely on inaccessible model internals (e.g., logits) or suffer from poor generalization across domains in strict black-box settings where only generated text is available. In this work, we propose SimMIA, a robust MIA framework tailored for this text-only regime by leveraging an advanced sampling strategy and scoring mechanism. Furthermore, we present WikiMIA-25, a new benchmark curated to evaluate MIA performance on modern proprietary LLMs. Experiments demonstrate that SimMIA achieves state-of-the-art results in the black-box setting, rivaling baselines that exploit internal model information.",
    "link": "http://arxiv.org/abs/2601.11314v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11311v1",
    "title": "FORESTLLM: Large Language Models Make Random Forest Great on Few-shot Tabular Learning",
    "authors": [
      "Zhihan Yang",
      "Jiaqi Wei",
      "Xiang Zhang",
      "Haoyu Dong",
      "Yiwen Wang",
      "Xiaoke Guo",
      "Pengkun Zhang",
      "Yiwei Xu",
      "Chenyu You"
    ],
    "summary": "Tabular data high-stakes critical decision-making in domains such as finance, healthcare, and scientific discovery. Yet, learning effectively from tabular data in few-shot settings, where labeled examples are scarce, remains a fundamental challenge. Traditional tree-based methods often falter in these regimes due to their reliance on statistical purity metrics, which become unstable and prone to overfitting with limited supervision. At the same time, direct applications of large language models (LLMs) often overlook its inherent structure, leading to suboptimal performance. To overcome these limitations, we propose FORESTLLM, a novel framework that unifies the structural inductive biases of decision forests with the semantic reasoning capabilities of LLMs. Crucially, FORESTLLM leverages the LLM only during training, treating it as an offline model designer that encodes rich, contextual knowledge into a lightweight, interpretable forest model, eliminating the need for LLM inference at test time. Our method is two-fold. First, we introduce a semantic splitting criterion in which the LLM evaluates candidate partitions based on their coherence over both labeled and unlabeled data, enabling the induction of more robust and generalizable tree structures under few-shot supervision. Second, we propose a one-time in-context inference mechanism for leaf node stabilization, where the LLM distills the decision path and its supporting examples into a concise, deterministic prediction, replacing noisy empirical estimates with semantically informed outputs. Across a diverse suite of few-shot classification and regression benchmarks, FORESTLLM achieves state-of-the-art performance.",
    "link": "http://arxiv.org/abs/2601.11311v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.06022v1",
    "title": "AdaFuse: Adaptive Ensemble Decoding with Test-Time Scaling for LLMs",
    "authors": [
      "Chengming Cui",
      "Tianxin Wei",
      "Ziyi Chen",
      "Ruizhong Qiu",
      "Zhichen Zeng",
      "Zhining Liu",
      "Xuying Ning",
      "Duo Zhou",
      "Jingrui He"
    ],
    "summary": "Large language models (LLMs) exhibit complementary strengths arising from differences in pretraining data, model architectures, and decoding behaviors. Inference-time ensembling provides a practical way to combine these capabilities without retraining. However, existing ensemble approaches suffer from fundamental limitations. Most rely on fixed fusion granularity, which lacks the flexibility required for mid-generation adaptation and fails to adapt to different generation characteristics across tasks. To address these challenges, we propose AdaFuse, an adaptive ensemble decoding framework that dynamically selects semantically appropriate fusion units during generation. Rather than committing to a fixed granularity, AdaFuse adjusts fusion behavior on the fly based on the decoding context, with words serving as basic building blocks for alignment. To be specific, we introduce an uncertainty-based criterion to decide whether to apply ensembling at each decoding step. Under confident decoding states, the model continues generation directly. In less certain states, AdaFuse invokes a diversity-aware scaling strategy to explore alternative candidate continuations and inform ensemble decisions. This design establishes a synergistic interaction between adaptive ensembling and test-time scaling, where ensemble decisions guide targeted exploration, and the resulting diversity in turn strengthens ensemble quality. Experiments on open-domain question answering, arithmetic reasoning, and machine translation demonstrate that AdaFuse consistently outperforms strong ensemble baselines, achieving an average relative improvement of 6.88%. The code is available at https://github.com/CCM0111/AdaFuse.",
    "link": "http://arxiv.org/abs/2601.06022v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.06021v1",
    "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
    "authors": [
      "Jiajie Zhang",
      "Xin Lv",
      "Ling Feng",
      "Lei Hou",
      "Juanzi Li"
    ],
    "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose \\textbf{Citation-aware Rubric Rewards (CaRR)}, a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce \\textbf{Citation-aware Group Relative Policy Optimization (C-GRPO)}, which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
    "link": "http://arxiv.org/abs/2601.06021v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.06007v1",
    "title": "Don't Break the Cache: An Evaluation of Prompt Caching for Long-Horizon Agentic Tasks",
    "authors": [
      "Elias Lumer",
      "Faheem Nizar",
      "Akshaya Jangiti",
      "Kevin Frank",
      "Anmol Gulati",
      "Mandar Phadate",
      "Vamse Kumar Subbiah"
    ],
    "summary": "Recent advancements in Large Language Model (LLM) agents have enabled complex multi-turn agentic tasks requiring extensive tool calling, where conversations can span dozens of API calls with increasingly large context windows. However, although major LLM providers offer prompt caching to reduce cost and latency, its benefits for agentic workloads remain underexplored in the research literature. To our knowledge, no prior work quantifies these cost savings or compares caching strategies for multi-turn agentic tasks. We present a comprehensive evaluation of prompt caching across three major LLM providers (OpenAI, Anthropic, and Google) and compare three caching strategies, including full context caching, system prompt only caching, and caching that excludes dynamic tool results. We evaluate on DeepResearchBench, a multi-turn agentic benchmark where agents autonomously execute real-world web search tool calls to answer complex research questions, measuring both API cost and time to first token (TTFT) across over 500 agent sessions with 10,000-token system prompts. Our results demonstrate that prompt caching reduces API costs by 45-80% and improves time to first token by 13-31% across providers. We find that strategic prompt cache block control, such as placing dynamic content at the end of the system prompt, avoiding dynamic traditional function calling, and excluding dynamic tool results, provides more consistent benefits than naive full-context caching, which can paradoxically increase latency. Our analysis reveals nuanced variations in caching behavior across providers, and we provide practical guidance for implementing prompt caching in production agentic systems.",
    "link": "http://arxiv.org/abs/2601.06007v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.06002v1",
    "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
    "authors": [
      "Qiguang Chen",
      "Yantao Du",
      "Ziniu Li",
      "Jinhao Liu",
      "Songyao Duan",
      "Jiarui Guo",
      "Minghao Liu",
      "Jiaheng Liu",
      "Tong Yang",
      "Ge Zhang",
      "Libo Qin",
      "Wanxiang Che",
      "Wenhao Huang"
    ],
    "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
    "link": "http://arxiv.org/abs/2601.06002v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05991v1",
    "title": "Open-Vocabulary 3D Instruction Ambiguity Detection",
    "authors": [
      "Jiayu Ding",
      "Haoran Tang",
      "Ge Li"
    ],
    "summary": "In safety-critical domains, linguistic ambiguity can have severe consequences; a vague command like \"Pass me the vial\" in a surgical setting could lead to catastrophic errors. Yet, most embodied AI research overlooks this, assuming instructions are clear and focusing on execution rather than confirmation. To address this critical safety gap, we are the first to define Open-Vocabulary 3D Instruction Ambiguity Detection, a fundamental new task where a model must determine if a command has a single, unambiguous meaning within a given 3D scene. To support this research, we build Ambi3D, the large-scale benchmark for this task, featuring over 700 diverse 3D scenes and around 22k instructions. Our analysis reveals a surprising limitation: state-of-the-art 3D Large Language Models (LLMs) struggle to reliably determine if an instruction is ambiguous. To address this challenge, we propose AmbiVer, a two-stage framework that collects explicit visual evidence from multiple views and uses it to guide an vision-language model (VLM) in judging instruction ambiguity. Extensive experiments demonstrate the challenge of our task and the effectiveness of AmbiVer, paving the way for safer and more trustworthy embodied AI. Code and dataset available at https://jiayuding031020.github.io/ambi3d/.",
    "link": "http://arxiv.org/abs/2601.05991v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05960v1",
    "title": "Distilling Feedback into Memory-as-a-Tool",
    "authors": [
      "V\u00edctor Gallego"
    ],
    "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.",
    "link": "http://arxiv.org/abs/2601.05960v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05943v1",
    "title": "Global Optimization for Combinatorial Geometry Problems",
    "authors": [
      "Timo Berthold",
      "Dominik Kamp",
      "Gioni Mexi",
      "Sebastian Pokutta",
      "Imre P\u00f3lik"
    ],
    "summary": "Recent progress in LLM-driven algorithm discovery, exemplified by DeepMind's AlphaEvolve, has produced new best-known solutions for a range of hard geometric and combinatorial problems. This raises a natural question: to what extent can modern off-the-shelf global optimization solvers match such results when the problems are formulated directly as nonlinear optimization problems (NLPs)?   We revisit a subset of problems from the AlphaEvolve benchmark suite and evaluate straightforward NLP formulations with two state-of-the-art solvers, the commercial FICO Xpress and the open-source SCIP. Without any solver modifications, both solvers reproduce, and in several cases improve upon, the best solutions previously reported in the literature, including the recent LLM-driven discoveries. Our results not only highlight the maturity of generic NLP technology and its ability to tackle nonlinear mathematical problems that were out of reach for general-purpose solvers only a decade ago, but also position global NLP solvers as powerful tools that may be exploited within LLM-driven algorithm discovery.",
    "link": "http://arxiv.org/abs/2601.05943v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05930v1",
    "title": "Can We Predict Before Executing Machine Learning Agents?",
    "authors": [
      "Jingsheng Zheng",
      "Jintian Zhang",
      "Yujie Luo",
      "Yuren Mao",
      "Yunjun Gao",
      "Lun Du",
      "Huajun Chen",
      "Ningyu Zhang"
    ],
    "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
    "link": "http://arxiv.org/abs/2601.05930v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05918v1",
    "title": "Agentic LLMs as Powerful Deanonymizers: Re-identification of Participants in the Anthropic Interviewer Dataset",
    "authors": [
      "Tianshi Li"
    ],
    "summary": "On December 4, 2025, Anthropic released Anthropic Interviewer, an AI tool for running qualitative interviews at scale, along with a public dataset of 1,250 interviews with professionals, including 125 scientists, about their use of AI for research. Focusing on the scientist subset, I show that widely available LLMs with web search and agentic capabilities can link six out of twenty-four interviews to specific scientific works, recovering associated authors and, in some cases, uniquely identifying the interviewees. My contribution is to show that modern LLM-based agents make such re-identification attacks easy and low-effort: off-the-shelf tools can, with a few natural-language prompts, search the web, cross-reference details, and propose likely matches, effectively lowering the technical barrier. Existing safeguards can be bypassed by breaking down the re-identification into benign tasks. I outline the attack at a high level, discuss implications for releasing rich qualitative data in the age of LLM agents, and propose mitigation recommendations and open problems. I have notified Anthropic of my findings.",
    "link": "http://arxiv.org/abs/2601.05918v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05905v1",
    "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
    "authors": [
      "Haoming Xu",
      "Ningyuan Zhao",
      "Yunzhi Yao",
      "Weihong Xu",
      "Hongru Wang",
      "Xinle Deng",
      "Shumin Deng",
      "Jeff Z. Pan",
      "Huajun Chen",
      "Ningyu Zhang"
    ],
    "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
    "link": "http://arxiv.org/abs/2601.05905v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05904v1",
    "title": "Can AI mediation improve democratic deliberation?",
    "authors": [
      "Michael Henry Tessler",
      "Georgina Evans",
      "Michiel A. Bakker",
      "Iason Gabriel",
      "Sophie Bridgers",
      "Rishub Jain",
      "Raphael Koster",
      "Verena Rieser",
      "Anca Dragan",
      "Matthew Botvinick",
      "Christopher Summerfield"
    ],
    "summary": "The strength of democracy lies in the free and equal exchange of diverse viewpoints. Living up to this ideal at scale faces inherent tensions: broad participation, meaningful deliberation, and political equality often trade off with one another (Fishkin, 2011). We ask whether and how artificial intelligence (AI) could help navigate this \"trilemma\" by engaging with a recent example of a large language model (LLM)-based system designed to help people with diverse viewpoints find common ground (Tessler, Bakker, et al., 2024). Here, we explore the implications of the introduction of LLMs into deliberation augmentation tools, examining their potential to enhance participation through scalability, improve political equality via fair mediation, and foster meaningful deliberation by, for example, surfacing trustworthy information. We also point to key challenges that remain. Ultimately, a range of empirical, technical, and theoretical advancements are needed to fully realize the promise of AI-mediated deliberation for enhancing citizen engagement and strengthening democratic deliberation.",
    "link": "http://arxiv.org/abs/2601.05904v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05903v1",
    "title": "HAPS: Hierarchical LLM Routing with Joint Architecture and Parameter Search",
    "authors": [
      "Zihang Tian",
      "Rui Li",
      "Jingsen Zhang",
      "Xiaohe Bo",
      "Wei Huo",
      "Xu Chen"
    ],
    "summary": "Large language model (LLM) routing aims to exploit the specialized strengths of different LLMs for diverse tasks. However, existing approaches typically focus on selecting LLM architectures while overlooking parameter settings, which are critical for task performance. In this paper, we introduce HAPS, a hierarchical LLM routing framework that jointly searches over model architectures and parameters. Specifically, we use a high-level router to select among candidate LLM architectures, and then search for the optimal parameters for the selected architectures based on a low-level router. We design a parameter generation network to share parameters between the two routers to mutually enhance their capabilities. In the training process, we design a reward-augmented objective to effectively optimize our framework. Experiments on two commonly used benchmarks show that HAPS consistently outperforms strong routing baselines. We have released our code at https://github.com/zihangtian/HAPS.",
    "link": "http://arxiv.org/abs/2601.05903v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05899v1",
    "title": "TowerMind: A Tower Defence Game Learning Environment and Benchmark for LLM as Agents",
    "authors": [
      "Dawei Wang",
      "Chengming Zhou",
      "Di Zhao",
      "Xinyuan Liu",
      "Marci Chi Ma",
      "Gary Ushaw",
      "Richard Davison"
    ],
    "summary": "Recent breakthroughs in Large Language Models (LLMs) have positioned them as a promising paradigm for agents, with long-term planning and decision-making emerging as core general-purpose capabilities for adapting to diverse scenarios and tasks. Real-time strategy (RTS) games serve as an ideal testbed for evaluating these two capabilities, as their inherent gameplay requires both macro-level strategic planning and micro-level tactical adaptation and action execution. Existing RTS game-based environments either suffer from relatively high computational demands or lack support for textual observations, which has constrained the use of RTS games for LLM evaluation. Motivated by this, we present TowerMind, a novel environment grounded in the tower defense (TD) subgenre of RTS games. TowerMind preserves the key evaluation strengths of RTS games for assessing LLMs, while featuring low computational demands and a multimodal observation space, including pixel-based, textual, and structured game-state representations. In addition, TowerMind supports the evaluation of model hallucination and provides a high degree of customizability. We design five benchmark levels to evaluate several widely used LLMs under different multimodal input settings. The results reveal a clear performance gap between LLMs and human experts across both capability and hallucination dimensions. The experiments further highlight key limitations in LLM behavior, such as inadequate planning validation, a lack of multifinality in decision-making, and inefficient action use. We also evaluate two classic reinforcement learning algorithms: Ape-X DQN and PPO. By offering a lightweight and multimodal design, TowerMind complements the existing RTS game-based environment landscape and introduces a new benchmark for the AI agent field. The source code is publicly available on GitHub(https://github.com/tb6147877/TowerMind).",
    "link": "http://arxiv.org/abs/2601.05899v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05887v1",
    "title": "Cybersecurity AI: A Game-Theoretic AI for Guiding Attack and Defense",
    "authors": [
      "V\u00edctor Mayoral-Vilches",
      "Mar\u00eda Sanz-G\u00f3mez",
      "Francesco Balassone",
      "Stefan Rass",
      "Lidia Salas-Espejo",
      "Benjamin Jablonski",
      "Luis Javier Navarrete-Lozano",
      "Maite del Mundo de Torres",
      "Crist\u00f3bal R. J. Veas Chavez"
    ],
    "summary": "AI-driven penetration testing now executes thousands of actions per hour but still lacks the strategic intuition humans apply in competitive security. To build cybersecurity superintelligence --Cybersecurity AI exceeding best human capability-such strategic intuition must be embedded into agentic reasoning processes. We present Generative Cut-the-Rope (G-CTR), a game-theoretic guidance layer that extracts attack graphs from agent's context, computes Nash equilibria with effort-aware scoring, and feeds a concise digest back into the LLM loop \\emph{guiding} the agent's actions. Across five real-world exercises, G-CTR matches 70--90% of expert graph structure while running 60--245x faster and over 140x cheaper than manual analysis. In a 44-run cyber-range, adding the digest lifts success from 20.0% to 42.9%, cuts cost-per-success by 2.7x, and reduces behavioral variance by 5.2x. In Attack-and-Defense exercises, a shared digest produces the Purple agent, winning roughly 2:1 over the LLM-only baseline and 3.7:1 over independently guided teams. This closed-loop guidance is what produces the breakthrough: it reduces ambiguity, collapses the LLM's search space, suppresses hallucinations, and keeps the model anchored to the most relevant parts of the problem, yielding large gains in success rate, consistency, and reliability.",
    "link": "http://arxiv.org/abs/2601.05887v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05879v1",
    "title": "Gender Bias in LLMs: Preliminary Evidence from Shared Parenting Scenario in Czech Family Law",
    "authors": [
      "Jakub Harasta",
      "Matej Vasina",
      "Martin Kornel",
      "Tomas Foltynek"
    ],
    "summary": "Access to justice remains limited for many people, leading laypersons to increasingly rely on Large Language Models (LLMs) for legal self-help. Laypeople use these tools intuitively, which may lead them to form expectations based on incomplete, incorrect, or biased outputs. This study examines whether leading LLMs exhibit gender bias in their responses to a realistic family law scenario. We present an expert-designed divorce scenario grounded in Czech family law and evaluate four state-of-the-art LLMs GPT-5 nano, Claude Haiku 4.5, Gemini 2.5 Flash, and Llama 3.3 in a fully zero-shot interaction. We deploy two versions of the scenario, one with gendered names and one with neutral labels, to establish a baseline for comparison. We further introduce nine legally relevant factors that vary the factual circumstances of the case and test whether these variations influence the models' proposed shared-parenting ratios. Our preliminary results highlight differences across models and suggest gender-dependent patterns in the outcomes generated by some systems. The findings underscore both the risks associated with laypeople's reliance on LLMs for legal guidance and the need for more robust evaluation of model behavior in sensitive legal contexts. We present exploratory and descriptive evidence intended to identify systematic asymmetries rather than to establish causal effects.",
    "link": "http://arxiv.org/abs/2601.05879v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05874v1",
    "title": "Continual-learning for Modelling Low-Resource Languages from Large Language Models",
    "authors": [
      "Santosh Srinath K",
      "Mudit Somani",
      "Varun Reddy Padala",
      "Prajna Devi Upadhyay",
      "Abhijit Das"
    ],
    "summary": "Modelling a language model for a multi-lingual scenario includes several potential challenges, among which catastrophic forgetting is the major challenge. For example, small language models (SLM) built for low-resource languages by adapting large language models (LLMs) pose the challenge of catastrophic forgetting. This work proposes to employ a continual learning strategy using parts-of-speech (POS)-based code-switching along with a replay adapter strategy to mitigate the identified gap of catastrophic forgetting while training SLM from LLM. Experiments conducted on vision language tasks such as visual question answering and language modelling task exhibits the success of the proposed architecture.",
    "link": "http://arxiv.org/abs/2601.05874v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05870v1",
    "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
    "authors": [
      "Huilin Deng",
      "Hongchen Luo",
      "Yue Zhu",
      "Long Li",
      "Zhuoyue Chen",
      "Xinghao Zhao",
      "Ming Li",
      "Jihai Zhang",
      "Mengchang Wang",
      "Yang Cao",
      "Yu Kang"
    ],
    "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
    "link": "http://arxiv.org/abs/2601.05870v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05858v1",
    "title": "CLewR: Curriculum Learning with Restarts for Machine Translation Preference Learning",
    "authors": [
      "Alexandra Dragomir",
      "Florin Brad",
      "Radu Tudor Ionescu"
    ],
    "summary": "Large language models (LLMs) have demonstrated competitive performance in zero-shot multilingual machine translation (MT). Some follow-up works further improved MT performance via preference optimization, but they leave a key aspect largely underexplored: the order in which data samples are given during training. We address this topic by integrating curriculum learning into various state-of-the-art preference optimization algorithms to boost MT performance. We introduce a novel curriculum learning strategy with restarts (CLewR), which reiterates easy-to-hard curriculum multiple times during training to effectively mitigate the catastrophic forgetting of easy examples. We demonstrate consistent gains across several model families (Gemma2, Qwen2.5, Llama3.1) and preference optimization techniques. We publicly release our code at https://github.com/alexandra-dragomir/CLewR.",
    "link": "http://arxiv.org/abs/2601.05858v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05835v1",
    "title": "Left, Right, or Center? Evaluating LLM Framing in News Classification and Generation",
    "authors": [
      "Molly Kennedy",
      "Ali Parker",
      "Yihong Liu",
      "Hinrich Sch\u00fctze"
    ],
    "summary": "Large Language Model (LLM) based summarization and text generation are increasingly used for producing and rewriting text, raising concerns about political framing in journalism where subtle wording choices can shape interpretation. Across nine state-of-the-art LLMs, we study political framing by testing whether LLMs' classification-based bias signals align with framing behavior in their generated summaries. We first compare few-shot ideology predictions against LEFT/CENTER/RIGHT labels. We then generate \"steered\" summaries under FAITHFUL, CENTRIST, LEFT, and RIGHT prompts, and score all outputs using a single fixed ideology evaluator. We find pervasive ideological center-collapse in both article-level ratings and generated text, indicating a systematic tendency toward centrist framing. Among evaluated models, Grok 4 is by far the most ideologically expressive generator, while Claude Sonnet 4.5 and Llama 3.1 achieve the strongest bias-rating performance among commercial and open-weight models, respectively.",
    "link": "http://arxiv.org/abs/2601.05835v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05827v1",
    "title": "SSR: Safeguarding Staking Rewards by Defining and Detecting Logical Defects in DeFi Staking",
    "authors": [
      "Zewei Lin",
      "Jiachi Chen",
      "Jingwen Zhang",
      "Zexu Wang",
      "Yuming Feng",
      "Weizhe Zhang",
      "Zibin Zheng"
    ],
    "summary": "Decentralized Finance (DeFi) staking is one of the most prominent applications within the DeFi ecosystem, where DeFi projects enable users to stake tokens on the platform and reward participants with additional tokens. However, logical defects in DeFi staking could enable attackers to claim unwarranted rewards by manipulating reward amounts, repeatedly claiming rewards, or engaging in other malicious actions. To mitigate these threats, we conducted the first study focused on defining and detecting logical defects in DeFi staking. Through the analysis of 64 security incidents and 144 audit reports, we identified six distinct types of logical defects, each accompanied by detailed descriptions and code examples. Building on this empirical research, we developed SSR (Safeguarding Staking Reward), a static analysis tool designed to detect logical defects in DeFi staking contracts. SSR utilizes a large language model (LLM) to extract fundamental information about staking logic and constructs a DeFi staking model. It then identifies logical defects by analyzing the model and the associated semantic features. We constructed a ground truth dataset based on known security incidents and audit reports to evaluate the effectiveness of SSR. The results indicate that SSR achieves an overall precision of 92.31%, a recall of 87.92%, and an F1-score of 88.85%. Additionally, to assess the prevalence of logical defects in real-world smart contracts, we compiled a large-scale dataset of 15,992 DeFi staking contracts. SSR detected that 3,557 (22.24%) of these contracts contained at least one logical defect.",
    "link": "http://arxiv.org/abs/2601.05827v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05215v1",
    "title": "MineNPC-Task: Task Suite for Memory-Aware Minecraft Agents",
    "authors": [
      "Tamil Sudaravan Mohan Doss",
      "Michael Xu",
      "Sudha Rao",
      "Andrew D. Wilson",
      "Balasaravanan Thoravi Kumaravel"
    ],
    "summary": "We present \\textsc{MineNPC-Task}, a user-authored benchmark and evaluation harness for testing memory-aware, mixed-initiative LLM agents in open-world \\emph{Minecraft}. Rather than relying on synthetic prompts, tasks are elicited from formative and summative co-play with expert players, normalized into parametric templates with explicit preconditions and dependency structure, and paired with machine-checkable validators under a bounded-knowledge policy that forbids out-of-world shortcuts. The harness captures plan/act/memory events-including plan previews, targeted clarifications, memory reads and writes, precondition checks, and repair attempts and reports outcomes relative to the total number of attempted subtasks, derived from in-world evidence.   As an initial snapshot, we instantiate the framework with GPT-4o and evaluate \\textbf{216} subtasks across \\textbf{8} experienced players. We observe recurring breakdown patterns in code execution, inventory/tool handling, referencing, and navigation, alongside recoveries supported by mixed-initiative clarifications and lightweight memory. Participants rated interaction quality and interface usability positively, while highlighting the need for stronger memory persistence across tasks. We release the complete task suite, validators, logs, and harness to support transparent, reproducible evaluation of future memory-aware embodied agents.",
    "link": "http://arxiv.org/abs/2601.05215v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05214v1",
    "title": "Internal Representations as Indicators of Hallucinations in Agent Tool Selection",
    "authors": [
      "Kait Healy",
      "Bharathi Srinivasan",
      "Visakh Madathil",
      "Jing Wu"
    ],
    "summary": "Large Language Models (LLMs) have shown remarkable capabilities in tool calling and tool usage, but suffer from hallucinations where they choose incorrect tools, provide malformed parameters and exhibit 'tool bypass' behavior by performing simulations and generating outputs instead of invoking specialized tools or external systems. This undermines the reliability of LLM based agents in production systems as it leads to inconsistent results, and bypasses security and audit controls. Such hallucinations in agent tool selection require early detection and error handling. Unlike existing hallucination detection methods that require multiple forward passes or external validation, we present a computationally efficient framework that detects tool-calling hallucinations in real-time by leveraging LLMs' internal representations during the same forward pass used for generation. We evaluate this approach on reasoning tasks across multiple domains, demonstrating strong detection performance (up to 86.4\\% accuracy) while maintaining real-time inference capabilities with minimal computational overhead, particularly excelling at detecting parameter-level hallucinations and inappropriate tool selections, critical for reliable agent deployment.",
    "link": "http://arxiv.org/abs/2601.05214v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05192v1",
    "title": "LELA: an LLM-based Entity Linking Approach with Zero-Shot Domain Adaptation",
    "authors": [
      "Samy Haffoudhi",
      "Fabian M. Suchanek",
      "Nils Holzenberger"
    ],
    "summary": "Entity linking (mapping ambiguous mentions in text to entities in a knowledge base) is a foundational step in tasks such as knowledge graph construction, question-answering, and information extraction. Our method, LELA, is a modular coarse-to-fine approach that leverages the capabilities of large language models (LLMs), and works with different target domains, knowledge bases and LLMs, without any fine-tuning phase. Our experiments across various entity linking settings show that LELA is highly competitive with fine-tuned approaches, and substantially outperforms the non-fine-tuned ones.",
    "link": "http://arxiv.org/abs/2601.05192v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05187v1",
    "title": "SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning",
    "authors": [
      "Yanchang Liang",
      "Xiaowei Zhao"
    ],
    "summary": "Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.",
    "link": "http://arxiv.org/abs/2601.05187v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05184v1",
    "title": "Observations and Remedies for Large Language Model Bias in Self-Consuming Performative Loop",
    "authors": [
      "Yaxuan Wang",
      "Zhongteng Cai",
      "Yujia Bao",
      "Xueru Zhang",
      "Yang Liu"
    ],
    "summary": "The rapid advancement of large language models (LLMs) has led to growing interest in using synthetic data to train future models. However, this creates a self-consuming retraining loop, where models are trained on their own outputs and may cause performance drops and induce emerging biases. In real-world applications, previously deployed LLMs may influence the data they generate, leading to a dynamic system driven by user feedback. For example, if a model continues to underserve users from a group, less query data will be collected from this particular demographic of users. In this study, we introduce the concept of \\textbf{S}elf-\\textbf{C}onsuming \\textbf{P}erformative \\textbf{L}oop (\\textbf{SCPL}) and investigate the role of synthetic data in shaping bias during these dynamic iterative training processes under controlled performative feedback. This controlled setting is motivated by the inaccessibility of real-world user preference data from dynamic production systems, and enables us to isolate and analyze feedback-driven bias evolution in a principled manner. We focus on two types of loops, including the typical retraining setting and the incremental fine-tuning setting, which is largely underexplored. Through experiments on three real-world tasks, we find that the performative loop increases preference bias and decreases disparate bias. We design a reward-based rejection sampling strategy to mitigate the bias, moving towards more trustworthy self-improving systems.",
    "link": "http://arxiv.org/abs/2601.05184v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05172v1",
    "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
    "authors": [
      "Haoyu Zhao",
      "Akide Liu",
      "Zeyu Zhang",
      "Weijie Wang",
      "Feng Chen",
      "Ruihan Zhu",
      "Gholamreza Haffari",
      "Bohan Zhuang"
    ],
    "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.   We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
    "link": "http://arxiv.org/abs/2601.05172v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05170v1",
    "title": "Reverse-engineering NLI: A study of the meta-inferential properties of Natural Language Inference",
    "authors": [
      "Rasmus Blanck",
      "Bill Noble",
      "Stergios Chatzikyriakidis"
    ],
    "summary": "Natural Language Inference (NLI) has been an important task for evaluating language models for Natural Language Understanding, but the logical properties of the task are poorly understood and often mischaracterized. Understanding the notion of inference captured by NLI is key to interpreting model performance on the task. In this paper we formulate three possible readings of the NLI label set and perform a comprehensive analysis of the meta-inferential properties they entail. Focusing on the SNLI dataset, we exploit (1) NLI items with shared premises and (2) items generated by LLMs to evaluate models trained on SNLI for meta-inferential consistency and derive insights into which reading of the logical relations is encoded by the dataset.",
    "link": "http://arxiv.org/abs/2601.05170v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05167v1",
    "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
    "authors": [
      "Chengsong Huang",
      "Tong Zheng",
      "Langlin Huang",
      "Jinyuan Li",
      "Haolin Liu",
      "Jiaxin Huang"
    ],
    "summary": "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.",
    "link": "http://arxiv.org/abs/2601.05167v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05162v1",
    "title": "GenAI-DrawIO-Creator: A Framework for Automated Diagram Generation",
    "authors": [
      "Jinze Yu",
      "Dayuan Jiang"
    ],
    "summary": "Diagrams are crucial for communicating complex information, yet creating and modifying them remains a labor-intensive task. We present GenAI-DrawIO-Creator, a novel framework that leverages Large Language Models (LLMs) to automate diagram generation and manipulation in the structured XML format used by draw.io. Our system integrates Claude 3.7 to reason about structured visual data and produce valid diagram representations. Key contributions include a high-level system design enabling real-time diagram updates, specialized prompt engineering and error-checking to ensure well-formed XML outputs. We demonstrate a working prototype capable of generating accurate diagrams (such as network architectures and flowcharts) from natural language or code, and even replicating diagrams from images. Simulated evaluations show that our approach significantly reduces diagram creation time and produces outputs with high structural fidelity. Our results highlight the promise of Claude 3.7 in handling structured visual reasoning tasks and lay the groundwork for future research in AI-assisted diagramming applications.",
    "link": "http://arxiv.org/abs/2601.05162v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05144v1",
    "title": "Distilling the Thought, Watermarking the Answer: A Principle Semantic Guided Watermark for Large Reasoning Models",
    "authors": [
      "Shuliang Liu",
      "Xingyu Li",
      "Hongyi Liu",
      "Yibo Yan",
      "Bingchen Duan",
      "Qi Zheng",
      "Dong Fang",
      "Lingfeng Su",
      "Xuming Hu"
    ],
    "summary": "Reasoning Large Language Models (RLLMs) excelling in complex tasks present unique challenges for digital watermarking, as existing methods often disrupt logical coherence or incur high computational costs. Token-based watermarking techniques can corrupt the reasoning flow by applying pseudo-random biases, while semantic-aware approaches improve quality but introduce significant latency or require auxiliary models. This paper introduces ReasonMark, a novel watermarking framework specifically designed for reasoning-intensive LLMs. Our approach decouples generation into an undisturbed Thinking Phase and a watermarked Answering Phase. We propose a Criticality Score to identify semantically pivotal tokens from the reasoning trace, which are distilled into a Principal Semantic Vector (PSV). The PSV then guides a semantically-adaptive mechanism that modulates watermark strength based on token-PSV alignment, ensuring robustness without compromising logical integrity. Extensive experiments show ReasonMark surpasses state-of-the-art methods by reducing text Perplexity by 0.35, increasing translation BLEU score by 0.164, and raising mathematical accuracy by 0.67 points. These advancements are achieved alongside a 0.34% higher watermark detection AUC and stronger robustness to attacks, all with a negligible increase in latency. This work enables the traceable and trustworthy deployment of reasoning LLMs in real-world applications.",
    "link": "http://arxiv.org/abs/2601.05144v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05114v1",
    "title": "Evaluative Fingerprints: Stable and Systematic Differences in LLM Evaluator Behavior",
    "authors": [
      "Wajid Nasser"
    ],
    "summary": "LLM-as-judge systems promise scalable, consistent evaluation. We find the opposite: judges are consistent, but not with each other; they are consistent with themselves. Across 3,240 evaluations (9 judges x 120 unique video x pack items x 3 independent runs), inter-judge agreement is near-zero (Krippendorff's \u03b1 = 0.042). On two dimensions, judges disagree more than random noise would predict (\u03b1 < 0). Yet this disagreement isn't chaos; it's structured. A classifier identifies which judge produced an evaluation with 77.1% accuracy from rubric scores alone, rising to 89.9% with disposition features. Within model families, the signal is even stronger: GPT-4.1 and GPT-5.2 are distinguishable with 99.6% accuracy. We call this the reliability paradox: judges cannot agree on what constitutes quality, yet their disagreement patterns are so stable they function as fingerprints. Each judge implements a distinct, stable theory of quality: an \"evaluative disposition\" that shapes how it interprets any rubric. We characterize these dispositions along multiple axes: harshness/leniency, dimension emphasis, within-judge stability (ICC), and evidence behavior (receipt validity, semantic linkage via NLI, and shotgun index). The implication is stark: LLM judges are not interchangeable instruments measuring a shared construct. They are distinct measurement devices, each encoding its own implicit theory of quality. Averaging their scores produces a synthetic verdict that corresponds to no judge's actual values.",
    "link": "http://arxiv.org/abs/2601.05114v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05111v1",
    "title": "Agent-as-a-Judge",
    "authors": [
      "Runyang You",
      "Hongru Cai",
      "Caiqi Zhang",
      "Qiancheng Xu",
      "Meng Liu",
      "Tiezheng Yu",
      "Yongqi Li",
      "Wenjie Li"
    ],
    "summary": "LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.",
    "link": "http://arxiv.org/abs/2601.05111v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05109v1",
    "title": "Nalar: An agent serving framework",
    "authors": [
      "Marco Laju",
      "Donghyun Son",
      "Saurabh Agarwal",
      "Nitin Kedia",
      "Myungjin Lee",
      "Jayanth Srinivasa",
      "Aditya Akella"
    ],
    "summary": "LLM-driven agentic applications increasingly automate complex, multi-step tasks, but serving them efficiently remains challenging due to heterogeneous components, dynamic and model-driven control flow, long-running state, and unpredictable latencies. Nalar is a ground-up agent-serving framework that cleanly separates workflow specification from execution while providing the runtime visibility and control needed for robust performance. Nalar preserves full Python expressiveness, using lightweight auto-generated stubs that turn agent and tool invocations into futures carrying dependency and context metadata. A managed state layer decouples logical state from physical placement, enabling safe reuse, migration, and consistent retry behavior. A two-level control architecture combines global policy computation with local event-driven enforcement to support adaptive routing, scheduling, and resource management across evolving workflows. Together, these mechanisms allow Nalar to deliver scalable, efficient, and policy-driven serving of heterogeneous agentic applications without burdening developers with orchestration logic. Across three agentic workloads, Nalar cuts tail latency by 34--74\\%, achieves up to $2.9\\times$ speedups, sustains 80 RPS where baselines fail, and scales to 130K futures with sub-500 ms control overhead.",
    "link": "http://arxiv.org/abs/2601.05109v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05107v1",
    "title": "Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction",
    "authors": [
      "Muzhao Tian",
      "Zisu Huang",
      "Xiaohua Wang",
      "Jingwen Xu",
      "Zhengkang Guo",
      "Qi Qian",
      "Yuanzhe Shen",
      "Kaitao Song",
      "Jiakang Yuan",
      "Changze Lv",
      "Xiaoqing Zheng"
    ],
    "summary": "As LLM-based agents are increasingly used in long-term interactions, cumulative memory is critical for enabling personalization and maintaining stylistic consistency. However, most existing systems adopt an ``all-or-nothing'' approach to memory usage: incorporating all relevant past information can lead to \\textit{Memory Anchoring}, where the agent is trapped by past interactions, while excluding memory entirely results in under-utilization and the loss of important interaction history. We show that an agent's reliance on memory can be modeled as an explicit and user-controllable dimension. We first introduce a behavioral metric of memory dependence to quantify the influence of past interactions on current outputs. We then propose \\textbf{Stee}rable \\textbf{M}emory Agent, \\texttt{SteeM}, a framework that allows users to dynamically regulate memory reliance, ranging from a fresh-start mode that promotes innovation to a high-fidelity mode that closely follows interaction history. Experiments across different scenarios demonstrate that our approach consistently outperforms conventional prompting and rigid memory masking strategies, yielding a more nuanced and effective control for personalized human-agent collaboration.",
    "link": "http://arxiv.org/abs/2601.05107v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05106v1",
    "title": "Token-Level LLM Collaboration via FusionRoute",
    "authors": [
      "Nuoya Xiong",
      "Yuhang Zhou",
      "Hanqing Zeng",
      "Zhaorun Chen",
      "Furong Huang",
      "Shuchao Bi",
      "Lizhu Zhang",
      "Zhuokai Zhao"
    ],
    "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
    "link": "http://arxiv.org/abs/2601.05106v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05103v1",
    "title": "Semantically Orthogonal Framework for Citation Classification: Disentangling Intent and Content",
    "authors": [
      "Changxu Duan",
      "Zhiyin Tan"
    ],
    "summary": "Understanding the role of citations is essential for research assessment and citation-aware digital libraries. However, existing citation classification frameworks often conflate citation intent (why a work is cited) with cited content type (what part is cited), limiting their effectiveness in auto classification due to a dilemma between fine-grained type distinctions and practical classification reliability. We introduce SOFT, a Semantically Orthogonal Framework with Two dimensions that explicitly separates citation intent from cited content type, drawing inspiration from semantic role theory. We systematically re-annotate the ACL-ARC dataset using SOFT and release a cross-disciplinary test set sampled from ACT2. Evaluation with both zero-shot and fine-tuned Large Language Models demonstrates that SOFT enables higher agreement between human annotators and LLMs, and supports stronger classification performance and robust cross-domain generalization compared to ACL-ARC and SciCite annotation frameworks. These results confirm SOFT's value as a clear, reusable annotation standard, improving clarity, consistency, and generalizability for digital libraries and scholarly communication infrastructures. All code and data are publicly available on GitHub https://github.com/zhiyintan/SOFT.",
    "link": "http://arxiv.org/abs/2601.05103v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05101v1",
    "title": "Arabic Prompts with English Tools: A Benchmark",
    "authors": [
      "Konstantin Kubrak",
      "Ahmed El-Moselhy",
      "Ammar Alsulami",
      "Remaz Altuwaim",
      "Hassan Ismail Fawaz",
      "Faisal Alsaby"
    ],
    "summary": "Large Language Models (LLMs) are now integral to numerous industries, increasingly serving as the core reasoning engine for autonomous agents that perform complex tasks through tool-use. While the development of Arabic-native LLMs is accelerating, the benchmarks for evaluating their capabilities lag behind, with most existing frameworks focusing on English. A critical and overlooked area is tool-calling, where the performance of models prompted in non-English languages like Arabic is poorly understood, especially since these models are often pretrained on predominantly English data. This paper addresses this critical gap by introducing the first dedicated benchmark for evaluating the tool-calling and agentic capabilities of LLMs in the Arabic language. Our work provides a standardized framework to measure the functional accuracy and robustness of models in Arabic agentic workflows. Our findings reveal a huge performance gap: when users interact in Arabic, tool-calling accuracy drops by an average of 5-10\\%, regardless of whether the tool descriptions themselves are in Arabic or English. By shedding light on these critical challenges, this benchmark aims to foster the development of more reliable and linguistically equitable AI agents for Arabic-speaking users.",
    "link": "http://arxiv.org/abs/2601.05101v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05075v1",
    "title": "SemPA: Improving Sentence Embeddings of Large Language Models through Semantic Preference Alignment",
    "authors": [
      "Ziyang Chen",
      "Zhenxuan Huang",
      "Yile Wang",
      "Weiqin Wang",
      "Lu Yin",
      "Hui Huang"
    ],
    "summary": "Traditional sentence embedding methods employ token-level contrastive learning on non-generative pre-trained models. Recently, there have emerged embedding methods based on generative large language models (LLMs). These methods either rely on fixed prompt templates or involve modifications to the model architecture. The former lacks further optimization of the model and results in limited performance, while the latter alters the internal computational mechanisms of the model, thereby compromising its generative capabilities. We propose SemPA, a novel approach that boosts the sentence representations while preserving the generative ability of LLMs via semantic preference alignment. We leverage sentence-level Direct Preference Optimization (DPO) to efficiently optimize LLMs on a paraphrase generation task, where the model learns to discriminate semantically equivalent sentences while preserving inherent generative capacity. Theoretically, we establish a formal connection between DPO and contrastive learning under the Plackett-Luce model framework. Empirically, experimental results on both semantic textual similarity tasks and various benchmarks for LLMs show that SemPA achieves better semantic representations without sacrificing the inherent generation capability of LLMs.",
    "link": "http://arxiv.org/abs/2601.05075v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05062v1",
    "title": "Compositional Steering of Large Language Models with Steering Tokens",
    "authors": [
      "Gorjan Radevski",
      "Kiril Gashteovski",
      "Giwon Hong",
      "Carolin Lawrence",
      "Goran Glava\u0161"
    ],
    "summary": "Deploying LLMs in real-world applications requires controllable output that satisfies multiple desiderata at the same time. While existing work extensively addresses LLM steering for a single behavior, \\textit{compositional steering} -- i.e., steering LLMs simultaneously towards multiple behaviors -- remains an underexplored problem. In this work, we propose \\emph{compositional steering tokens} for multi-behavior steering. We first embed individual behaviors, expressed as natural language instructions, into dedicated tokens via self-distillation. Contrary to most prior work, which operates in the activation space, our behavior steers live in the space of input tokens, enabling more effective zero-shot composition. We then train a dedicated \\textit{composition token} on pairs of behaviors and show that it successfully captures the notion of composition: it generalizes well to \\textit{unseen} compositions, including those with unseen behaviors as well as those with an unseen \\textit{number} of behaviors. Our experiments across different LLM architectures show that steering tokens lead to superior multi-behavior control compared to competing approaches (instructions, activation steering, and LoRA merging). Moreover, we show that steering tokens complement natural language instructions, with their combination resulting in further gains.",
    "link": "http://arxiv.org/abs/2601.05062v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05053v1",
    "title": "Reinforced Efficient Reasoning via Semantically Diverse Exploration",
    "authors": [
      "Ziqi Zhao",
      "Zhaochun Ren",
      "Jiahong Zou",
      "Liu Yang",
      "Zhiwei Xu",
      "Xuri Ge",
      "Zhumin Chen",
      "Xinyu Ma",
      "Daiting Shi",
      "Shuaiqiang Wang",
      "Dawei Yin",
      "Xin Xin"
    ],
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has proven effective in enhancing the reasoning of large language models (LLMs). Monte Carlo Tree Search (MCTS)-based extensions improve upon vanilla RLVR (e.g., GRPO) by providing tree-based reasoning rollouts that enable fine-grained and segment-level credit assignment. However, existing methods still suffer from limited exploration diversity and inefficient reasoning. To address the above challenges, we propose reinforced efficient reasoning via semantically diverse explorations, i.e., ROSE, for LLMs. To encourage more diverse reasoning exploration, our method incorporates a semantic-entropy-based branching strategy and an $\\varepsilon$-exploration mechanism. The former operates on already sampled reasoning rollouts to capture semantic uncertainty and select branching points with high semantic divergence to generate new successive reasoning paths, whereas the latter stochastically initiates reasoning rollouts from the root, preventing the search process from becoming overly local. To improve efficiency, we design a length-aware segment-level advantage estimator that rewards concise and correct reasoning while penalizing unnecessarily long reasoning chains. Extensive experiments on various mathematical reasoning benchmarks with Qwen and Llama models validate the effectiveness and efficiency of ROSE. Codes are available at https://github.com/ZiqiZhao1/ROSE-rl.",
    "link": "http://arxiv.org/abs/2601.05053v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  }
]