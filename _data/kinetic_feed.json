[
  {
    "id": "http://arxiv.org/abs/2601.05215v1",
    "title": "MineNPC-Task: Task Suite for Memory-Aware Minecraft Agents",
    "authors": [
      "Tamil Sudaravan Mohan Doss",
      "Michael Xu",
      "Sudha Rao",
      "Andrew D. Wilson",
      "Balasaravanan Thoravi Kumaravel"
    ],
    "summary": "We present \\textsc{MineNPC-Task}, a user-authored benchmark and evaluation harness for testing memory-aware, mixed-initiative LLM agents in open-world \\emph{Minecraft}. Rather than relying on synthetic prompts, tasks are elicited from formative and summative co-play with expert players, normalized into parametric templates with explicit preconditions and dependency structure, and paired with machine-checkable validators under a bounded-knowledge policy that forbids out-of-world shortcuts. The harness captures plan/act/memory events-including plan previews, targeted clarifications, memory reads and writes, precondition checks, and repair attempts and reports outcomes relative to the total number of attempted subtasks, derived from in-world evidence.   As an initial snapshot, we instantiate the framework with GPT-4o and evaluate \\textbf{216} subtasks across \\textbf{8} experienced players. We observe recurring breakdown patterns in code execution, inventory/tool handling, referencing, and navigation, alongside recoveries supported by mixed-initiative clarifications and lightweight memory. Participants rated interaction quality and interface usability positively, while highlighting the need for stronger memory persistence across tasks. We release the complete task suite, validators, logs, and harness to support transparent, reproducible evaluation of future memory-aware embodied agents.",
    "link": "http://arxiv.org/abs/2601.05215v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05214v1",
    "title": "Internal Representations as Indicators of Hallucinations in Agent Tool Selection",
    "authors": [
      "Kait Healy",
      "Bharathi Srinivasan",
      "Visakh Madathil",
      "Jing Wu"
    ],
    "summary": "Large Language Models (LLMs) have shown remarkable capabilities in tool calling and tool usage, but suffer from hallucinations where they choose incorrect tools, provide malformed parameters and exhibit 'tool bypass' behavior by performing simulations and generating outputs instead of invoking specialized tools or external systems. This undermines the reliability of LLM based agents in production systems as it leads to inconsistent results, and bypasses security and audit controls. Such hallucinations in agent tool selection require early detection and error handling. Unlike existing hallucination detection methods that require multiple forward passes or external validation, we present a computationally efficient framework that detects tool-calling hallucinations in real-time by leveraging LLMs' internal representations during the same forward pass used for generation. We evaluate this approach on reasoning tasks across multiple domains, demonstrating strong detection performance (up to 86.4\\% accuracy) while maintaining real-time inference capabilities with minimal computational overhead, particularly excelling at detecting parameter-level hallucinations and inappropriate tool selections, critical for reliable agent deployment.",
    "link": "http://arxiv.org/abs/2601.05214v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05192v1",
    "title": "LELA: an LLM-based Entity Linking Approach with Zero-Shot Domain Adaptation",
    "authors": [
      "Samy Haffoudhi",
      "Fabian M. Suchanek",
      "Nils Holzenberger"
    ],
    "summary": "Entity linking (mapping ambiguous mentions in text to entities in a knowledge base) is a foundational step in tasks such as knowledge graph construction, question-answering, and information extraction. Our method, LELA, is a modular coarse-to-fine approach that leverages the capabilities of large language models (LLMs), and works with different target domains, knowledge bases and LLMs, without any fine-tuning phase. Our experiments across various entity linking settings show that LELA is highly competitive with fine-tuned approaches, and substantially outperforms the non-fine-tuned ones.",
    "link": "http://arxiv.org/abs/2601.05192v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05187v1",
    "title": "SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning",
    "authors": [
      "Yanchang Liang",
      "Xiaowei Zhao"
    ],
    "summary": "Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.",
    "link": "http://arxiv.org/abs/2601.05187v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05184v1",
    "title": "Observations and Remedies for Large Language Model Bias in Self-Consuming Performative Loop",
    "authors": [
      "Yaxuan Wang",
      "Zhongteng Cai",
      "Yujia Bao",
      "Xueru Zhang",
      "Yang Liu"
    ],
    "summary": "The rapid advancement of large language models (LLMs) has led to growing interest in using synthetic data to train future models. However, this creates a self-consuming retraining loop, where models are trained on their own outputs and may cause performance drops and induce emerging biases. In real-world applications, previously deployed LLMs may influence the data they generate, leading to a dynamic system driven by user feedback. For example, if a model continues to underserve users from a group, less query data will be collected from this particular demographic of users. In this study, we introduce the concept of \\textbf{S}elf-\\textbf{C}onsuming \\textbf{P}erformative \\textbf{L}oop (\\textbf{SCPL}) and investigate the role of synthetic data in shaping bias during these dynamic iterative training processes under controlled performative feedback. This controlled setting is motivated by the inaccessibility of real-world user preference data from dynamic production systems, and enables us to isolate and analyze feedback-driven bias evolution in a principled manner. We focus on two types of loops, including the typical retraining setting and the incremental fine-tuning setting, which is largely underexplored. Through experiments on three real-world tasks, we find that the performative loop increases preference bias and decreases disparate bias. We design a reward-based rejection sampling strategy to mitigate the bias, moving towards more trustworthy self-improving systems.",
    "link": "http://arxiv.org/abs/2601.05184v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05172v1",
    "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
    "authors": [
      "Haoyu Zhao",
      "Akide Liu",
      "Zeyu Zhang",
      "Weijie Wang",
      "Feng Chen",
      "Ruihan Zhu",
      "Gholamreza Haffari",
      "Bohan Zhuang"
    ],
    "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.   We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
    "link": "http://arxiv.org/abs/2601.05172v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05170v1",
    "title": "Reverse-engineering NLI: A study of the meta-inferential properties of Natural Language Inference",
    "authors": [
      "Rasmus Blanck",
      "Bill Noble",
      "Stergios Chatzikyriakidis"
    ],
    "summary": "Natural Language Inference (NLI) has been an important task for evaluating language models for Natural Language Understanding, but the logical properties of the task are poorly understood and often mischaracterized. Understanding the notion of inference captured by NLI is key to interpreting model performance on the task. In this paper we formulate three possible readings of the NLI label set and perform a comprehensive analysis of the meta-inferential properties they entail. Focusing on the SNLI dataset, we exploit (1) NLI items with shared premises and (2) items generated by LLMs to evaluate models trained on SNLI for meta-inferential consistency and derive insights into which reading of the logical relations is encoded by the dataset.",
    "link": "http://arxiv.org/abs/2601.05170v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05167v1",
    "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
    "authors": [
      "Chengsong Huang",
      "Tong Zheng",
      "Langlin Huang",
      "Jinyuan Li",
      "Haolin Liu",
      "Jiaxin Huang"
    ],
    "summary": "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.",
    "link": "http://arxiv.org/abs/2601.05167v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05162v1",
    "title": "GenAI-DrawIO-Creator: A Framework for Automated Diagram Generation",
    "authors": [
      "Jinze Yu",
      "Dayuan Jiang"
    ],
    "summary": "Diagrams are crucial for communicating complex information, yet creating and modifying them remains a labor-intensive task. We present GenAI-DrawIO-Creator, a novel framework that leverages Large Language Models (LLMs) to automate diagram generation and manipulation in the structured XML format used by draw.io. Our system integrates Claude 3.7 to reason about structured visual data and produce valid diagram representations. Key contributions include a high-level system design enabling real-time diagram updates, specialized prompt engineering and error-checking to ensure well-formed XML outputs. We demonstrate a working prototype capable of generating accurate diagrams (such as network architectures and flowcharts) from natural language or code, and even replicating diagrams from images. Simulated evaluations show that our approach significantly reduces diagram creation time and produces outputs with high structural fidelity. Our results highlight the promise of Claude 3.7 in handling structured visual reasoning tasks and lay the groundwork for future research in AI-assisted diagramming applications.",
    "link": "http://arxiv.org/abs/2601.05162v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05144v1",
    "title": "Distilling the Thought, Watermarking the Answer: A Principle Semantic Guided Watermark for Large Reasoning Models",
    "authors": [
      "Shuliang Liu",
      "Xingyu Li",
      "Hongyi Liu",
      "Yibo Yan",
      "Bingchen Duan",
      "Qi Zheng",
      "Dong Fang",
      "Lingfeng Su",
      "Xuming Hu"
    ],
    "summary": "Reasoning Large Language Models (RLLMs) excelling in complex tasks present unique challenges for digital watermarking, as existing methods often disrupt logical coherence or incur high computational costs. Token-based watermarking techniques can corrupt the reasoning flow by applying pseudo-random biases, while semantic-aware approaches improve quality but introduce significant latency or require auxiliary models. This paper introduces ReasonMark, a novel watermarking framework specifically designed for reasoning-intensive LLMs. Our approach decouples generation into an undisturbed Thinking Phase and a watermarked Answering Phase. We propose a Criticality Score to identify semantically pivotal tokens from the reasoning trace, which are distilled into a Principal Semantic Vector (PSV). The PSV then guides a semantically-adaptive mechanism that modulates watermark strength based on token-PSV alignment, ensuring robustness without compromising logical integrity. Extensive experiments show ReasonMark surpasses state-of-the-art methods by reducing text Perplexity by 0.35, increasing translation BLEU score by 0.164, and raising mathematical accuracy by 0.67 points. These advancements are achieved alongside a 0.34% higher watermark detection AUC and stronger robustness to attacks, all with a negligible increase in latency. This work enables the traceable and trustworthy deployment of reasoning LLMs in real-world applications.",
    "link": "http://arxiv.org/abs/2601.05144v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05114v1",
    "title": "Evaluative Fingerprints: Stable and Systematic Differences in LLM Evaluator Behavior",
    "authors": [
      "Wajid Nasser"
    ],
    "summary": "LLM-as-judge systems promise scalable, consistent evaluation. We find the opposite: judges are consistent, but not with each other; they are consistent with themselves. Across 3,240 evaluations (9 judges x 120 unique video x pack items x 3 independent runs), inter-judge agreement is near-zero (Krippendorff's \u03b1 = 0.042). On two dimensions, judges disagree more than random noise would predict (\u03b1 < 0). Yet this disagreement isn't chaos; it's structured. A classifier identifies which judge produced an evaluation with 77.1% accuracy from rubric scores alone, rising to 89.9% with disposition features. Within model families, the signal is even stronger: GPT-4.1 and GPT-5.2 are distinguishable with 99.6% accuracy. We call this the reliability paradox: judges cannot agree on what constitutes quality, yet their disagreement patterns are so stable they function as fingerprints. Each judge implements a distinct, stable theory of quality: an \"evaluative disposition\" that shapes how it interprets any rubric. We characterize these dispositions along multiple axes: harshness/leniency, dimension emphasis, within-judge stability (ICC), and evidence behavior (receipt validity, semantic linkage via NLI, and shotgun index). The implication is stark: LLM judges are not interchangeable instruments measuring a shared construct. They are distinct measurement devices, each encoding its own implicit theory of quality. Averaging their scores produces a synthetic verdict that corresponds to no judge's actual values.",
    "link": "http://arxiv.org/abs/2601.05114v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05111v1",
    "title": "Agent-as-a-Judge",
    "authors": [
      "Runyang You",
      "Hongru Cai",
      "Caiqi Zhang",
      "Qiancheng Xu",
      "Meng Liu",
      "Tiezheng Yu",
      "Yongqi Li",
      "Wenjie Li"
    ],
    "summary": "LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.",
    "link": "http://arxiv.org/abs/2601.05111v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05109v1",
    "title": "Nalar: An agent serving framework",
    "authors": [
      "Marco Laju",
      "Donghyun Son",
      "Saurabh Agarwal",
      "Nitin Kedia",
      "Myungjin Lee",
      "Jayanth Srinivasa",
      "Aditya Akella"
    ],
    "summary": "LLM-driven agentic applications increasingly automate complex, multi-step tasks, but serving them efficiently remains challenging due to heterogeneous components, dynamic and model-driven control flow, long-running state, and unpredictable latencies. Nalar is a ground-up agent-serving framework that cleanly separates workflow specification from execution while providing the runtime visibility and control needed for robust performance. Nalar preserves full Python expressiveness, using lightweight auto-generated stubs that turn agent and tool invocations into futures carrying dependency and context metadata. A managed state layer decouples logical state from physical placement, enabling safe reuse, migration, and consistent retry behavior. A two-level control architecture combines global policy computation with local event-driven enforcement to support adaptive routing, scheduling, and resource management across evolving workflows. Together, these mechanisms allow Nalar to deliver scalable, efficient, and policy-driven serving of heterogeneous agentic applications without burdening developers with orchestration logic. Across three agentic workloads, Nalar cuts tail latency by 34--74\\%, achieves up to $2.9\\times$ speedups, sustains 80 RPS where baselines fail, and scales to 130K futures with sub-500 ms control overhead.",
    "link": "http://arxiv.org/abs/2601.05109v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05107v1",
    "title": "Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction",
    "authors": [
      "Muzhao Tian",
      "Zisu Huang",
      "Xiaohua Wang",
      "Jingwen Xu",
      "Zhengkang Guo",
      "Qi Qian",
      "Yuanzhe Shen",
      "Kaitao Song",
      "Jiakang Yuan",
      "Changze Lv",
      "Xiaoqing Zheng"
    ],
    "summary": "As LLM-based agents are increasingly used in long-term interactions, cumulative memory is critical for enabling personalization and maintaining stylistic consistency. However, most existing systems adopt an ``all-or-nothing'' approach to memory usage: incorporating all relevant past information can lead to \\textit{Memory Anchoring}, where the agent is trapped by past interactions, while excluding memory entirely results in under-utilization and the loss of important interaction history. We show that an agent's reliance on memory can be modeled as an explicit and user-controllable dimension. We first introduce a behavioral metric of memory dependence to quantify the influence of past interactions on current outputs. We then propose \\textbf{Stee}rable \\textbf{M}emory Agent, \\texttt{SteeM}, a framework that allows users to dynamically regulate memory reliance, ranging from a fresh-start mode that promotes innovation to a high-fidelity mode that closely follows interaction history. Experiments across different scenarios demonstrate that our approach consistently outperforms conventional prompting and rigid memory masking strategies, yielding a more nuanced and effective control for personalized human-agent collaboration.",
    "link": "http://arxiv.org/abs/2601.05107v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05106v1",
    "title": "Token-Level LLM Collaboration via FusionRoute",
    "authors": [
      "Nuoya Xiong",
      "Yuhang Zhou",
      "Hanqing Zeng",
      "Zhaorun Chen",
      "Furong Huang",
      "Shuchao Bi",
      "Lizhu Zhang",
      "Zhuokai Zhao"
    ],
    "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
    "link": "http://arxiv.org/abs/2601.05106v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
<<<<<<< HEAD
    "id": "http://arxiv.org/abs/2601.05103v1",
    "title": "Semantically Orthogonal Framework for Citation Classification: Disentangling Intent and Content",
    "authors": [
      "Changxu Duan",
      "Zhiyin Tan"
    ],
    "summary": "Understanding the role of citations is essential for research assessment and citation-aware digital libraries. However, existing citation classification frameworks often conflate citation intent (why a work is cited) with cited content type (what part is cited), limiting their effectiveness in auto classification due to a dilemma between fine-grained type distinctions and practical classification reliability. We introduce SOFT, a Semantically Orthogonal Framework with Two dimensions that explicitly separates citation intent from cited content type, drawing inspiration from semantic role theory. We systematically re-annotate the ACL-ARC dataset using SOFT and release a cross-disciplinary test set sampled from ACT2. Evaluation with both zero-shot and fine-tuned Large Language Models demonstrates that SOFT enables higher agreement between human annotators and LLMs, and supports stronger classification performance and robust cross-domain generalization compared to ACL-ARC and SciCite annotation frameworks. These results confirm SOFT's value as a clear, reusable annotation standard, improving clarity, consistency, and generalizability for digital libraries and scholarly communication infrastructures. All code and data are publicly available on GitHub https://github.com/zhiyintan/SOFT.",
    "link": "http://arxiv.org/abs/2601.05103v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05101v1",
    "title": "Arabic Prompts with English Tools: A Benchmark",
    "authors": [
      "Konstantin Kubrak",
      "Ahmed El-Moselhy",
      "Ammar Alsulami",
      "Remaz Altuwaim",
      "Hassan Ismail Fawaz",
      "Faisal Alsaby"
    ],
    "summary": "Large Language Models (LLMs) are now integral to numerous industries, increasingly serving as the core reasoning engine for autonomous agents that perform complex tasks through tool-use. While the development of Arabic-native LLMs is accelerating, the benchmarks for evaluating their capabilities lag behind, with most existing frameworks focusing on English. A critical and overlooked area is tool-calling, where the performance of models prompted in non-English languages like Arabic is poorly understood, especially since these models are often pretrained on predominantly English data. This paper addresses this critical gap by introducing the first dedicated benchmark for evaluating the tool-calling and agentic capabilities of LLMs in the Arabic language. Our work provides a standardized framework to measure the functional accuracy and robustness of models in Arabic agentic workflows. Our findings reveal a huge performance gap: when users interact in Arabic, tool-calling accuracy drops by an average of 5-10\\%, regardless of whether the tool descriptions themselves are in Arabic or English. By shedding light on these critical challenges, this benchmark aims to foster the development of more reliable and linguistically equitable AI agents for Arabic-speaking users.",
    "link": "http://arxiv.org/abs/2601.05101v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05075v1",
    "title": "SemPA: Improving Sentence Embeddings of Large Language Models through Semantic Preference Alignment",
    "authors": [
      "Ziyang Chen",
      "Zhenxuan Huang",
      "Yile Wang",
      "Weiqin Wang",
      "Lu Yin",
      "Hui Huang"
    ],
    "summary": "Traditional sentence embedding methods employ token-level contrastive learning on non-generative pre-trained models. Recently, there have emerged embedding methods based on generative large language models (LLMs). These methods either rely on fixed prompt templates or involve modifications to the model architecture. The former lacks further optimization of the model and results in limited performance, while the latter alters the internal computational mechanisms of the model, thereby compromising its generative capabilities. We propose SemPA, a novel approach that boosts the sentence representations while preserving the generative ability of LLMs via semantic preference alignment. We leverage sentence-level Direct Preference Optimization (DPO) to efficiently optimize LLMs on a paraphrase generation task, where the model learns to discriminate semantically equivalent sentences while preserving inherent generative capacity. Theoretically, we establish a formal connection between DPO and contrastive learning under the Plackett-Luce model framework. Empirically, experimental results on both semantic textual similarity tasks and various benchmarks for LLMs show that SemPA achieves better semantic representations without sacrificing the inherent generation capability of LLMs.",
    "link": "http://arxiv.org/abs/2601.05075v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05062v1",
    "title": "Compositional Steering of Large Language Models with Steering Tokens",
    "authors": [
      "Gorjan Radevski",
      "Kiril Gashteovski",
      "Giwon Hong",
      "Carolin Lawrence",
      "Goran Glava\u0161"
    ],
    "summary": "Deploying LLMs in real-world applications requires controllable output that satisfies multiple desiderata at the same time. While existing work extensively addresses LLM steering for a single behavior, \\textit{compositional steering} -- i.e., steering LLMs simultaneously towards multiple behaviors -- remains an underexplored problem. In this work, we propose \\emph{compositional steering tokens} for multi-behavior steering. We first embed individual behaviors, expressed as natural language instructions, into dedicated tokens via self-distillation. Contrary to most prior work, which operates in the activation space, our behavior steers live in the space of input tokens, enabling more effective zero-shot composition. We then train a dedicated \\textit{composition token} on pairs of behaviors and show that it successfully captures the notion of composition: it generalizes well to \\textit{unseen} compositions, including those with unseen behaviors as well as those with an unseen \\textit{number} of behaviors. Our experiments across different LLM architectures show that steering tokens lead to superior multi-behavior control compared to competing approaches (instructions, activation steering, and LoRA merging). Moreover, we show that steering tokens complement natural language instructions, with their combination resulting in further gains.",
    "link": "http://arxiv.org/abs/2601.05062v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05053v1",
    "title": "Reinforced Efficient Reasoning via Semantically Diverse Exploration",
    "authors": [
      "Ziqi Zhao",
      "Zhaochun Ren",
      "Jiahong Zou",
      "Liu Yang",
      "Zhiwei Xu",
      "Xuri Ge",
      "Zhumin Chen",
      "Xinyu Ma",
      "Daiting Shi",
      "Shuaiqiang Wang",
      "Dawei Yin",
      "Xin Xin"
    ],
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has proven effective in enhancing the reasoning of large language models (LLMs). Monte Carlo Tree Search (MCTS)-based extensions improve upon vanilla RLVR (e.g., GRPO) by providing tree-based reasoning rollouts that enable fine-grained and segment-level credit assignment. However, existing methods still suffer from limited exploration diversity and inefficient reasoning. To address the above challenges, we propose reinforced efficient reasoning via semantically diverse explorations, i.e., ROSE, for LLMs. To encourage more diverse reasoning exploration, our method incorporates a semantic-entropy-based branching strategy and an $\\varepsilon$-exploration mechanism. The former operates on already sampled reasoning rollouts to capture semantic uncertainty and select branching points with high semantic divergence to generate new successive reasoning paths, whereas the latter stochastically initiates reasoning rollouts from the root, preventing the search process from becoming overly local. To improve efficiency, we design a length-aware segment-level advantage estimator that rewards concise and correct reasoning while penalizing unnecessarily long reasoning chains. Extensive experiments on various mathematical reasoning benchmarks with Qwen and Llama models validate the effectiveness and efficiency of ROSE. Codes are available at https://github.com/ZiqiZhao1/ROSE-rl.",
    "link": "http://arxiv.org/abs/2601.05053v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
=======
>>>>>>> cf600fb (Research split into Kinetis and GRC)
    "id": "http://arxiv.org/abs/2601.04170v1",
    "title": "Agent Drift: Quantifying Behavioral Degradation in Multi-Agent LLM Systems Over Extended Interactions",
    "authors": [
      "Abhishek Rath"
    ],
    "summary": "Multi-agent Large Language Model (LLM) systems have emerged as powerful architectures for complex task decomposition and collaborative problem-solving. However, their long-term behavioral stability remains largely unexamined. This study introduces the concept of agent drift, defined as the progressive degradation of agent behavior, decision quality, and inter-agent coherence over extended interaction sequences. We present a comprehensive theoretical framework for understanding drift phenomena, proposing three distinct manifestations: semantic drift (progressive deviation from original intent), coordination drift (breakdown in multi-agent consensus mechanisms), and behavioral drift (emergence of unintended strategies).   We introduce the Agent Stability Index (ASI), a novel composite metric framework for quantifying drift across twelve dimensions, including response consistency, tool usage patterns, reasoning pathway stability, and inter-agent agreement rates. Through simulation-based analysis and theoretical modeling, we demonstrate how unchecked agent drift can lead to substantial reductions in task completion accuracy and increased human intervention requirements.   We propose three mitigation strategies: episodic memory consolidation, drift-aware routing protocols, and adaptive behavioral anchoring. Theoretical analysis suggests these approaches can significantly reduce drift-related errors while maintaining system throughput. This work establishes a foundational methodology for monitoring, measuring, and mitigating agent drift in production agentic AI systems, with direct implications for enterprise deployment reliability and AI safety research.",
    "link": "http://arxiv.org/abs/2601.04170v1",
    "date": "2026-01-07",
    "fetched_at": "2026-01-08"
  },
  {
    "id": "http://arxiv.org/abs/2601.04135v1",
    "title": "LLMberjack: Guided Trimming of Debate Trees for Multi-Party Conversation Creation",
    "authors": [
      "Leonardo Bottona",
      "Nicol\u00f2 Penzo",
      "Bruno Lepri",
      "Marco Guerini",
      "Sara Tonelli"
    ],
    "summary": "We present LLMberjack, a platform for creating multi-party conversations starting from existing debates, originally structured as reply trees. The system offers an interactive interface that visualizes discussion trees and enables users to construct coherent linearized dialogue sequences while preserving participant identity and discourse relations. It integrates optional large language model (LLM) assistance to support automatic editing of the messages and speakers' descriptions. We demonstrate the platform's utility by showing how tree visualization facilitates the creation of coherent, meaningful conversation threads and how LLM support enhances output quality while reducing human effort. The tool is open-source and designed to promote transparent and reproducible workflows to create multi-party conversations, addressing a lack of resources of this type.",
    "link": "http://arxiv.org/abs/2601.04135v1",
    "date": "2026-01-07",
    "fetched_at": "2026-01-08"
  },
  {
    "id": "http://arxiv.org/abs/2601.04131v1",
    "title": "ContextFocus: Activation Steering for Contextual Faithfulness in Large Language Models",
    "authors": [
      "Nikhil Anand",
      "Shwetha Somasundaram",
      "Anirudh Phukan",
      "Apoorv Saxena",
      "Koyel Mukherjee"
    ],
    "summary": "Large Language Models (LLMs) encode vast amounts of parametric knowledge during pre-training. As world knowledge evolves, effective deployment increasingly depends on their ability to faithfully follow externally retrieved context. When such evidence conflicts with the model's internal knowledge, LLMs often default to memorized facts, producing unfaithful outputs. In this work, we introduce ContextFocus, a lightweight activation steering approach that improves context faithfulness in such knowledge-conflict settings while preserving fluency and efficiency. Unlike prior approaches, our solution requires no model finetuning and incurs minimal inference-time overhead, making it highly efficient. We evaluate ContextFocus on the ConFiQA benchmark, comparing it against strong baselines including ContextDPO, COIECD, and prompting-based methods. Furthermore, we show that our method is complementary to prompting strategies and remains effective on larger models. Extensive experiments show that ContextFocus significantly improves contextual-faithfulness. Our results highlight the effectiveness, robustness, and efficiency of ContextFocus in improving contextual-faithfulness of LLM outputs.",
    "link": "http://arxiv.org/abs/2601.04131v1",
    "date": "2026-01-07",
    "fetched_at": "2026-01-08"
  },
  {
    "id": "http://arxiv.org/abs/2601.04126v1",
    "title": "InfiniteWeb: Scalable Web Environment Synthesis for GUI Agent Training",
    "authors": [
      "Ziyun Zhang",
      "Zezhou Wang",
      "Xiaoyi Zhang",
      "Zongyu Guo",
      "Jiahao Li",
      "Bin Li",
      "Yan Lu"
    ],
    "summary": "GUI agents that interact with graphical interfaces on behalf of users represent a promising direction for practical AI assistants. However, training such agents is hindered by the scarcity of suitable environments. We present InfiniteWeb, a system that automatically generates functional web environments at scale for GUI agent training. While LLMs perform well on generating a single webpage, building a realistic and functional website with many interconnected pages faces challenges. We address these challenges through unified specification, task-centric test-driven development, and a combination of website seed with reference design image to ensure diversity. Our system also generates verifiable task evaluators enabling dense reward signals for reinforcement learning. Experiments show that InfiniteWeb surpasses commercial coding agents at realistic website construction, and GUI agents trained on our generated environments achieve significant performance improvements on OSWorld and Online-Mind2Web, demonstrating the effectiveness of proposed system.",
    "link": "http://arxiv.org/abs/2601.04126v1",
    "date": "2026-01-07",
    "fetched_at": "2026-01-08"
  },
  {
    "id": "http://arxiv.org/abs/2601.04093v1",
    "title": "SearchAttack: Red-Teaming LLMs against Real-World Threats via Framing Unsafe Web Information-Seeking Tasks",
    "authors": [
      "Yu Yan",
      "Sheng Sun",
      "Mingfeng Li",
      "Zheming Yang",
      "Chiwei Zhu",
      "Fei Ma",
      "Benfeng Xu",
      "Min Liu"
    ],
    "summary": "Recently, people have suffered and become increasingly aware of the unreliability gap in LLMs for open and knowledge-intensive tasks, and thus turn to search-augmented LLMs to mitigate this issue. However, when the search engine is triggered for harmful tasks, the outcome is no longer under the LLM's control. Once the returned content directly contains targeted, ready-to-use harmful takeaways, the LLM's safeguards cannot withdraw that exposure. Motivated by this dilemma, we identify web search as a critical attack surface and propose \\textbf{\\textit{SearchAttack}} for red-teaming. SearchAttack outsources the harmful semantics to web search, retaining only the query's skeleton and fragmented clues, and further steers LLMs to reconstruct the retrieved content via structural rubrics to achieve malicious goals. Extensive experiments are conducted to red-team the search-augmented LLMs for responsible vulnerability assessment. Empirically, SearchAttack demonstrates strong effectiveness in attacking these systems.",
    "link": "http://arxiv.org/abs/2601.04093v1",
    "date": "2026-01-07",
    "fetched_at": "2026-01-08"
  },
  {
    "id": "http://arxiv.org/abs/2601.04086v1",
    "title": "KDCM: Reducing Hallucination in LLMs through Explicit Reasoning Structures",
    "authors": [
      "Jinbo Hao",
      "Kai Yang",
      "Qingzhen Su",
      "Yifan Li",
      "Chao Jiang"
    ],
    "summary": "To mitigate hallucinations in large language models (LLMs), we propose a framework that focuses on errors induced by prompts. Our method extends a chain-style knowledge distillation approach by incorporating a programmable module that guides knowledge graph exploration. This module is embedded as executable code within the reasoning prompt, allowing the model to leverage external structured knowledge during inference. Based on this design, we develop an enhanced distillation-based reasoning framework that explicitly regulates intermediate reasoning steps, resulting in more reliable predictions. We evaluate the proposed approach on multiple public benchmarks using GPT-4 and LLaMA-3.3. Experimental results show that code-guided reasoning significantly improves contextual modeling and reduces prompt-induced hallucinations. Specifically, HIT@1, HIT@3, and HIT@5 increase by 15.64%, 13.38%, and 13.28%, respectively, with scores exceeding 95% across several evaluation settings. These findings indicate that the proposed method effectively constrains erroneous reasoning while improving both accuracy and interpretability.",
    "link": "http://arxiv.org/abs/2601.04086v1",
    "date": "2026-01-07",
    "fetched_at": "2026-01-08"
  },
  {
    "id": "http://arxiv.org/abs/2601.04055v1",
    "title": "Modular Prompt Optimization: Optimizing Structured Prompts with Section-Local Textual Gradients",
    "authors": [
      "Prith Sharma",
      "Austin Z. Henley"
    ],
    "summary": "Prompt quality plays a central role in controlling the behavior, reliability, and reasoning performance of large language models (LLMs), particularly for smaller open-source instruction-tuned models that depend heavily on explicit structure. While recent work has explored automatic prompt optimization using textual gradients and self-refinement, most existing methods treat prompts as monolithic blocks of text, making it difficult to localize errors, preserve critical instructions, or prevent uncontrolled prompt growth. We introduce Modular Prompt Optimization (MPO), a schema-based prompt optimization framework that treats prompts as structured objects composed of fixed semantic sections, including system role, context, task description, constraints, and output format. MPO applies section-local textual gradients, generated by a critic language model, to refine each section independently while keeping the overall prompt schema fixed. Section updates are consolidated through de-duplication to reduce redundancy and interference between components, yielding an interpretable and robust optimization process. We evaluate MPO on two reasoning benchmarks, ARC-Challenge and MMLU, using LLaMA-3 8B-Instruct and Mistral-7B-Instruct as solver models. Across both benchmarks and models, MPO consistently outperforms an untuned structured prompt and the TextGrad baseline, achieving substantial accuracy gains without modifying model parameters or altering prompt structure. These results demonstrate that maintaining a fixed prompt schema while applying localized, section-wise optimization is an effective and practical approach for improving reasoning performance in small open-source LMs.",
    "link": "http://arxiv.org/abs/2601.04055v1",
    "date": "2026-01-07",
    "fetched_at": "2026-01-08"
  },
  {
    "id": "http://arxiv.org/abs/2601.04052v1",
    "title": "Stable Language Guidance for Vision-Language-Action Models",
    "authors": [
      "Zhihao Zhan",
      "Yuhao Chen",
      "Jiaying Zhou",
      "Qinhan Lv",
      "Hao Liu",
      "Keze Wang",
      "Liang Lin",
      "Guangrun Wang"
    ],
    "summary": "Vision-Language-Action (VLA) models have demonstrated impressive capabilities in generalized robotic control; however, they remain notoriously brittle to linguistic perturbations. We identify a critical ``modality collapse'' phenomenon where strong visual priors overwhelm sparse linguistic signals, causing agents to overfit to specific instruction phrasings while ignoring the underlying semantic intent. To address this, we propose \\textbf{Residual Semantic Steering (RSS)}, a probabilistic framework that disentangles physical affordance from semantic execution. RSS introduces two theoretical innovations: (1) \\textbf{Monte Carlo Syntactic Integration}, which approximates the true semantic posterior via dense, LLM-driven distributional expansion, and (2) \\textbf{Residual Affordance Steering}, a dual-stream decoding mechanism that explicitly isolates the causal influence of language by subtracting the visual affordance prior. Theoretical analysis suggests that RSS effectively maximizes the mutual information between action and intent while suppressing visual distractors. Empirical results across diverse manipulation benchmarks demonstrate that RSS achieves state-of-the-art robustness, maintaining performance even under adversarial linguistic perturbations.",
    "link": "http://arxiv.org/abs/2601.04052v1",
    "date": "2026-01-07",
    "fetched_at": "2026-01-08"
  },
  {
    "id": "http://arxiv.org/abs/2601.04043v1",
    "title": "When Helpers Become Hazards: A Benchmark for Analyzing Multimodal LLM-Powered Safety in Daily Life",
    "authors": [
      "Xinyue Lou",
      "Jinan Xu",
      "Jingyi Yin",
      "Xiaolong Wang",
      "Zhaolu Kang",
      "Youwei Liao",
      "Yixuan Wang",
      "Xiangyu Shi",
      "Fengran Mo",
      "Su Yao",
      "Kaiyu Huang"
    ],
    "summary": "As Multimodal Large Language Models (MLLMs) become an indispensable assistant in human life, the unsafe content generated by MLLMs poses a danger to human behavior, perpetually overhanging human society like a sword of Damocles. To investigate and evaluate the safety impact of MLLMs responses on human behavior in daily life, we introduce SaLAD, a multimodal safety benchmark which contains 2,013 real-world image-text samples across 10 common categories, with a balanced design covering both unsafe scenarios and cases of oversensitivity. It emphasizes realistic risk exposure, authentic visual inputs, and fine-grained cross-modal reasoning, ensuring that safety risks cannot be inferred from text alone. We further propose a safety-warning-based evaluation framework that encourages models to provide clear and informative safety warnings, rather than generic refusals. Results on 18 MLLMs demonstrate that the top-performing models achieve a safe response rate of only 57.2% on unsafe queries. Moreover, even popular safety alignment methods limit effectiveness of the models in our scenario, revealing the vulnerabilities of current MLLMs in identifying dangerous behaviors in daily life. Our dataset is available at https://github.com/xinyuelou/SaLAD.",
    "link": "http://arxiv.org/abs/2601.04043v1",
    "date": "2026-01-07",
    "fetched_at": "2026-01-08"
  },
  {
    "id": "http://arxiv.org/abs/2601.04034v1",
    "title": "HoneyTrap: Deceiving Large Language Model Attackers to Honeypot Traps with Resilient Multi-Agent Defense",
    "authors": [
      "Siyuan Li",
      "Xi Lin",
      "Jun Wu",
      "Zehao Liu",
      "Haoyu Li",
      "Tianjie Ju",
      "Xiang Chen",
      "Jianhua Li"
    ],
    "summary": "Jailbreak attacks pose significant threats to large language models (LLMs), enabling attackers to bypass safeguards. However, existing reactive defense approaches struggle to keep up with the rapidly evolving multi-turn jailbreaks, where attackers continuously deepen their attacks to exploit vulnerabilities. To address this critical challenge, we propose HoneyTrap, a novel deceptive LLM defense framework leveraging collaborative defenders to counter jailbreak attacks. It integrates four defensive agents, Threat Interceptor, Misdirection Controller, Forensic Tracker, and System Harmonizer, each performing a specialized security role and collaborating to complete a deceptive defense. To ensure a comprehensive evaluation, we introduce MTJ-Pro, a challenging multi-turn progressive jailbreak dataset that combines seven advanced jailbreak strategies designed to gradually deepen attack strategies across multi-turn attacks. Besides, we present two novel metrics: Mislead Success Rate (MSR) and Attack Resource Consumption (ARC), which provide more nuanced assessments of deceptive defense beyond conventional measures. Experimental results on GPT-4, GPT-3.5-turbo, Gemini-1.5-pro, and LLaMa-3.1 demonstrate that HoneyTrap achieves an average reduction of 68.77% in attack success rates compared to state-of-the-art baselines. Notably, even in a dedicated adaptive attacker setting with intensified conditions, HoneyTrap remains resilient, leveraging deceptive engagement to prolong interactions, significantly increasing the time and computational costs required for successful exploitation. Unlike simple rejection, HoneyTrap strategically wastes attacker resources without impacting benign queries, improving MSR and ARC by 118.11% and 149.16%, respectively.",
    "link": "http://arxiv.org/abs/2601.04034v1",
    "date": "2026-01-07",
    "fetched_at": "2026-01-08"
  },
  {
    "id": "http://arxiv.org/abs/2601.04025v1",
    "title": "Simulated Students in Tutoring Dialogues: Substance or Illusion?",
    "authors": [
      "Alexander Scarlatos",
      "Jaewook Lee",
      "Simon Woodhead",
      "Andrew Lan"
    ],
    "summary": "Advances in large language models (LLMs) enable many new innovations in education. However, evaluating the effectiveness of new technology requires real students, which is time-consuming and hard to scale up. Therefore, many recent works on LLM-powered tutoring solutions have used simulated students for both training and evaluation, often via simple prompting. Surprisingly, little work has been done to ensure or even measure the quality of simulated students. In this work, we formally define the student simulation task, propose a set of evaluation metrics that span linguistic, behavioral, and cognitive aspects, and benchmark a wide range of student simulation methods on these metrics. We experiment on a real-world math tutoring dialogue dataset, where both automated and human evaluation results show that prompting strategies for student simulation perform poorly; supervised fine-tuning and preference optimization yield much better but still limited performance, motivating future work on this challenging task.",
    "link": "http://arxiv.org/abs/2601.04025v1",
    "date": "2026-01-07",
    "fetched_at": "2026-01-08"
  },
  {
    "id": "http://arxiv.org/abs/2601.03997v1",
    "title": "VotIE: Information Extraction from Meeting Minutes",
    "authors": [
      "Jos\u00e9 Pedro Evans",
      "Lu\u00eds Filipe Cunha",
      "Purifica\u00e7\u00e3o Silvano",
      "Al\u00edpio Jorge",
      "Nuno Guimar\u00e3es",
      "S\u00e9rgio Nunes",
      "Ricardo Campos"
    ],
    "summary": "Municipal meeting minutes record key decisions in local democratic processes. Unlike parliamentary proceedings, which typically adhere to standardized formats, they encode voting outcomes in highly heterogeneous, free-form narrative text that varies widely across municipalities, posing significant challenges for automated extraction. In this paper, we introduce VotIE (Voting Information Extraction), a new information extraction task aimed at identifying structured voting events in narrative deliberative records, and establish the first benchmark for this task using Portuguese municipal minutes, building on the recently introduced CitiLink corpus. Our experiments yield two key findings. First, under standard in-domain evaluation, fine-tuned encoders, specifically XLM-R-CRF, achieve the strongest performance, reaching 93.2\\% macro F1, outperforming generative approaches. Second, in a cross-municipality setting that evaluates transfer to unseen administrative contexts, these models suffer substantial performance degradation, whereas few-shot LLMs demonstrate greater robustness, with significantly smaller declines in performance. Despite this generalization advantage, the high computational cost of generative models currently constrains their practicality. As a result, lightweight fine-tuned encoders remain a more practical option for large-scale, real-world deployment. To support reproducible research in administrative NLP, we publicly release our benchmark, trained models, and evaluation framework.",
    "link": "http://arxiv.org/abs/2601.03997v1",
    "date": "2026-01-07",
    "fetched_at": "2026-01-08"
  },
  {
    "id": "http://arxiv.org/abs/2601.03993v1",
    "title": "PosterVerse: A Full-Workflow Framework for Commercial-Grade Poster Generation with HTML-Based Scalable Typography",
    "authors": [
      "Junle Liu",
      "Peirong Zhang",
      "Yuyi Zhang",
      "Pengyu Yan",
      "Hui Zhou",
      "Xinyue Zhou",
      "Fengjun Guo",
      "Lianwen Jin"
    ],
    "summary": "Commercial-grade poster design demands the seamless integration of aesthetic appeal with precise, informative content delivery. Current automated poster generation systems face significant limitations, including incomplete design workflows, poor text rendering accuracy, and insufficient flexibility for commercial applications. To address these challenges, we propose PosterVerse, a full-workflow, commercial-grade poster generation method that seamlessly automates the entire design process while delivering high-density and scalable text rendering. PosterVerse replicates professional design through three key stages: (1) blueprint creation using fine-tuned LLMs to extract key design elements from user requirements, (2) graphical background generation via customized diffusion models to create visually appealing imagery, and (3) unified layout-text rendering with an MLLM-powered HTML engine to guarantee high text accuracy and flexible customization. In addition, we introduce PosterDNA, a commercial-grade, HTML-based dataset tailored for training and validating poster design models. To the best of our knowledge, PosterDNA is the first Chinese poster generation dataset to introduce HTML typography files, enabling scalable text rendering and fundamentally solving the challenges of rendering small and high-density text. Experimental results demonstrate that PosterVerse consistently produces commercial-grade posters with appealing visuals, accurate text alignment, and customizable layouts, making it a promising solution for automating commercial poster design. The code and model are available at https://github.com/wuhaer/PosterVerse.",
    "link": "http://arxiv.org/abs/2601.03993v1",
    "date": "2026-01-07",
    "fetched_at": "2026-01-08"
  },
  {
    "id": "http://arxiv.org/abs/2601.03986v1",
    "title": "Benchmark^2: Systematic Evaluation of LLM Benchmarks",
    "authors": [
      "Qi Qian",
      "Chengsong Huang",
      "Jingwen Xu",
      "Changze Lv",
      "Muling Wu",
      "Wenhao Liu",
      "Xiaohua Wang",
      "Zhenghua Wang",
      "Zisu Huang",
      "Muzhao Tian",
      "Jianhan Xu",
      "Kun Hu",
      "He-Da Wang",
      "Yao Hu",
      "Xuanjing Huang",
      "Xiaoqing Zheng"
    ],
    "summary": "The rapid proliferation of benchmarks for evaluating large language models (LLMs) has created an urgent need for systematic methods to assess benchmark quality itself. We propose Benchmark^2, a comprehensive framework comprising three complementary metrics: (1) Cross-Benchmark Ranking Consistency, measuring whether a benchmark produces model rankings aligned with peer benchmarks; (2) Discriminability Score, quantifying a benchmark's ability to differentiate between models; and (3) Capability Alignment Deviation, identifying problematic instances where stronger models fail but weaker models succeed within the same model family. We conduct extensive experiments across 15 benchmarks spanning mathematics, reasoning, and knowledge domains, evaluating 11 LLMs across four model families. Our analysis reveals significant quality variations among existing benchmarks and demonstrates that selective benchmark construction based on our metrics can achieve comparable evaluation performance with substantially reduced test sets.",
    "link": "http://arxiv.org/abs/2601.03986v1",
    "date": "2026-01-07",
    "fetched_at": "2026-01-08"
  },
  {
    "id": "http://arxiv.org/abs/2601.03981v1",
    "title": "RADAR: Retrieval-Augmented Detector with Adversarial Refinement for Robust Fake News Detection",
    "authors": [
      "Song-Duo Ma",
      "Yi-Hung Liu",
      "Hsin-Yu Lin",
      "Pin-Yu Chen",
      "Hong-Yan Huang",
      "Shau-Yung Hsu",
      "Yun-Nung Chen"
    ],
    "summary": "To efficiently combat the spread of LLM-generated misinformation, we present RADAR, a retrieval-augmented detector with adversarial refinement for robust fake news detection. Our approach employs a generator that rewrites real articles with factual perturbations, paired with a lightweight detector that verifies claims using dense passage retrieval. To enable effective co-evolution, we introduce verbal adversarial feedback (VAF). Rather than relying on scalar rewards, VAF issues structured natural-language critiques; these guide the generator toward more sophisticated evasion attempts, compelling the detector to adapt and improve. On a fake news detection benchmark, RADAR achieves 86.98% ROC-AUC, significantly outperforming general-purpose LLMs with retrieval. Ablation studies confirm that detector-side retrieval yields the largest gains, while VAF and few-shot demonstrations provide critical signals for robust training.",
    "link": "http://arxiv.org/abs/2601.03981v1",
    "date": "2026-01-07",
    "fetched_at": "2026-01-08"
  },
  {
    "id": "http://arxiv.org/abs/2601.03979v1",
    "title": "SoK: Privacy Risks and Mitigations in Retrieval-Augmented Generation Systems",
    "authors": [
      "Andreea-Elena Bodea",
      "Stephen Meisenbacher",
      "Alexandra Klymenko",
      "Florian Matthes"
    ],
    "summary": "The continued promise of Large Language Models (LLMs), particularly in their natural language understanding and generation capabilities, has driven a rapidly increasing interest in identifying and developing LLM use cases. In an effort to complement the ingrained \"knowledge\" of LLMs, Retrieval-Augmented Generation (RAG) techniques have become widely popular. At its core, RAG involves the coupling of LLMs with domain-specific knowledge bases, whereby the generation of a response to a user question is augmented with contextual and up-to-date information. The proliferation of RAG has sparked concerns about data privacy, particularly with the inherent risks that arise when leveraging databases with potentially sensitive information. Numerous recent works have explored various aspects of privacy risks in RAG systems, from adversarial attacks to proposed mitigations. With the goal of surveying and unifying these works, we ask one simple question: What are the privacy risks in RAG, and how can they be measured and mitigated? To answer this question, we conduct a systematic literature review of RAG works addressing privacy, and we systematize our findings into a comprehensive set of privacy risks, mitigation techniques, and evaluation strategies. We supplement these findings with two primary artifacts: a Taxonomy of RAG Privacy Risks and a RAG Privacy Process Diagram. Our work contributes to the study of privacy in RAG not only by conducting the first systematization of risks and mitigations, but also by uncovering important considerations when mitigating privacy risks in RAG systems and assessing the current maturity of proposed mitigations.",
    "link": "http://arxiv.org/abs/2601.03979v1",
    "date": "2026-01-07",
    "fetched_at": "2026-01-08"
  },
  {
    "id": "http://arxiv.org/abs/2601.03959v1",
    "title": "FUSION: Full-Body Unified Motion Prior for Body and Hands via Diffusion",
    "authors": [
      "Enes Duran",
      "Nikos Athanasiou",
      "Muhammed Kocabas",
      "Michael J. Black",
      "Omid Taheri"
    ],
    "summary": "Hands are central to interacting with our surroundings and conveying gestures, making their inclusion essential for full-body motion synthesis. Despite this, existing human motion synthesis methods fall short: some ignore hand motions entirely, while others generate full-body motions only for narrowly scoped tasks under highly constrained settings. A key obstacle is the lack of large-scale datasets that jointly capture diverse full-body motion with detailed hand articulation. While some datasets capture both, they are limited in scale and diversity. Conversely, large-scale datasets typically focus either on body motion without hands or on hand motions without the body. To overcome this, we curate and unify existing hand motion datasets with large-scale body motion data to generate full-body sequences that capture both hand and body. We then propose the first diffusion-based unconditional full-body motion prior, FUSION, which jointly models body and hand motion. Despite using a pose-based motion representation, FUSION surpasses state-of-the-art skeletal control models on the Keypoint Tracking task in the HumanML3D dataset and achieves superior motion naturalness. Beyond standard benchmarks, we demonstrate that FUSION can go beyond typical uses of motion priors through two applications: (1) generating detailed full-body motion including fingers during interaction given the motion of an object, and (2) generating Self-Interaction motions using an LLM to transform natural language cues into actionable motion constraints. For these applications, we develop an optimization pipeline that refines the latent space of our diffusion model to generate task-specific motions. Experiments on these tasks highlight precise control over hand motion while maintaining plausible full-body coordination. The code will be public.",
    "link": "http://arxiv.org/abs/2601.03959v1",
    "date": "2026-01-07",
    "fetched_at": "2026-01-08"
  },
  {
    "id": "http://arxiv.org/abs/2601.03948v1",
    "title": "Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification",
    "authors": [
      "Rui Sun",
      "Yifan Sun",
      "Sheng Xu",
      "Li Zhao",
      "Jing Li",
      "Daxin Jiang",
      "Chen Hua",
      "Zuo Bai"
    ],
    "summary": "Reinforcement Learning (RL) has enabled Large Language Models (LLMs) to achieve remarkable reasoning in domains like mathematics and coding, where verifiable rewards provide clear signals. However, extending this paradigm to financial decision is challenged by the market's stochastic nature: rewards are verifiable but inherently noisy, causing standard RL to degenerate into reward hacking. To address this, we propose Trade-R1, a model training framework that bridges verifiable rewards to stochastic environments via process-level reasoning verification. Our key innovation is a verification method that transforms the problem of evaluating reasoning over lengthy financial documents into a structured Retrieval-Augmented Generation (RAG) task. We construct a triangular consistency metric, assessing pairwise alignment between retrieved evidence, reasoning chains, and decisions to serve as a validity filter for noisy market returns. We explore two reward integration strategies: Fixed-effect Semantic Reward (FSR) for stable alignment signals, and Dynamic-effect Semantic Reward (DSR) for coupled magnitude optimization. Experiments on different country asset selection demonstrate that our paradigm reduces reward hacking, with DSR achieving superior cross-market generalization while maintaining the highest reasoning consistency.",
    "link": "http://arxiv.org/abs/2601.03948v1",
    "date": "2026-01-07",
    "fetched_at": "2026-01-08"
  },
  {
    "id": "http://arxiv.org/abs/2601.03940v1",
    "title": "Large-Scale Aspect-Based Sentiment Analysis with Reasoning-Infused LLMs",
    "authors": [
      "Pawe\u0142 Liskowski",
      "Krzysztof Jankowski"
    ],
    "summary": "We introduce Arctic-ABSA, a collection of powerful models for real-life aspect-based sentiment analysis (ABSA). Our models are tailored to commercial needs, trained on a large corpus of public data alongside carefully generated synthetic data, resulting in a dataset 20 times larger than SemEval14. We extend typical ABSA models by expanding the number of sentiment classes from the standard three (positive, negative, neutral) to five, adding mixed and unknown classes, while also jointly predicting overall text sentiment and supporting multiple languages. We experiment with reasoning injection by fine-tuning on Chain-of-Thought (CoT) examples and introduce a novel reasoning pretraining technique for encoder-only models that significantly improves downstream fine-tuning and generalization. Our 395M-parameter encoder and 8B-parameter decoder achieve up to 10 percentage points higher accuracy than GPT-4o and Claude 3.5 Sonnet, while setting new state-of-the-art results on the SemEval14 benchmark. A single multilingual model maintains 87-91% accuracy across six languages without degrading English performance. We release ABSA-mix, a large-scale benchmark aggregating 17 public ABSA datasets across 92 domains.",
    "link": "http://arxiv.org/abs/2601.03940v1",
    "date": "2026-01-07",
    "fetched_at": "2026-01-08"
  },
  {
    "id": "http://arxiv.org/abs/2601.03938v1",
    "title": "FOREVER: Forgetting Curve-Inspired Memory Replay for Language Model Continual Learning",
    "authors": [
      "Yujie Feng",
      "Hao Wang",
      "Jian Li",
      "Xu Chu",
      "Zhaolu Kang",
      "Yiran Liu",
      "Yasha Wang",
      "Philip S. Yu",
      "Xiao-Ming Wu"
    ],
    "summary": "Continual learning (CL) for large language models (LLMs) aims to enable sequential knowledge acquisition without catastrophic forgetting. Memory replay methods are widely used for their practicality and effectiveness, but most rely on fixed, step-based heuristics that often misalign with the model's actual learning progress, since identical training steps can result in varying degrees of parameter change. Motivated by recent findings that LLM forgetting mirrors the Ebbinghaus human forgetting curve, we propose FOREVER (FORgEtting curVe-inspired mEmory Replay), a novel CL framework that aligns replay schedules with a model-centric notion of time. FOREVER defines model time using the magnitude of optimizer updates, allowing forgetting curve-inspired replay intervals to align with the model's internal evolution rather than raw training steps. Building on this approach, FOREVER incorporates a forgetting curve-based replay scheduler to determine when to replay and an intensity-aware regularization mechanism to adaptively control how to replay. Extensive experiments on three CL benchmarks and models ranging from 0.6B to 13B parameters demonstrate that FOREVER consistently mitigates catastrophic forgetting.",
    "link": "http://arxiv.org/abs/2601.03938v1",
    "date": "2026-01-07",
    "fetched_at": "2026-01-08"
  }
]