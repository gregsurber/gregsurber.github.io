[
  {
    "id": "http://arxiv.org/abs/2602.13194v1",
    "title": "Semantic Chunking and the Entropy of Natural Language",
    "authors": [
      "Weishun Zhong",
      "Doron Sivan",
      "Tankut Can",
      "Mikhail Katkov",
      "Misha Tsodyks"
    ],
    "summary": "The entropy rate of printed English is famously estimated to be about one bit per character, a benchmark that modern large language models (LLMs) have only recently approached. This entropy rate implies that English contains nearly 80 percent redundancy relative to the five bits per character expected for random text. We introduce a statistical model that attempts to capture the intricate multi-scale structure of natural language, providing a first-principles account of this redundancy level. Our model describes a procedure of self-similarly segmenting text into semantically coherent chunks down to the single-word level. The semantic structure of the text can then be hierarchically decomposed, allowing for analytical treatment. Numerical experiments with modern LLMs and open datasets suggest that our model quantitatively captures the structure of real texts at different levels of the semantic hierarchy. The entropy rate predicted by our model agrees with the estimated entropy rate of printed English. Moreover, our theory further reveals that the entropy rate of natural language is not fixed but should increase systematically with the semantic complexity of corpora, which are captured by the only free parameter in our model.",
    "link": "http://arxiv.org/abs/2602.13194v1",
    "date": "2026-02-13",
    "fetched_at": "2026-02-16"
  },
  {
    "id": "http://arxiv.org/abs/2602.13165v1",
    "title": "Asynchronous Verified Semantic Caching for Tiered LLM Architectures",
    "authors": [
      "Asmit Kumar Singh",
      "Haozhe Wang",
      "Laxmi Naga Santosh Attaluri",
      "Tak Chiam",
      "Weihua Zhu"
    ],
    "summary": "Large language models (LLMs) now sit in the critical path of search, assistance, and agentic workflows, making semantic caching essential for reducing inference cost and latency. Production deployments typically use a tiered static-dynamic design: a static cache of curated, offline vetted responses mined from logs, backed by a dynamic cache populated online. In practice, both tiers are commonly governed by a single embedding similarity threshold, which induces a hard tradeoff: conservative thresholds miss safe reuse opportunities, while aggressive thresholds risk serving semantically incorrect responses. We introduce \\textbf{Krites}, an asynchronous, LLM-judged caching policy that expands static coverage without changing serving decisions. On the critical path, Krites behaves exactly like a standard static threshold policy. When the nearest static neighbor of the prompt falls just below the static threshold, Krites asynchronously invokes an LLM judge to verify whether the static response is acceptable for the new prompt. Approved matches are promoted into the dynamic cache, allowing future repeats and paraphrases to reuse curated static answers and expanding static reach over time. In trace-driven simulations on conversational and search workloads, Krites increases the fraction of requests served with curated static answers (direct static hits plus verified promotions) by up to $\\textbf{3.9}$ times for conversational traffic and search-style queries relative to tuned baselines, with unchanged critical path latency.",
    "link": "http://arxiv.org/abs/2602.13165v1",
    "date": "2026-02-13",
    "fetched_at": "2026-02-16"
  },
  {
    "id": "http://arxiv.org/abs/2602.13156v1",
    "title": "In-Context Autonomous Network Incident Response: An End-to-End Large Language Model Agent Approach",
    "authors": [
      "Yiran Gao",
      "Kim Hammar",
      "Tao Li"
    ],
    "summary": "Rapidly evolving cyberattacks demand incident response systems that can autonomously learn and adapt to changing threats. Prior work has extensively explored the reinforcement learning approach, which involves learning response strategies through extensive simulation of the incident. While this approach can be effective, it requires handcrafted modeling of the simulator and suppresses useful semantics from raw system logs and alerts. To address these limitations, we propose to leverage large language models' (LLM) pre-trained security knowledge and in-context learning to create an end-to-end agentic solution for incident response planning. Specifically, our agent integrates four functionalities, perception, reasoning, planning, and action, into one lightweight LLM (14b model). Through fine-tuning and chain-of-thought reasoning, our LLM agent is capable of processing system logs and inferring the underlying network state (perception), updating its conjecture of attack models (reasoning), simulating consequences under different response strategies (planning), and generating an effective response (action). By comparing LLM-simulated outcomes with actual observations, the LLM agent repeatedly refines its attack conjecture and corresponding response, thereby demonstrating in-context adaptation. Our agentic approach is free of modeling and can run on commodity hardware. When evaluated on incident logs reported in the literature, our agent achieves recovery up to 23% faster than those of frontier LLMs.",
    "link": "http://arxiv.org/abs/2602.13156v1",
    "date": "2026-02-13",
    "fetched_at": "2026-02-16"
  },
  {
    "id": "http://arxiv.org/abs/2602.13151v1",
    "title": "Quantization-Robust LLM Unlearning via Low-Rank Adaptation",
    "authors": [
      "Jo\u00e3o Vitor Boer Abitante",
      "Joana Meneguzzo Pasquali",
      "Luan Fonseca Garcia",
      "Ewerton de Oliveira",
      "Thomas da Silva Paula",
      "Rodrigo C. Barros",
      "Lucas S. Kupssinsk\u00fc"
    ],
    "summary": "Large Language Model (LLM) unlearning aims to remove targeted knowledge from a trained model, but practical deployments often require post-training quantization (PTQ) for efficient inference. However, aggressive low-bit PTQ can mask or erase unlearning updates, causing quantized models to revert to pre-unlearning behavior. We show that standard full-parameter fine-tuning often induce parameter changes that are too small to survive 4-bit quantization. We propose quantization-robust unlearning via low-rank adaptation (LoRA): we freeze the base model and concentrate unlearning into trainable adapters so that the effective update is preserved after quantization. On Llama-2-7B evaluated with MUSE dataset (BOOKS and NEWS), LoRA improves 4-bit utility by up to 7.93 points (NPO+GDR on BOOKS: 50.17 to 58.10) and yields higher 4-bit utility on NEWS for GA+GDR (40.06 to 44.82, increase of 4.76). LoRA also substantially reduces privacy leakage under 4-bit PTQ, e.g., for GA+KLR on BOOKS, PrivLeak moves from -25.68 to -5.86 (closer to ideal 0), while maintaining strong forgetting (VerMem and KnowMem near 0). Thus, using LoRA for Machine Unlearning is beneficial for scenarios where quantization is necessary for model deployment.",
    "link": "http://arxiv.org/abs/2602.13151v1",
    "date": "2026-02-13",
    "fetched_at": "2026-02-16"
  },
  {
    "id": "http://arxiv.org/abs/2602.13134v1",
    "title": "Awakening Dormant Users: Generative Recommendation with Counterfactual Functional Role Reasoning",
    "authors": [
      "Huishi Luo",
      "Shuokai Li",
      "Hanchen Yang",
      "Zhongbo Sun",
      "Haojie Ding",
      "Boheng Zhang",
      "Zijia Cai",
      "Renliang Qian",
      "Fan Yang",
      "Tingting Gao",
      "Chenyi Lei",
      "Wenwu Ou",
      "Fuzhen Zhuang"
    ],
    "summary": "Awakening dormant users, who remain engaged but exhibit low conversion, is a pivotal driver for incremental GMV growth in large-scale e-commerce platforms. However, existing approaches often yield suboptimal results since they typically rely on single-step estimation of an item's intrinsic value (e.g., immediate click probability). This mechanism overlooks the instrumental effect of items, where specific interactions act as triggers to shape latent intent and drive subsequent decisions along a conversion trajectory. To bridge this gap, we propose RoleGen, a novel framework that synergizes a Conversion Trajectory Reasoner with a Generative Behavioral Backbone. Specifically, the LLM-based Reasoner explicitly models the context-dependent Functional Role of items to reconstruct intent evolution. It further employs counterfactual inference to simulate diverse conversion paths, effectively mitigating interest collapse. These reasoned candidate items are integrated into the generative backbone, which is optimized via a collaborative \"Reasoning-Execution-Feedback-Reflection\" closed-loop strategy to ensure grounded execution. Extensive offline experiments and online A/B testing on the Kuaishou e-commerce platform demonstrate that RoleGen achieves a 6.2% gain in Recall@1 and a 7.3% increase in online order volume, confirming its effectiveness in activating the dormant user base.",
    "link": "http://arxiv.org/abs/2602.13134v1",
    "date": "2026-02-13",
    "fetched_at": "2026-02-16"
  },
  {
    "id": "http://arxiv.org/abs/2602.13110v1",
    "title": "SCOPE: Selective Conformal Optimized Pairwise LLM Judging",
    "authors": [
      "Sher Badshah",
      "Ali Emami",
      "Hassan Sajjad"
    ],
    "summary": "Large language models (LLMs) are increasingly used as judges to replace costly human preference labels in pairwise evaluation. Despite their practicality, LLM judges remain prone to miscalibration and systematic biases. This paper proposes SCOPE (Selective Conformal Optimized Pairwise Evaluation), a framework for selective pairwise judging with finite-sample statistical guarantees. Under exchangeability, SCOPE calibrates an acceptance threshold such that the error rate among non-abstained judgments is at most a user-specified level $\u03b1$. To provide SCOPE with a bias-neutral uncertainty signal, we introduce Bidirectional Preference Entropy (BPE), which queries the judge under both response positions, aggregates the implied preference probabilities to enforce invariance to response order, and converts the aggregated probability into an entropy-based uncertainty score. Across MT-Bench, RewardBench, and Chatbot Arena, BPE improves uncertainty quality over standard confidence proxies, providing a stronger selection signal that enables SCOPE to consistently meet the target risk level while retaining good coverage across judge scales. In particular, at $\u03b1= 0.10$, \\textsc{Scope} consistently satisfies the risk bound across all benchmarks and judge scales (empirical risk $\\approx 0.097$ to $0.099$), while retaining substantial coverage, reaching $0.89$ on RewardBench with Qwen-14B and $0.98$ on RewardBench with Qwen-32B. Compared to na\u00efve baselines, \\textsc{Scope} accepts up to $2.4\\times$ more judgments on MT-Bench with Qwen-7B under the same target risk constraint, demonstrating that BPE enables reliable and high-coverage LLM-based evaluation.",
    "link": "http://arxiv.org/abs/2602.13110v1",
    "date": "2026-02-13",
    "fetched_at": "2026-02-16"
  },
  {
    "id": "http://arxiv.org/abs/2602.13103v1",
    "title": "R-Diverse: Mitigating Diversity Illusion in Self-Play LLM Training",
    "authors": [
      "Gengsheng Li",
      "Jinghan He",
      "Shijie Wang",
      "Dan Zhang",
      "Ruiqi Liu",
      "Renrui Zhang",
      "Zijun Yao",
      "Junfeng Fang",
      "Haiyun Guo",
      "Jinqiao Wang"
    ],
    "summary": "Self-play bootstraps LLM reasoning through an iterative Challenger-Solver loop: the Challenger is trained to generate questions that target the Solver's capabilities, and the Solver is optimized on the generated data to expand its reasoning skills. However, existing frameworks like R-Zero often exhibit non-sustained improvement, where early gains degrade as self-play continues. We identify a key failure mode, Diversity Illusion, where the Solver's training signals appear diverse yet collapse into recurring underlying patterns. It manifests as (1) Local Diversity Illusion, where diversity is enforced only within-batch, inducing cross-iteration mode cycling; and (2) Surface Diversity Illusion, where questions vary superficially but require near-identical reasoning skills. To mitigate them, we propose R-Diverse with two aligned innovations: Memory-Augmented Penalty (MAP), which uses a persistent memory bank to discourage recycling across iterations, and Skill-Aware Measurement (SAM), which evaluates diversity by the reasoning skills exercised rather than surface variation of questions. Across 10 math and general reasoning benchmarks, R-Diverse sustains gains over more iterations and consistently outperforms prior self-play methods. Code is available at https://github.com/Gengsheng-Li/R-Diverse.",
    "link": "http://arxiv.org/abs/2602.13103v1",
    "date": "2026-02-13",
    "fetched_at": "2026-02-16"
  },
  {
    "id": "http://arxiv.org/abs/2602.13093v1",
    "title": "Consistency of Large Reasoning Models Under Multi-Turn Attacks",
    "authors": [
      "Yubo Li",
      "Ramayya Krishnan",
      "Rema Padman"
    ],
    "summary": "Large reasoning models with reasoning capabilities achieve state-of-the-art performance on complex tasks, but their robustness under multi-turn adversarial pressure remains underexplored. We evaluate nine frontier reasoning models under adversarial attacks. Our findings reveal that reasoning confers meaningful but incomplete robustness: most reasoning models studied significantly outperform instruction-tuned baselines, yet all exhibit distinct vulnerability profiles, with misleading suggestions universally effective and social pressure showing model-specific efficacy. Through trajectory analysis, we identify five failure modes (Self-Doubt, Social Conformity, Suggestion Hijacking, Emotional Susceptibility, and Reasoning Fatigue) with the first two accounting for 50% of failures. We further demonstrate that Confidence-Aware Response Generation (CARG), effective for standard LLMs, fails for reasoning models due to overconfidence induced by extended reasoning traces; counterintuitively, random confidence embedding outperforms targeted extraction. Our results highlight that reasoning capabilities do not automatically confer adversarial robustness and that confidence-based defenses require fundamental redesign for reasoning models.",
    "link": "http://arxiv.org/abs/2602.13093v1",
    "date": "2026-02-13",
    "fetched_at": "2026-02-16"
  },
  {
    "id": "http://arxiv.org/abs/2602.13084v1",
    "title": "Exploring a New Competency Modeling Process with Large Language Models",
    "authors": [
      "Silin Du",
      "Manqing Xin",
      "Raymond Jia Wang"
    ],
    "summary": "Competency modeling is widely used in human resource management to select, develop, and evaluate talent. However, traditional expert-driven approaches rely heavily on manual analysis of large volumes of interview transcripts, making them costly and prone to randomness, ambiguity, and limited reproducibility. This study proposes a new competency modeling process built on large language models (LLMs). Instead of merely automating isolated steps, we reconstruct the workflow by decomposing expert practices into structured computational components. Specifically, we leverage LLMs to extract behavioral and psychological descriptions from raw textual data and map them to predefined competency libraries through embedding-based similarity. We further introduce a learnable parameter that adaptively integrates different information sources, enabling the model to determine the relative importance of behavioral and psychological signals. To address the long-standing challenge of validation, we develop an offline evaluation procedure that allows systematic model selection without requiring additional large-scale data collection. Empirical results from a real-world implementation in a software outsourcing company demonstrate strong predictive validity, cross-library consistency, and structural robustness. Overall, our framework transforms competency modeling from a largely qualitative and expert-dependent practice into a transparent, data-driven, and evaluable analytical process.",
    "link": "http://arxiv.org/abs/2602.13084v1",
    "date": "2026-02-13",
    "fetched_at": "2026-02-16"
  },
  {
    "id": "http://arxiv.org/abs/2602.13073v1",
    "title": "LCSB: Layer-Cyclic Selective Backpropagation for Memory-Efficient On-Device LLM Fine-Tuning",
    "authors": [
      "Juneyoung Park",
      "Eunbeen Yoon",
      "Seongwan Kim. Jaeho Lee"
    ],
    "summary": "Memory-efficient backpropagation (MeBP) has enabled first-order fine-tuning of large language models (LLMs) on mobile devices with less than 1GB memory. However, MeBP requires backward computation through all transformer layers at every step, where weight decompression alone accounts for 32--42% of backward time. We propose Layer-Cyclic Selective Backpropagation (LCSB), which computes gradients for only a subset of layers per step. Our key insight is that residual connections guarantee gradient flow through identity paths, while AdamW momentum provides implicit updates for non-selected layers. We interpret LCSB as Block Coordinate Descent on the LoRA parameter space, providing theoretical justification for convergence. LCSB achieves up to 1.40$\\times$ speedup with less than 2\\% quality degradation across five models and three tasks. Surprisingly, in 4-bit quantized settings, LCSB exhibits superior stability: a 3B model that completely diverges under full backpropagation converges smoothly with LCSB, suggesting an implicit regularization effect from selective gradient computation.",
    "link": "http://arxiv.org/abs/2602.13073v1",
    "date": "2026-02-13",
    "fetched_at": "2026-02-16"
  },
  {
    "id": "http://arxiv.org/abs/2602.13071v1",
    "title": "Bus-Conditioned Zero-Shot Trajectory Generation via Task Arithmetic",
    "authors": [
      "Shuai Liu",
      "Ning Cao",
      "Yile Chen",
      "Yue Jiang",
      "Gao Cong"
    ],
    "summary": "Mobility trajectory data provide essential support for smart city applications. However, such data are often difficult to obtain. Meanwhile, most existing trajectory generation methods implicitly assume that at least a subset of real mobility data from target city is available, which limits their applicability in data-inaccessible scenarios. In this work, we propose a new problem setting, called bus-conditioned zero-shot trajectory generation, where no mobility trajectories from a target city are accessible. The generation process relies solely on source city mobility data and publicly available bus timetables from both cities. Under this setting, we propose MobTA, the first approach to introduce task arithmetic into trajectory generation. MobTA models the parameter shift from bus-timetable-based trajectory generation to mobility trajectory generation in source city, and applies this shift to target city through arithmetic operations on task vectors. This enables trajectory generation that reflects target-city mobility patterns without requiring any real mobility data from it. Furthermore, we theoretically analyze MobTA's stability across base and instruction-tuned LLMs. Extensive experiments show that MobTA significantly outperforms existing methods, and achieves performance close to models finetuned using target city mobility trajectories.",
    "link": "http://arxiv.org/abs/2602.13071v1",
    "date": "2026-02-13",
    "fetched_at": "2026-02-16"
  },
  {
    "id": "http://arxiv.org/abs/2602.13069v1",
    "title": "Memory-Efficient Structured Backpropagation for On-Device LLM Fine-Tuning",
    "authors": [
      "Juneyoung Park",
      "Yuri Hong",
      "Seongwan Kim",
      "Jaeho Lee"
    ],
    "summary": "On-device fine-tuning enables privacy-preserving personalization of large language models, but mobile devices impose severe memory constraints, typically 6--12GB shared across all workloads. Existing approaches force a trade-off between exact gradients with high memory (MeBP) and low memory with noisy estimates (MeZO). We propose Memory-efficient Structured Backpropagation (MeSP), which bridges this gap by manually deriving backward passes that exploit LoRA's low-rank structure. Our key insight is that the intermediate projection $h = xA$ can be recomputed during backward at minimal cost since rank $r \\ll d_{in}$, eliminating the need to store it. MeSP achieves 49\\% average memory reduction compared to MeBP on Qwen2.5 models (0.5B--3B) while computing mathematically identical gradients. Our analysis also reveals that MeZO's gradient estimates show near-zero correlation with true gradients (cosine similarity $\\approx$0.001), explaining its slow convergence. MeSP reduces peak memory from 361MB to 136MB for Qwen2.5-0.5B, enabling fine-tuning scenarios previously infeasible on memory-constrained devices.",
    "link": "http://arxiv.org/abs/2602.13069v1",
    "date": "2026-02-13",
    "fetched_at": "2026-02-16"
  },
  {
    "id": "http://arxiv.org/abs/2602.13042v1",
    "title": "GPTZero: Robust Detection of LLM-Generated Texts",
    "authors": [
      "George Alexandru Adam",
      "Alexander Cui",
      "Edwin Thomas",
      "Emily Napier",
      "Nazar Shmatko",
      "Jacob Schnell",
      "Jacob Junqi Tian",
      "Alekhya Dronavalli",
      "Edward Tian",
      "Dongwon Lee"
    ],
    "summary": "While historical considerations surrounding text authenticity revolved primarily around plagiarism, the advent of large language models (LLMs) has introduced a new challenge: distinguishing human-authored from AI-generated text. This shift raises significant concerns, including the undermining of skill evaluations, the mass-production of low-quality content, and the proliferation of misinformation. Addressing these issues, we introduce GPTZero a state-of-the-art industrial AI detection solution, offering reliable discernment between human and LLM-generated text. Our key contributions include: introducing a hierarchical, multi-task architecture enabling a flexible taxonomy of human and AI texts, demonstrating state-of-the-art accuracy on a variety of domains with granular predictions, and achieving superior robustness to adversarial attacks and paraphrasing via multi-tiered automated red teaming. GPTZero offers accurate and explainable detection, and educates users on its responsible use, ensuring fair and transparent assessment of text.",
    "link": "http://arxiv.org/abs/2602.13042v1",
    "date": "2026-02-13",
    "fetched_at": "2026-02-16"
  },
  {
    "id": "http://arxiv.org/abs/2602.13035v1",
    "title": "Look Inward to Explore Outward: Learning Temperature Policy from LLM Internal States via Hierarchical RL",
    "authors": [
      "Yixiao Zhou",
      "Yang Li",
      "Dongzhou Cheng",
      "Hehe Fan",
      "Yu Cheng"
    ],
    "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) trains large language models (LLMs) from sampled trajectories, making decoding strategy a core component of learning rather than a purely inference-time choice. Sampling temperature directly controls the exploration--exploitation trade-off by modulating policy entropy, yet existing methods rely on static values or heuristic adaptations that are decoupled from task-level rewards. We propose Introspective LLM, a hierarchical reinforcement learning framework that learns to control sampling temperature during generation. At each decoding step, the model selects a temperature based on its hidden state and samples the next token from the resulting distribution. Temperature and token policies are jointly optimized from downstream rewards using a coordinate ascent scheme. Experiments on mathematical reasoning benchmarks show that learned temperature policies outperform fixed and heuristic baselines, while exhibiting interpretable exploration behaviors aligned with reasoning uncertainty.",
    "link": "http://arxiv.org/abs/2602.13035v1",
    "date": "2026-02-13",
    "fetched_at": "2026-02-16"
  },
  {
    "id": "http://arxiv.org/abs/2602.13033v1",
    "title": "Buy versus Build an LLM: A Decision Framework for Governments",
    "authors": [
      "Jiahao Lu",
      "Ziwei Xu",
      "William Tjhi",
      "Junnan Li",
      "Antoine Bosselut",
      "Pang Wei Koh",
      "Mohan Kankanhalli"
    ],
    "summary": "Large Language Models (LLMs) represent a new frontier of digital infrastructure that can support a wide range of public-sector applications, from general purpose citizen services to specialized and sensitive state functions. When expanding AI access, governments face a set of strategic choices over whether to buy existing services, build domestic capabilities, or adopt hybrid approaches across different domains and use cases. These are critical decisions especially when leading model providers are often foreign corporations, and LLM outputs are increasingly treated as trusted inputs to public decision-making and public discourse. In practice, these decisions are not intended to mandate a single approach across all domains; instead, national AI strategies are typically pluralistic, with sovereign, commercial and open-source models coexisting to serve different purposes. Governments may rely on commercial models for non-sensitive or commodity tasks, while pursuing greater control for critical, high-risk or strategically important applications.   This paper provides a strategic framework for making this decision by evaluating these options across dimensions including sovereignty, safety, cost, resource capability, cultural fit, and sustainability. Importantly, \"building\" does not imply that governments must act alone: domestic capabilities may be developed through public research institutions, universities, state-owned enterprises, joint ventures, or broader national ecosystems. By detailing the technical requirements and practical challenges of each pathway, this work aims to serve as a reference for policy-makers to determine whether a buy or build approach best aligns with their specific national needs and societal goals.",
    "link": "http://arxiv.org/abs/2602.13033v1",
    "date": "2026-02-13",
    "fetched_at": "2026-02-16"
  },
  {
    "id": "http://arxiv.org/abs/2602.12996v1",
    "title": "Know More, Know Clearer: A Meta-Cognitive Framework for Knowledge Augmentation in Large Language Models",
    "authors": [
      "Hao Chen",
      "Ye He",
      "Yuchun Fan",
      "Yukun Yan",
      "Zhenghao Liu",
      "Qingfu Zhu",
      "Maosong Sun",
      "Wanxiang Che"
    ],
    "summary": "Knowledge augmentation has significantly enhanced the performance of Large Language Models (LLMs) in knowledge-intensive tasks. However, existing methods typically operate on the simplistic premise that model performance equates with internal knowledge, overlooking the knowledge-confidence gaps that lead to overconfident errors or uncertain truths. To bridge this gap, we propose a novel meta-cognitive framework for reliable knowledge augmentation via differentiated intervention and alignment. Our approach leverages internal cognitive signals to partition the knowledge space into mastered, confused, and missing regions, guiding targeted knowledge expansion. Furthermore, we introduce a cognitive consistency mechanism to synchronize subjective certainty with objective accuracy, ensuring calibrated knowledge boundaries. Extensive experiments demonstrate the our framework consistently outperforms strong baselines, validating its rationality in not only enhancing knowledge capabilities but also fostering cognitive behaviors that better distinguish knowns from unknowns.",
    "link": "http://arxiv.org/abs/2602.12996v1",
    "date": "2026-02-13",
    "fetched_at": "2026-02-16"
  },
  {
    "id": "http://arxiv.org/abs/2602.12984v1",
    "title": "SciAgentGym: Benchmarking Multi-Step Scientific Tool-use in LLM Agents",
    "authors": [
      "Yujiong Shen",
      "Yajie Yang",
      "Zhiheng Xi",
      "Binze Hu",
      "Huayu Sha",
      "Jiazheng Zhang",
      "Qiyuan Peng",
      "Junlin Shang",
      "Jixuan Huang",
      "Yutao Fan",
      "Jingqi Tong",
      "Shihan Dou",
      "Ming Zhang",
      "Lei Bai",
      "Zhenfei Yin",
      "Tao Gui",
      "Xingjun Ma",
      "Qi Zhang",
      "Xuanjing Huang",
      "Yu-Gang Jiang"
    ],
    "summary": "Scientific reasoning inherently demands integrating sophisticated toolkits to navigate domain-specific knowledge. Yet, current benchmarks largely overlook agents' ability to orchestrate tools for such rigorous workflows. To bridge this gap, we introduce SciAgentGym, a scalable interactive environment featuring 1,780 domain-specific tools across four natural science disciplines, supported by a robust execution infrastructure. Complementing this, we present SciAgentBench, a tiered evaluation suite designed to stress-test agentic capabilities from elementary actions to long-horizon workflows. Our evaluation identifies a critical bottleneck: state-of-the-art models struggle with complex scientific tool-use. Even for a leading model like GPT-5, success rates drop sharply from 60.6% to 30.9% as interaction horizons extend, primarily due to failures in multi-step workflow execution. To address this, we propose SciForge, a data synthesis method that models the tool action space as a dependency graph to generate logic-aware training trajectories. By fine-tuning on these trajectories, our SciAgent-8B outperforms the significantly larger Qwen3-VL-235B-Instruct while exhibiting positive cross-domain transfer of scientific tool-use capabilities. These results underscore the promising potential of next-generation autonomous scientific agents.",
    "link": "http://arxiv.org/abs/2602.12984v1",
    "date": "2026-02-13",
    "fetched_at": "2026-02-16"
  },
  {
    "id": "http://arxiv.org/abs/2602.12971v1",
    "title": "INHerit-SG: Incremental Hierarchical Semantic Scene Graphs with RAG-Style Retrieval",
    "authors": [
      "YukTungSamuel Fang",
      "Zhikang Shi",
      "Jiabin Qiu",
      "Zixuan Chen",
      "Jieqi Shi",
      "Hao Xu",
      "Jing Huo",
      "Yang Gao"
    ],
    "summary": "Driven by advancements in foundation models, semantic scene graphs have emerged as a prominent paradigm for high-level 3D environmental abstraction in robot navigation. However, existing approaches are fundamentally misaligned with the needs of embodied tasks. As they rely on either offline batch processing or implicit feature embeddings, the maps can hardly support interpretable human-intent reasoning in complex environments. To address these limitations, we present INHerit-SG. We redefine the map as a structured, RAG-ready knowledge base where natural-language descriptions are introduced as explicit semantic anchors to better align with human intent. An asynchronous dual-process architecture, together with a Floor-Room-Area-Object hierarchy, decouples geometric segmentation from time-consuming semantic reasoning. An event-triggered map update mechanism reorganizes the graph only when meaningful semantic events occur. This strategy enables our graph to maintain long-term consistency with relatively low computational overhead. For retrieval, we deploy multi-role Large Language Models (LLMs) to decompose queries into atomic constraints and handle logical negations, and employ a hard-to-soft filtering strategy to ensure robust reasoning. This explicit interpretability improves the success rate and reliability of complex retrievals, enabling the system to adapt to a broader spectrum of human interaction tasks. We evaluate INHerit-SG on a newly constructed dataset, HM3DSem-SQR, and in real-world environments. Experiments demonstrate that our system achieves state-of-the-art performance on complex queries, and reveal its scalability for downstream navigation tasks. Project Page: https://fangyuktung.github.io/INHeritSG.github.io/",
    "link": "http://arxiv.org/abs/2602.12971v1",
    "date": "2026-02-13",
    "fetched_at": "2026-02-16"
  },
  {
    "id": "http://arxiv.org/abs/2602.12968v1",
    "title": "RGAlign-Rec: Ranking-Guided Alignment for Latent Query Reasoning in Recommendation Systems",
    "authors": [
      "Junhua Liu",
      "Yang Jihao",
      "Cheng Chang",
      "Kunrong LI",
      "Bin Fu",
      "Kwan Hui Lim"
    ],
    "summary": "Proactive intent prediction is a critical capability in modern e-commerce chatbots, enabling \"zero-query\" recommendations by anticipating user needs from behavioral and contextual signals. However, existing industrial systems face two fundamental challenges: (1) the semantic gap between discrete user features and the semantic intents within the chatbot's Knowledge Base, and (2) the objective misalignment between general-purpose LLM outputs and task-specific ranking utilities. To address these issues, we propose RGAlign-Rec, a closed-loop alignment framework that integrates an LLM-based semantic reasoner with a Query-Enhanced (QE) ranking model. We also introduce Ranking-Guided Alignment (RGA), a multi-stage training paradigm that utilizes downstream ranking signals as feedback to refine the LLM's latent reasoning. Extensive experiments on a large-scale industrial dataset from Shopee demonstrate that RGAlign-Rec achieves a 0.12% gain in GAUC, leading to a significant 3.52% relative reduction in error rate, and a 0.56% improvement in Recall@3. Online A/B testing further validates the cumulative effectiveness of our framework: the Query-Enhanced model (QE-Rec) initially yields a 0.98% improvement in CTR, while the subsequent Ranking-Guided Alignment stage contributes an additional 0.13% gain. These results indicate that ranking-aware alignment effectively synchronizes semantic reasoning with ranking objectives, significantly enhancing both prediction accuracy and service quality in real-world proactive recommendation systems.",
    "link": "http://arxiv.org/abs/2602.12968v1",
    "date": "2026-02-13",
    "fetched_at": "2026-02-16"
  },
  {
    "id": "http://arxiv.org/abs/2602.12966v1",
    "title": "ProbeLLM: Automating Principled Diagnosis of LLM Failures",
    "authors": [
      "Yue Huang",
      "Zhengzhe Jiang",
      "Yuchen Ma",
      "Yu Jiang",
      "Xiangqi Wang",
      "Yujun Zhou",
      "Yuexing Hao",
      "Kehan Guo",
      "Pin-Yu Chen",
      "Stefan Feuerriegel",
      "Xiangliang Zhang"
    ],
    "summary": "Understanding how and why large language models (LLMs) fail is becoming a central challenge as models rapidly evolve and static evaluations fall behind. While automated probing has been enabled by dynamic test generation, existing approaches often discover isolated failure cases, lack principled control over exploration, and provide limited insight into the underlying structure of model weaknesses. We propose ProbeLLM, a benchmark-agnostic automated probing framework that elevates weakness discovery from individual failures to structured failure modes. ProbeLLM formulates probing as a hierarchical Monte Carlo Tree Search, explicitly allocating limited probing budgets between global exploration of new failure regions and local refinement of recurring error patterns. By restricting probing to verifiable test cases and leveraging tool-augmented generation and verification, ProbeLLM grounds failure discovery in reliable evidence. Discovered failures are further consolidated into interpretable failure modes via failure-aware embeddings and boundary-aware induction. Across diverse benchmarks and LLMs, ProbeLLM reveals substantially broader, cleaner, and more fine-grained failure landscapes than static benchmarks and prior automated methods, supporting a shift from case-centric evaluation toward principled weakness discovery.",
    "link": "http://arxiv.org/abs/2602.12966v1",
    "date": "2026-02-13",
    "fetched_at": "2026-02-16"
  },
  {
    "id": "http://arxiv.org/abs/2602.06964v1",
    "title": "Learning a Generative Meta-Model of LLM Activations",
    "authors": [
      "Grace Luo",
      "Jiahai Feng",
      "Trevor Darrell",
      "Alec Radford",
      "Jacob Steinhardt"
    ],
    "summary": "Existing approaches for analyzing neural network activations, such as PCA and sparse autoencoders, rely on strong structural assumptions. Generative models offer an alternative: they can uncover structure without such assumptions and act as priors that improve intervention fidelity. We explore this direction by training diffusion models on one billion residual stream activations, creating \"meta-models\" that learn the distribution of a network's internal states. We find that diffusion loss decreases smoothly with compute and reliably predicts downstream utility. In particular, applying the meta-model's learned prior to steering interventions improves fluency, with larger gains as loss decreases. Moreover, the meta-model's neurons increasingly isolate concepts into individual units, with sparse probing scores that scale as loss decreases. These results suggest generative meta-models offer a scalable path toward interpretability without restrictive structural assumptions. Project page: https://generative-latent-prior.github.io.",
    "link": "http://arxiv.org/abs/2602.06964v1",
    "date": "2026-02-06",
    "fetched_at": "2026-02-09"
  },
  {
    "id": "http://arxiv.org/abs/2602.06953v1",
    "title": "DAWN: Dependency-Aware Fast Inference for Diffusion LLMs",
    "authors": [
      "Lizhuo Luo",
      "Zhuoran Shi",
      "Jiajun Luo",
      "Zhi Wang",
      "Shen Ren",
      "Wenya Wang",
      "Tianwei Zhang"
    ],
    "summary": "Diffusion large language models (dLLMs) have shown advantages in text generation, particularly due to their inherent ability for parallel decoding. However, constrained by the quality--speed trade-off, existing inference solutions adopt conservative parallel strategies, leaving substantial efficiency potential underexplored. A core challenge is that parallel decoding assumes each position can be filled independently, but tokens are often semantically coupled. Thus, the correct choice at one position constrains valid choices at others. Without modeling these inter-token dependencies, parallel strategies produce deteriorated outputs. Motivated by this insight, we propose DAWN, a training-free, dependency-aware decoding method for fast dLLM inference. DAWN extracts token dependencies and leverages two key motivations: (1) positions dependent on unmasked certain positions become more reliable, (2) simultaneously unmasking strongly coupled uncertain positions induces errors. Given those findings, DAWN leverages a dependency graph to select more reliable unmasking positions at each iteration, achieving high parallelism with negligible loss in generation quality. Extensive experiments across multiple models and datasets demonstrate that DAWN speedups the inference by 1.80-8.06x over baselines while preserving the generation quality. Code is released at https://github.com/lizhuo-luo/DAWN.",
    "link": "http://arxiv.org/abs/2602.06953v1",
    "date": "2026-02-06",
    "fetched_at": "2026-02-09"
  },
  {
    "id": "http://arxiv.org/abs/2602.06932v1",
    "title": "When RL Meets Adaptive Speculative Training: A Unified Training-Serving System",
    "authors": [
      "Junxiong Wang",
      "Fengxiang Bie",
      "Jisen Li",
      "Zhongzhu Zhou",
      "Zelei Shao",
      "Yubo Wang",
      "Yinghui Liu",
      "Qingyang Wu",
      "Avner May",
      "Sri Yanamandra",
      "Yineng Zhang",
      "Ce Zhang",
      "Tri Dao",
      "Percy Liang",
      "Ben Athiwaratkun",
      "Shuaiwen Leon Song",
      "Chenfeng Xu",
      "Xiaoxia Wu"
    ],
    "summary": "Speculative decoding can significantly accelerate LLM serving, yet most deployments today disentangle speculator training from serving, treating speculator training as a standalone offline modeling problem. We show that this decoupled formulation introduces substantial deployment and adaptation lag: (1) high time-to-serve, since a speculator must be trained offline for a considerable period before deployment; (2) delayed utility feedback, since the true end-to-end decoding speedup is only known after training and cannot be inferred reliably from acceptance rate alone due to model-architecture and system-level overheads; and (3) domain-drift degradation, as the target model is repurposed to new domains and the speculator becomes stale and less effective.   To address these issues, we present Aurora, a unified training-serving system that closes the loop by continuously learning a speculator directly from live inference traces. Aurora reframes online speculator learning as an asynchronous reinforcement-learning problem: accepted tokens provide positive feedback, while rejected speculator proposals provide implicit negative feedback that we exploit to improve sample efficiency. Our design integrates an SGLang-based inference server with an asynchronous training server, enabling hot-swapped speculator updates without service interruption. Crucially, Aurora supports day-0 deployment: a speculator can be served immediately and rapidly adapted to live traffic, improving system performance while providing immediate utility feedback. Across experiments, Aurora achieves a 1.5x day-0 speedup on recently released frontier models (e.g., MiniMax M2.1 229B and Qwen3-Coder-Next 80B). Aurora also adapts effectively to distribution shifts in user traffic, delivering an additional 1.25x speedup over a well-trained but static speculator on widely used models (e.g., Qwen3 and Llama3).",
    "link": "http://arxiv.org/abs/2602.06932v1",
    "date": "2026-02-06",
    "fetched_at": "2026-02-09"
  },
  {
    "id": "http://arxiv.org/abs/2602.06920v1",
    "title": "Halluverse-M^3: A multitask multilingual benchmark for hallucination in LLMs",
    "authors": [
      "Samir Abdaljalil",
      "Parichit Sharma",
      "Erchin Serpedin",
      "Hasan Kurban"
    ],
    "summary": "Hallucinations in large language models remain a persistent challenge, particularly in multilingual and generative settings where factual consistency is difficult to maintain. While recent models show strong performance on English-centric benchmarks, their behavior across languages, tasks, and hallucination types is not yet well understood. In this work, we introduce Halluverse-M^3, a dataset designed to enable systematic analysis of hallucinations across multiple languages, multiple generation tasks, and multiple hallucination categories. Halluverse-M^3 covers four languages, English, Arabic, Hindi, and Turkish, and supports two generation tasks: question answering and dialogue summarization. The dataset explicitly distinguishes between entity-level, relation-level, and sentence-level hallucinations. Hallucinated outputs are constructed through a controlled editing process and validated by human annotators, ensuring clear alignment between original content and hallucinated generations. Using this dataset, we evaluate a diverse set of contemporary open-source and proprietary language models on fine-grained hallucination detection. Our results show that question answering is consistently easier than dialogue summarization, while sentence-level hallucinations remain challenging even for the strongest models. Performance is highest in English and degrades in lower-resource languages, with Hindi exhibiting the lowest detection accuracy. Overall, Halluverse-M^3 provides a realistic and challenging benchmark for studying hallucinations in multilingual, multi-task settings. We release the dataset to support future research on hallucination detection and mitigation\\footnote{https://huggingface.co/datasets/sabdalja/HalluVerse-M3}.",
    "link": "http://arxiv.org/abs/2602.06920v1",
    "date": "2026-02-06",
    "fetched_at": "2026-02-09"
  },
  {
    "id": "http://arxiv.org/abs/2602.06911v1",
    "title": "TamperBench: Systematically Stress-Testing LLM Safety Under Fine-Tuning and Tampering",
    "authors": [
      "Saad Hossain",
      "Tom Tseng",
      "Punya Syon Pandey",
      "Samanvay Vajpayee",
      "Matthew Kowal",
      "Nayeema Nonta",
      "Samuel Simko",
      "Stephen Casper",
      "Zhijing Jin",
      "Kellin Pelrine",
      "Sirisha Rambhatla"
    ],
    "summary": "As increasingly capable open-weight large language models (LLMs) are deployed, improving their tamper resistance against unsafe modifications, whether accidental or intentional, becomes critical to minimize risks. However, there is no standard approach to evaluate tamper resistance. Varied data sets, metrics, and tampering configurations make it difficult to compare safety, utility, and robustness across different models and defenses. To this end, we introduce TamperBench, the first unified framework to systematically evaluate the tamper resistance of LLMs. TamperBench (i) curates a repository of state-of-the-art weight-space fine-tuning attacks and latent-space representation attacks; (ii) enables realistic adversarial evaluation through systematic hyperparameter sweeps per attack-model pair; and (iii) provides both safety and utility evaluations. TamperBench requires minimal additional code to specify any fine-tuning configuration, alignment-stage defense method, and metric suite while ensuring end-to-end reproducibility. We use TamperBench to evaluate 21 open-weight LLMs, including defense-augmented variants, across nine tampering threats using standardized safety and capability metrics with hyperparameter sweeps per model-attack pair. This yields novel insights, including effects of post-training on tamper resistance, that jailbreak-tuning is typically the most severe attack, and that Triplet emerges as a leading alignment-stage defense. Code is available at: https://github.com/criticalml-uw/TamperBench",
    "link": "http://arxiv.org/abs/2602.06911v1",
    "date": "2026-02-06",
    "fetched_at": "2026-02-09"
  },
  {
    "id": "http://arxiv.org/abs/2602.06887v1",
    "title": "Plato's Form: Toward Backdoor Defense-as-a-Service for LLMs with Prototype Representations",
    "authors": [
      "Chen Chen",
      "Yuchen Sun",
      "Jiaxin Gao",
      "Yanwen Jia",
      "Xueluan Gong",
      "Qian Wang",
      "Kwok-Yan Lam"
    ],
    "summary": "Large language models (LLMs) are increasingly deployed in security-sensitive applications, yet remain vulnerable to backdoor attacks. However, existing backdoor defenses are difficult to operationalize for Backdoor Defense-as-a-Service (BDaaS), as they require unrealistic side information (e.g., downstream clean data, known triggers/targets, or task domain specifics), and lack reusable, scalable purification across diverse backdoored models. In this paper, we present PROTOPURIFY, a backdoor purification framework via parameter edits under minimal assumptions. PROTOPURIFY first builds a backdoor vector pool from clean and backdoored model pairs, aggregates vectors into candidate prototypes, and selects the most aligned candidate for the target model via similarity matching. PROTOPURIFY then identifies a boundary layer through layer-wise prototype alignment and performs targeted purification by suppressing prototype-aligned components in the affected layers, achieving fine-grained mitigation with minimal impact on benign utility. Designed as a BDaaS-ready primitive, PROTOPURIFY supports reusability, customizability, interpretability, and runtime efficiency. Experiments across various LLMs on both classification and generation tasks show that PROTOPURIFY consistently outperforms 6 representative defenses against 6 diverse attacks, including single-trigger, multi-trigger, and triggerless backdoor settings. PROTOPURIFY reduces ASR to below 10%, and even as low as 1.6% in some cases, while incurring less than a 3% drop in clean utility. PROTOPURIFY further demonstrates robustness against adaptive backdoor variants and stability on non-backdoored models.",
    "link": "http://arxiv.org/abs/2602.06887v1",
    "date": "2026-02-06",
    "fetched_at": "2026-02-09"
  },
  {
    "id": "http://arxiv.org/abs/2602.06875v1",
    "title": "TraceCoder: A Trace-Driven Multi-Agent Framework for Automated Debugging of LLM-Generated Code",
    "authors": [
      "Jiangping Huang",
      "Wenguang Ye",
      "Weisong Sun",
      "Jian Zhang",
      "Mingyue Zhang",
      "Yang Liu"
    ],
    "summary": "Large Language Models (LLMs) often generate code with subtle but critical bugs, especially for complex tasks. Existing automated repair methods typically rely on superficial pass/fail signals, offering limited visibility into program behavior and hindering precise error localization. In addition, without a way to learn from prior failures, repair processes often fall into repetitive and inefficient cycles. To overcome these challenges, we present TraceCoder, a collaborative multi-agent framework that emulates the observe-analyze-repair process of human experts. The framework first instruments the code with diagnostic probes to capture fine-grained runtime traces, enabling deep insight into its internal execution. It then conducts causal analysis on these traces to accurately identify the root cause of the failure. This process is further enhanced by a novel Historical Lesson Learning Mechanism (HLLM), which distills insights from prior failed repair attempts to inform subsequent correction strategies and prevent recurrence of similar mistakes. To ensure stable convergence, a Rollback Mechanism enforces that each repair iteration constitutes a strict improvement toward the correct solution. Comprehensive experiments across multiple benchmarks show that TraceCoder achieves up to a 34.43\\% relative improvement in Pass@1 accuracy over existing advanced baselines. Ablation studies verify the significance of each system component, with the iterative repair process alone contributing a 65.61\\% relative gain in accuracy. Furthermore, TraceCoder significantly outperforms leading iterative methods in terms of both accuracy and cost-efficiency.",
    "link": "http://arxiv.org/abs/2602.06875v1",
    "date": "2026-02-06",
    "fetched_at": "2026-02-09"
  },
  {
    "id": "http://arxiv.org/abs/2602.06869v1",
    "title": "Uncovering Cross-Objective Interference in Multi-Objective Alignment",
    "authors": [
      "Yining Lu",
      "Meng Jiang"
    ],
    "summary": "We study a persistent failure mode in multi-objective alignment for large language models (LLMs): training improves performance on only a subset of objectives while causing others to degrade. We formalize this phenomenon as cross-objective interference and conduct the first systematic study across classic scalarization algorithms, showing that interference is pervasive and exhibits strong model dependence.   To explain this phenomenon, we derive a local covariance law showing that an objective improves at first order when its reward exhibits positive covariance with the scalarized score. We extend this analysis to clipped surrogate objectives used in modern alignment, demonstrating that the covariance law remains valid under mild conditions despite clipping. Building on this analysis, we propose Covariance Targeted Weight Adaptation (CTWA), a plug-and-play method that maintains positive covariance between objective rewards and the training signal to effectively mitigate cross-objective interference. Finally, we complement these local improvement conditions with a global convergence analysis under the Polyak--\u0141ojasiewicz condition, establishing when non-convex scalarized optimization achieves global convergence and how cross-objective interference depends on specific model geometric properties.",
    "link": "http://arxiv.org/abs/2602.06869v1",
    "date": "2026-02-06",
    "fetched_at": "2026-02-09"
  },
  {
    "id": "http://arxiv.org/abs/2602.06855v1",
    "title": "AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents",
    "authors": [
      "Alisia Lupidi",
      "Bhavul Gauri",
      "Thomas Simon Foster",
      "Bassel Al Omari",
      "Despoina Magka",
      "Alberto Pepe",
      "Alexis Audran-Reiss",
      "Muna Aghamelu",
      "Nicolas Baldwin",
      "Lucia Cipolina-Kun",
      "Jean-Christophe Gagnon-Audet",
      "Chee Hau Leow",
      "Sandra Lefdal",
      "Hossam Mossalam",
      "Abhinav Moudgil",
      "Saba Nazir",
      "Emanuel Tewolde",
      "Isabel Urrego",
      "Jordi Armengol Estape",
      "Amar Budhiraja",
      "Gaurav Chaurasia",
      "Abhishek Charnalia",
      "Derek Dunfield",
      "Karen Hambardzumyan",
      "Daniel Izcovich",
      "Martin Josifoski",
      "Ishita Mediratta",
      "Kelvin Niu",
      "Parth Pathak",
      "Michael Shvartsman",
      "Edan Toledo",
      "Anton Protopopov",
      "Roberta Raileanu",
      "Alexander Miller",
      "Tatiana Shavrina",
      "Jakob Foerster",
      "Yoram Bachrach"
    ],
    "summary": "LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds. Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.",
    "link": "http://arxiv.org/abs/2602.06855v1",
    "date": "2026-02-06",
    "fetched_at": "2026-02-09"
  },
  {
    "id": "http://arxiv.org/abs/2602.06854v1",
    "title": "SEMA: Simple yet Effective Learning for Multi-Turn Jailbreak Attacks",
    "authors": [
      "Mingqian Feng",
      "Xiaodong Liu",
      "Weiwei Yang",
      "Jialin Song",
      "Xuekai Zhu",
      "Chenliang Xu",
      "Jianfeng Gao"
    ],
    "summary": "Multi-turn jailbreaks capture the real threat model for safety-aligned chatbots, where single-turn attacks are merely a special case. Yet existing approaches break under exploration complexity and intent drift. We propose SEMA, a simple yet effective framework that trains a multi-turn attacker without relying on any existing strategies or external data. SEMA comprises two stages. Prefilling self-tuning enables usable rollouts by fine-tuning on non-refusal, well-structured, multi-turn adversarial prompts that are self-generated with a minimal prefix, thereby stabilizing subsequent learning. Reinforcement learning with intent-drift-aware reward trains the attacker to elicit valid multi-turn adversarial prompts while maintaining the same harmful objective. We anchor harmful intent in multi-turn jailbreaks via an intent-drift-aware reward that combines intent alignment, compliance risk, and level of detail. Our open-loop attack regime avoids dependence on victim feedback, unifies single- and multi-turn settings, and reduces exploration complexity. Across multiple datasets, victim models, and jailbreak judges, our method achieves state-of-the-art (SOTA) attack success rates (ASR), outperforming all single-turn baselines, manually scripted and template-driven multi-turn baselines, as well as our SFT (Supervised Fine-Tuning) and DPO (Direct Preference Optimization) variants. For instance, SEMA performs an average $80.1\\%$ ASR@1 across three closed-source and open-source victim models on AdvBench, 33.9% over SOTA. The approach is compact, reproducible, and transfers across targets, providing a stronger and more realistic stress test for large language model (LLM) safety and enabling automatic redteaming to expose and localize failure modes. Our code is available at: https://github.com/fmmarkmq/SEMA.",
    "link": "http://arxiv.org/abs/2602.06854v1",
    "date": "2026-02-06",
    "fetched_at": "2026-02-09"
  },
  {
    "id": "http://arxiv.org/abs/2602.06852v1",
    "title": "The Quantum Sieve Tracer: A Hybrid Framework for Layer-Wise Activation Tracing in Large Language Models",
    "authors": [
      "Jonathan Pan"
    ],
    "summary": "Mechanistic interpretability aims to reverse-engineer the internal computations of Large Language Models (LLMs), yet separating sparse semantic signals from high-dimensional polysemantic noise remains a significant challenge. This paper introduces the Quantum Sieve Tracer, a hybrid quantum-classical framework designed to characterize factual recall circuits. We implement a modular pipeline that first localizes critical layers using classical causal tracing, then maps specific attention head activations into an exponentially large quantum Hilbert space. Using open-weight models (Meta Llama-3.2-1B and Alibaba Qwen2.5-1.5B-Instruct), we perform a two-stage analysis that reveals a fundamental architectural divergence. While Qwen's layer 7 circuit functions as a classic Recall Hub, we discover that Llama's layer 9 acts as an Interference Suppression circuit, where ablating the identified heads paradoxically improves factual recall. Our results demonstrate that quantum kernels can distinguish between these constructive (recall) and reductive (suppression) mechanisms, offering a high-resolution tool for analyzing the fine-grained topology of attention.",
    "link": "http://arxiv.org/abs/2602.06852v1",
    "date": "2026-02-06",
    "fetched_at": "2026-02-09"
  },
  {
    "id": "http://arxiv.org/abs/2602.06841v1",
    "title": "From Features to Actions: Explainability in Traditional and Agentic AI Systems",
    "authors": [
      "Sindhuja Chaduvula",
      "Jessee Ho",
      "Kina Kim",
      "Aravind Narayanan",
      "Mahshid Alinoori",
      "Muskan Garg",
      "Dhanesh Ramachandram",
      "Shaina Raza"
    ],
    "summary": "Over the last decade, explainable AI has primarily focused on interpreting individual model predictions, producing post-hoc explanations that relate inputs to outputs under a fixed decision structure. Recent advances in large language models (LLMs) have enabled agentic AI systems whose behaviour unfolds over multi-step trajectories. In these settings, success and failure are determined by sequences of decisions rather than a single output. While useful, it remains unclear how explanation approaches designed for static predictions translate to agentic settings where behaviour emerges over time. In this work, we bridge the gap between static and agentic explainability by comparing attribution-based explanations with trace-based diagnostics across both settings. To make this distinction explicit, we empirically compare attribution-based explanations used in static classification tasks with trace-based diagnostics used in agentic benchmarks (TAU-bench Airline and AssistantBench). Our results show that while attribution methods achieve stable feature rankings in static settings (Spearman $\u03c1= 0.86$), they cannot be applied reliably to diagnose execution-level failures in agentic trajectories. In contrast, trace-grounded rubric evaluation for agentic settings consistently localizes behaviour breakdowns and reveals that state tracking inconsistency is 2.7$\\times$ more prevalent in failed runs and reduces success probability by 49\\%. These findings motivate a shift towards trajectory-level explainability for agentic systems when evaluating and diagnosing autonomous AI behaviour.   Resources:   https://github.com/VectorInstitute/unified-xai-evaluation-framework https://vectorinstitute.github.io/unified-xai-evaluation-framework",
    "link": "http://arxiv.org/abs/2602.06841v1",
    "date": "2026-02-06",
    "fetched_at": "2026-02-09"
  },
  {
    "id": "http://arxiv.org/abs/2602.06836v1",
    "title": "LLM Active Alignment: A Nash Equilibrium Perspective",
    "authors": [
      "Tonghan Wang",
      "Yuqi Pan",
      "Xinyi Yang",
      "Yanchen Jiang",
      "Milind Tambe",
      "David C. Parkes"
    ],
    "summary": "We develop a game-theoretic framework for predicting and steering the behavior of populations of large language models (LLMs) through Nash equilibrium (NE) analysis. To avoid the intractability of equilibrium computation in open-ended text spaces, we model each agent's action as a mixture over human subpopulations. Agents choose actively and strategically which groups to align with, yielding an interpretable and behaviorally substantive policy class. We derive closed-form NE characterizations, adopting standard concave-utility assumptions to enable analytical system-level predictions and give explicit, actionable guidance for shifting alignment targets toward socially desirable outcomes. The method functions as an active alignment layer on top of existing alignment pipelines such as RLHF. In a social-media setting, we show that a population of LLMs, especially reasoning-based models, may exhibit political exclusion, pathologies where some subpopulations are ignored by all LLM agents, which can be avoided by our method, illustrating the promise of applying the method to regulate multi-agent LLM dynamics across domains.",
    "link": "http://arxiv.org/abs/2602.06836v1",
    "date": "2026-02-06",
    "fetched_at": "2026-02-09"
  },
  {
    "id": "http://arxiv.org/abs/2602.06822v1",
    "title": "POP: Online Structural Pruning Enables Efficient Inference of Large Foundation Models",
    "authors": [
      "Yi Chen",
      "Wonjin Shin",
      "Shuhong Liu",
      "Tho Mai",
      "Jeongmo Lee",
      "Chuanbo Hua",
      "Kun Wang",
      "Jun Liu",
      "Joo-Young Kim"
    ],
    "summary": "Large foundation models (LFMs) achieve strong performance through scaling, yet current structural pruning methods derive fixed pruning decisions during inference, overlooking sparsity patterns that emerge in the autoregressive token generation. In this paper, we propose POP (Partition-guided Online Pruning), an efficient online structural pruning framework that enables context-conditioned dynamic pruning with minimal computational overhead. POP partitions model channels into retained, candidate, and pruned regions, where prefilling defines a coarse pruning partition, and the decoding stage generates a fine-grained mask within the candidate region, avoiding full-channel re-evaluation. The coarse pruning partition preserves consistently important weights, while the fine-grained masking provides context-conditioned variation during decoding. Moreover, POP is a lightweight, plug-and-play method that requires no preprocessing, including offline calibration, retraining, or learning predictors. Extensive evaluations across diverse LFMs, including large language models (LLMs), mixture-of-experts models (MoEs), and vision-language models (VLMs), demonstrate that POP consistently delivers higher accuracy than existing pruning approaches while incurring smaller computational overhead and minimizing inference latency.",
    "link": "http://arxiv.org/abs/2602.06822v1",
    "date": "2026-02-06",
    "fetched_at": "2026-02-09"
  },
  {
    "id": "http://arxiv.org/abs/2602.06819v1",
    "title": "Bridging 6G IoT and AI: LLM-Based Efficient Approach for Physical Layer's Optimization Tasks",
    "authors": [
      "Ahsan Mehmood",
      "Naveed Ul Hassan",
      "Ghassan M. Kraidy"
    ],
    "summary": "This paper investigates the role of large language models (LLMs) in sixth-generation (6G) Internet of Things (IoT) networks and proposes a prompt-engineering-based real-time feedback and verification (PE-RTFV) framework that perform physical-layer's optimization tasks through an iteratively process. By leveraging the naturally available closed-loop feedback inherent in wireless communication systems, PE-RTFV enables real-time physical-layer optimization without requiring model retraining. The proposed framework employs an optimization LLM (O-LLM) to generate task-specific structured prompts, which are provided to an agent LLM (A-LLM) to produce task-specific solutions. Utilizing real-time system feedback, the O-LLM iteratively refines the prompts to guide the A-LLM toward improved solutions in a gradient-descent-like optimization process. We test PE-RTFV approach on wireless-powered IoT testbed case study on user-goal-driven constellation design through semantically solving rate-energy (RE)-region optimization problem which demonstrates that PE-RTFV achieves near-genetic-algorithm performance within only a few iterations, validating its effectiveness for complex physical-layer optimization tasks in resource-constrained IoT networks.",
    "link": "http://arxiv.org/abs/2602.06819v1",
    "date": "2026-02-06",
    "fetched_at": "2026-02-09"
  },
  {
    "id": "http://arxiv.org/abs/2602.06818v1",
    "title": "Wild Guesses and Mild Guesses in Active Concept Learning",
    "authors": [
      "Anirudh Chari",
      "Neil Pattanaik"
    ],
    "summary": "Human concept learning is typically active: learners choose which instances to query or test in order to reduce uncertainty about an underlying rule or category. Active concept learning must balance informativeness of queries against the stability of the learner that generates and scores hypotheses. We study this trade-off in a neuro-symbolic Bayesian learner whose hypotheses are executable programs proposed by a large language model (LLM) and reweighted by Bayesian updating. We compare a Rational Active Learner that selects queries to maximize approximate expected information gain (EIG) and the human-like Positive Test Strategy (PTS) that queries instances predicted to be positive under the current best hypothesis. Across concept-learning tasks in the classic Number Game, EIG is effective when falsification is necessary (e.g., compound or exception-laden rules), but underperforms on simple concepts. We trace this failure to a support mismatch between the EIG policy and the LLM proposal distribution: highly diagnostic boundary queries drive the posterior toward regions where the generator produces invalid or overly specific programs, yielding a support-mismatch trap in the particle approximation. PTS is information-suboptimal but tends to maintain proposal validity by selecting \"safe\" queries, leading to faster convergence on simple rules. Our results suggest that \"confirmation bias\" may not be a cognitive error, but rather a rational adaptation for maintaining tractable inference in the sparse, open-ended hypothesis spaces characteristic of human thought.",
    "link": "http://arxiv.org/abs/2602.06818v1",
    "date": "2026-02-06",
    "fetched_at": "2026-02-09"
  },
  {
    "id": "http://arxiv.org/abs/2602.06799v1",
    "title": "Visual Word Sense Disambiguation with CLIP through Dual-Channel Text Prompting and Image Augmentations",
    "authors": [
      "Shamik Bhattacharya",
      "Daniel Perkins",
      "Yaren Dogan",
      "Vineeth Konjeti",
      "Sudarshan Srinivasan",
      "Edmon Begoli"
    ],
    "summary": "Ambiguity poses persistent challenges in natural language understanding for large language models (LLMs). To better understand how lexical ambiguity can be resolved through the visual domain, we develop an interpretable Visual Word Sense Disambiguation (VWSD) framework. The model leverages CLIP to project ambiguous language and candidate images into a shared multimodal space. We enrich textual embeddings using a dual-channel ensemble of semantic and photo-based prompts with WordNet synonyms, while image embeddings are refined through robust test-time augmentations. We then use cosine similarity to determine the image that best aligns with the ambiguous text. When evaluated on the SemEval-2023 VWSD dataset, enriching the embeddings raises the MRR from 0.7227 to 0.7590 and the Hit Rate from 0.5810 to 0.6220. Ablation studies reveal that dual-channel prompting provides strong, low-latency performance, whereas aggressive image augmentation yields only marginal gains. Additional experiments with WordNet definitions and multilingual prompt ensembles further suggest that noisy external signals tend to dilute semantic specificity, reinforcing the effectiveness of precise, CLIP-aligned prompts for visual word sense disambiguation.",
    "link": "http://arxiv.org/abs/2602.06799v1",
    "date": "2026-02-06",
    "fetched_at": "2026-02-09"
  },
  {
    "id": "http://arxiv.org/abs/2602.06797v1",
    "title": "Optimal Learning-Rate Schedules under Functional Scaling Laws: Power Decay and Warmup-Stable-Decay",
    "authors": [
      "Binghui Li",
      "Zilin Wang",
      "Fengling Chen",
      "Shiyang Zhao",
      "Ruiheng Zheng",
      "Lei Wu"
    ],
    "summary": "We study optimal learning-rate schedules (LRSs) under the functional scaling law (FSL) framework introduced in Li et al. (2025), which accurately models the loss dynamics of both linear regression and large language model (LLM) pre-training. Within FSL, loss dynamics are governed by two exponents: a source exponent $s>0$ controlling the rate of signal learning, and a capacity exponent $\u03b2>1$ determining the rate of noise forgetting. Focusing on a fixed training horizon $N$, we derive the optimal LRSs and reveal a sharp phase transition. In the easy-task regime $s \\ge 1 - 1/\u03b2$, the optimal schedule follows a power decay to zero, $\u03b7^*(z) = \u03b7_{\\mathrm{peak}}(1 - z/N)^{2\u03b2- 1}$, where the peak learning rate scales as $\u03b7_{\\mathrm{peak}} \\eqsim N^{-\u03bd}$ for an explicit exponent $\u03bd= \u03bd(s,\u03b2)$. In contrast, in the hard-task regime $s < 1 - 1/\u03b2$, the optimal LRS exhibits a warmup-stable-decay (WSD) (Hu et al. (2024)) structure: it maintains the largest admissible learning rate for most of training and decays only near the end, with the decay phase occupying a vanishing fraction of the horizon.   We further analyze optimal shape-fixed schedules, where only the peak learning rate is tuned -- a strategy widely adopted in practiceand characterize their strengths and intrinsic limitations. This yields a principled evaluation of commonly used schedules such as cosine and linear decay. Finally, we apply the power-decay LRS to one-pass stochastic gradient descent (SGD) for kernel regression and show the last iterate attains the exact minimax-optimal rate, eliminating the logarithmic suboptimality present in prior analyses. Numerical experiments corroborate our theoretical predictions.",
    "link": "http://arxiv.org/abs/2602.06797v1",
    "date": "2026-02-06",
    "fetched_at": "2026-02-09"
  },
  {
    "id": "http://arxiv.org/abs/2602.06795v1",
    "title": "Generating Data-Driven Reasoning Rubrics for Domain-Adaptive Reward Modeling",
    "authors": [
      "Kate Sanders",
      "Nathaniel Weir",
      "Sapana Chaudhary",
      "Kaj Bostrom",
      "Huzefa Rangwala"
    ],
    "summary": "An impediment to using Large Language Models (LLMs) for reasoning output verification is that LLMs struggle to reliably identify errors in thinking traces, particularly in long outputs, domains requiring expert knowledge, and problems without verifiable rewards. We propose a data-driven approach to automatically construct highly granular reasoning error taxonomies to enhance LLM-driven error detection on unseen reasoning traces. Our findings indicate that classification approaches that leverage these error taxonomies, or \"rubrics\", demonstrate strong error identification compared to baseline methods in technical domains like coding, math, and chemical engineering. These rubrics can be used to build stronger LLM-as-judge reward functions for reasoning model training via reinforcement learning. Experimental results show that these rewards have the potential to improve models' task accuracy on difficult domains over models trained by general LLMs-as-judges by +45%, and approach performance of models trained by verifiable rewards while using as little as 20% as many gold labels. Through our approach, we extend the usage of reward rubrics from assessing qualitative model behavior to assessing quantitative model correctness on tasks typically learned via RLVR rewards. This extension opens the door for teaching models to solve complex technical problems without a full dataset of gold labels, which are often highly costly to procure.",
    "link": "http://arxiv.org/abs/2602.06795v1",
    "date": "2026-02-06",
    "fetched_at": "2026-02-09"
  },
  {
    "id": "http://arxiv.org/abs/2602.06791v1",
    "title": "Rare Event Analysis of Large Language Models",
    "authors": [
      "Jake McAllister Dorman",
      "Edward Gillman",
      "Dominic C. Rose",
      "Jamie F. Mair",
      "Juan P. Garrahan"
    ],
    "summary": "Being probabilistic models, during inference large language models (LLMs) display rare events: behaviour that is far from typical but highly significant. By definition all rare events are hard to see, but the enormous scale of LLM usage means that events completely unobserved during development are likely to become prominent in deployment. Here we present an end-to-end framework for the systematic analysis of rare events in LLMs. We provide a practical implementation spanning theory, efficient generation strategies, probability estimation and error analysis, which we illustrate with concrete examples. We outline extensions and applications to other models and contexts, highlighting the generality of the concepts and techniques presented here.",
    "link": "http://arxiv.org/abs/2602.06791v1",
    "date": "2026-02-06",
    "fetched_at": "2026-02-09"
  },
  {
    "id": "http://arxiv.org/abs/2601.23273v1",
    "title": "UPA: Unsupervised Prompt Agent via Tree-Based Search and Selection",
    "authors": [
      "Siran Peng",
      "Weisong Zhao",
      "Tianyu Fu",
      "Chenxu Zhao",
      "Tianshuo Zhang",
      "Haoyuan Zhang",
      "Xiangyu Zhu",
      "Minghui Wu",
      "Zhen Lei"
    ],
    "summary": "Prompt agents have recently emerged as a promising paradigm for automated prompt optimization, framing refinement as a sequential decision-making problem over a structured prompt space. While this formulation enables the use of advanced planning algorithms, these methods typically assume access to supervised reward signals, which are often unavailable in practical scenarios. In this work, we propose UPA, an Unsupervised Prompt Agent that realizes structured search and selection without relying on supervised feedback. Specifically, during search, UPA iteratively constructs an evolving tree structure to navigate the prompt space, guided by fine-grained and order-invariant pairwise comparisons from Large Language Models (LLMs). Crucially, as these local comparisons do not inherently yield a consistent global scale, we decouple systematic prompt exploration from final selection, introducing a two-stage framework grounded in the Bradley-Terry-Luce (BTL) model. This framework first performs path-wise Bayesian aggregation of local comparisons to filter candidates under uncertainty, followed by global tournament-style comparisons to infer latent prompt quality and identify the optimal prompt. Experiments across multiple tasks demonstrate that UPA consistently outperforms existing prompt optimization methods, showing that agent-style optimization remains highly effective even in fully unsupervised settings.",
    "link": "http://arxiv.org/abs/2601.23273v1",
    "date": "2026-01-30",
    "fetched_at": "2026-02-02"
  },
  {
    "id": "http://arxiv.org/abs/2601.23257v1",
    "title": "Outcome-Conditioned Reasoning Distillation for Resolving Software Issues",
    "authors": [
      "Chenglin Li",
      "Yisen Xu",
      "Zehao Wang",
      "Shin Hwei Tan",
      " Tse-Hsun",
      " Chen"
    ],
    "summary": "Software issue resolution in large repositories is a long-range decision process: choices made during localization shape the space of viable edits, and missteps can compound into incorrect patches. Despite this, many LLM-based repair pipelines still operate in a reset-and-solve manner, producing fresh reasoning for every new issue instead of carrying forward what worked in past fixes. This is wasteful because repositories routinely contain earlier issues with overlapping structure, failure modes, or constraints, where prior repair experience could provide useful guidance. Existing approaches typically harvest this signal through forward-time trial procedures, such as repeated refinement or search, incurring high inference cost while still risking divergence from the eventual correct patch. We present an Outcome-Conditioned Reasoning Distillation(O-CRD) framework that uses resolved in-repository issues with verified patches as supervision. Starting from a historical fix, the method reconstructs a stage-wise repair trace backward from the verified outcome, then reuses the distilled guidance at inference time to steer file/function localization and patch synthesis, without fine-tuning or online search. On SWE-Bench Lite, this approach increases Pass@1 by 10.4% with GPT-4o, 8.6% with DeepSeek-V3, and 10.3% with GPT-5, indicating that outcome-conditioned reuse of verified repairs can replace costly forward exploration for software issue resolution.",
    "link": "http://arxiv.org/abs/2601.23257v1",
    "date": "2026-01-30",
    "fetched_at": "2026-02-02"
  },
  {
    "id": "http://arxiv.org/abs/2601.23254v1",
    "title": "GrepRAG: An Empirical Study and Optimization of Grep-Like Retrieval for Code Completion",
    "authors": [
      "Baoyi Wang",
      "Xingliang Wang",
      "Guochang Li",
      "Chen Zhi",
      "Junxiao Han",
      "Xinkui Zhao",
      "Nan Wang",
      "Shuiguang Deng",
      "Jianwei Yin"
    ],
    "summary": "Repository-level code completion remains challenging for large language models (LLMs) due to cross-file dependencies and limited context windows. Prior work addresses this challenge using Retrieval-Augmented Generation (RAG) frameworks based on semantic indexing or structure-aware graph analysis, but these approaches incur substantial computational overhead for index construction and maintenance. Motivated by common developer workflows that rely on lightweight search utilities (e.g., ripgrep), we revisit a fundamental yet underexplored question: how far can simple, index-free lexical retrieval support repository-level code completion before more complex retrieval mechanisms become necessary? To answer this question, we systematically investigate lightweight, index-free, intent-aware lexical retrieval through extensive empirical analysis. We first introduce Naive GrepRAG, a baseline framework in which LLMs autonomously generate ripgrep commands to retrieve relevant context. Despite its simplicity, Naive GrepRAG achieves performance comparable to sophisticated graph-based baselines. Further analysis shows that its effectiveness stems from retrieving lexically precise code fragments that are spatially closer to the completion site. We also identify key limitations of lexical retrieval, including sensitivity to noisy matches from high-frequency ambiguous keywords and context fragmentation caused by rigid truncation boundaries. To address these issues, we propose GrepRAG, which augments lexical retrieval with a lightweight post-processing pipeline featuring identifier-weighted re-ranking and structure-aware deduplication. Extensive evaluation on CrossCodeEval and RepoEval-Updated demonstrates that GrepRAG consistently outperforms state-of-the-art (SOTA) methods, achieving 7.04-15.58 percent relative improvement in code exact match (EM) over the best baseline on CrossCodeEval.",
    "link": "http://arxiv.org/abs/2601.23254v1",
    "date": "2026-01-30",
    "fetched_at": "2026-02-02"
  },
  {
    "id": "http://arxiv.org/abs/2601.23232v1",
    "title": "ShotFinder: Imagination-Driven Open-Domain Video Shot Retrieval via Web Search",
    "authors": [
      "Tao Yu",
      "Haopeng Jin",
      "Hao Wang",
      "Shenghua Chai",
      "Yujia Yang",
      "Junhao Gong",
      "Jiaming Guo",
      "Minghui Zhang",
      "Xinlong Chen",
      "Zhenghao Zhang",
      "Yuxuan Zhou",
      "Yanpei Gong",
      "YuanCheng Liu",
      "Yiming Ding",
      "Kangwei Zeng",
      "Pengfei Yang",
      "Zhongtian Luo",
      "Yufei Xiong",
      "Shanbin Zhang",
      "Shaoxiong Cheng",
      "Huang Ruilin",
      "Li Shuo",
      "Yuxi Niu",
      "Xinyuan Zhang",
      "Yueya Xu",
      "Jie Mao",
      "Ruixuan Ji",
      "Yaru Zhao",
      "Mingchen Zhang",
      "Jiabing Yang",
      "Jiaqi Liu",
      "YiFan Zhang",
      "Hongzhu Yi",
      "Xinming Wang",
      "Cheng Zhong",
      "Xiao Ma",
      "Zhang Zhang",
      "Yan Huang",
      "Liang Wang"
    ],
    "summary": "In recent years, large language models (LLMs) have made rapid progress in information retrieval, yet existing research has mainly focused on text or static multimodal settings. Open-domain video shot retrieval, which involves richer temporal structure and more complex semantics, still lacks systematic benchmarks and analysis. To fill this gap, we introduce ShotFinder, a benchmark that formalizes editing requirements as keyframe-oriented shot descriptions and introduces five types of controllable single-factor constraints: Temporal order, Color, Visual style, Audio, and Resolution. We curate 1,210 high-quality samples from YouTube across 20 thematic categories, using large models for generation with human verification. Based on the benchmark, we propose ShotFinder, a text-driven three-stage retrieval and localization pipeline: (1) query expansion via video imagination, (2) candidate video retrieval with a search engine, and (3) description-guided temporal localization. Experiments on multiple closed-source and open-source models reveal a significant gap to human performance, with clear imbalance across constraints: temporal localization is relatively tractable, while color and visual style remain major challenges. These results reveal that open-domain video shot retrieval is still a critical capability that multimodal large models have yet to overcome.",
    "link": "http://arxiv.org/abs/2601.23232v1",
    "date": "2026-01-30",
    "fetched_at": "2026-02-02"
  },
  {
    "id": "http://arxiv.org/abs/2601.23219v1",
    "title": "MonoScale: Scaling Multi-Agent System with Monotonic Improvement",
    "authors": [
      "Shuai Shao",
      "Yixiang Liu",
      "Bingwei Lu",
      "Weinan Zhang"
    ],
    "summary": "In recent years, LLM-based multi-agent systems (MAS) have advanced rapidly, using a router to decompose tasks and delegate subtasks to specialized agents. A natural way to expand capability is to scale up the agent pool by continually integrating new functional agents or tool interfaces, but naive expansion can trigger performance collapse when the router cold-starts on newly added, heterogeneous, and unreliable agents. We propose MonoScale, an expansion-aware update framework that proactively generates a small set of agent-conditioned familiarization tasks, harvests evidence from both successful and failed interactions, and distills it into auditable natural-language memory to guide future routing. We formalize sequential augmentation as a contextual bandit and perform trust-region memory updates, yielding a monotonic non-decreasing performance guarantee across onboarding rounds. Experiments on GAIA and Humanity's Last Exam show stable gains as the agent pool grows, outperforming naive scale-up and strong-router fixed-pool baselines.",
    "link": "http://arxiv.org/abs/2601.23219v1",
    "date": "2026-01-30",
    "fetched_at": "2026-02-02"
  },
  {
    "id": "http://arxiv.org/abs/2601.23211v1",
    "title": "Multi-Agent Systems Should be Treated as Principal-Agent Problems",
    "authors": [
      "Paulius Rauba",
      "Simonas Cepenas",
      "Mihaela van der Schaar"
    ],
    "summary": "Consider a multi-agent systems setup in which a principal (a supervisor agent) assigns subtasks to specialized agents and aggregates their responses into a single system-level output. A core property of such systems is information asymmetry: agents observe task-specific information, produce intermediate reasoning traces, and operate with different context windows. In isolation, such asymmetry is not problematic, since agents report truthfully to the principal when incentives are fully aligned. However, this assumption breaks down when incentives diverge. Recent evidence suggests that LLM-based agents can acquire their own goals, such as survival or self-preservation, a phenomenon known as scheming, and may deceive humans or other agents. This leads to agency loss: a gap between the principal's intended outcome and the realized system behavior. Drawing on core ideas from microeconomic theory, we argue that these characteristics, information asymmetry and misaligned goals, are best studied through the lens of principal-agent problems. We explain why multi-agent systems, both human-to-LLM and LLM-to-LLM, naturally induce information asymmetry under this formulation, and we use scheming, where LLM agents pursue covert goals, as a concrete case study. We show that recently introduced terminology used to describe scheming, such as covert subversion or deferred subversion, corresponds to well-studied concepts in the mechanism design literature, which not only characterizes the problem but also prescribes concrete mitigation strategies. More broadly, we argue for applying tools developed to study human agent behavior to the analysis of non-human agents.",
    "link": "http://arxiv.org/abs/2601.23211v1",
    "date": "2026-01-30",
    "fetched_at": "2026-02-02"
  },
  {
    "id": "http://arxiv.org/abs/2601.23206v1",
    "title": "High-quality generation of dynamic game content via small language models: A proof of concept",
    "authors": [
      "Morten I. K. Munk",
      "Arturo Valdivia",
      "Paolo Burelli"
    ],
    "summary": "Large language models (LLMs) offer promise for dynamic game content generation, but they face critical barriers, including narrative incoherence and high operational costs. Due to their large size, they are often accessed in the cloud, limiting their application in offline games. Many of these practical issues are solved by pivoting to small language models (SLMs), but existing studies using SLMs have resulted in poor output quality. We propose a strategy of achieving high-quality SLM generation through aggressive fine-tuning on deliberately scoped tasks with narrow context, constrained structure, or both. In short, more difficult tasks require narrower scope and higher specialization to the training corpus. Training data is synthetically generated via a DAG-based approach, grounding models in the specific game world. Such models can form the basis for agentic networks designed around the narratological framework at hand, representing a more practical and robust solution than cloud-dependent LLMs. To validate this approach, we present a proof-of-concept focusing on a single specialized SLM as the fundamental building block. We introduce a minimal RPG loop revolving around rhetorical battles of reputations, powered by this model. We demonstrate that a simple retry-until-success strategy reaches adequate quality (as defined by an LLM-as-a-judge scheme) with predictable latency suitable for real-time generation. While local quality assessment remains an open question, our results demonstrate feasibility for real-time generation under typical game engine constraints.",
    "link": "http://arxiv.org/abs/2601.23206v1",
    "date": "2026-01-30",
    "fetched_at": "2026-02-02"
  },
  {
    "id": "http://arxiv.org/abs/2601.23204v1",
    "title": "TSAQA: Time Series Analysis Question And Answering Benchmark",
    "authors": [
      "Baoyu Jing",
      "Sanhorn Chen",
      "Lecheng Zheng",
      "Boyu Liu",
      "Zihao Li",
      "Jiaru Zou",
      "Tianxin Wei",
      "Zhining Liu",
      "Zhichen Zeng",
      "Ruizhong Qiu",
      "Xiao Lin",
      "Yuchen Yan",
      "Dongqi Fu",
      "Jingchao Ni",
      "Jingrui He",
      "Hanghang Tong"
    ],
    "summary": "Time series data are integral to critical applications across domains such as finance, healthcare, transportation, and environmental science. While recent work has begun to explore multi-task time series question answering (QA), current benchmarks remain limited to forecasting and anomaly detection tasks. We introduce TSAQA, a novel unified benchmark designed to broaden task coverage and evaluate diverse temporal analysis capabilities. TSAQA integrates six diverse tasks under a single framework ranging from conventional analysis, including anomaly detection and classification, to advanced analysis, such as characterization, comparison, data transformation, and temporal relationship analysis. Spanning 210k samples across 13 domains, the dataset employs diverse formats, including true-or-false (TF), multiple-choice (MC), and a novel puzzling (PZ), to comprehensively assess time series analysis. Zero-shot evaluation demonstrates that these tasks are challenging for current Large Language Models (LLMs): the best-performing commercial LLM, Gemini-2.5-Flash, achieves an average score of only 65.08. Although instruction tuning boosts open-source performance: the best-performing open-source model, LLaMA-3.1-8B, shows significant room for improvement, highlighting the complexity of temporal analysis for LLMs.",
    "link": "http://arxiv.org/abs/2601.23204v1",
    "date": "2026-01-30",
    "fetched_at": "2026-02-02"
  },
  {
    "id": "http://arxiv.org/abs/2601.23200v1",
    "title": "Large Language Models for Patent Classification: Strengths, Trade-offs, and the Long Tail Effect",
    "authors": [
      "Lorenzo Emer",
      "Marco Lippi",
      "Andrea Mina",
      "Andrea Vandin"
    ],
    "summary": "Patent classification into CPC codes underpins large scale analyses of technological change but remains challenging due to its hierarchical, multi label, and highly imbalanced structure. While pre Generative AI supervised encoder based models became the de facto standard for large scale patent classification, recent advances in large language models (LLMs) raise questions about whether they can provide complementary capabilities, particularly for rare or weakly represented technological categories. In this work, we perform a systematic comparison of encoder based classifiers (BERT, SciBERT, and PatentSBERTa) and open weight LLMs on a highly imbalanced benchmark dataset (USPTO 70k). We evaluate LLMs under zero shot, few shot, and retrieval augmented prompting, and further assess parameter efficient fine tuning of the best performing model. Our results show that encoder based models achieve higher aggregate performance, driven by strong results on frequent CPC subclasses, but struggle on rare ones. In contrast, LLMs achieve relatively higher performance on infrequent subclasses, often associated with early stage, cross domain, or weakly institutionalised technologies, particularly at higher hierarchical levels. These findings indicate that encoder based and LLM based approaches play complementary roles in patent classification. We additionally quantify inference time and energy consumption, showing that encoder based models are up to three orders of magnitude more efficient than LLMs. Overall, our results inform responsible patentometrics and technology mapping, and motivate hybrid classification approaches that combine encoder efficiency with the long tail coverage of LLMs under computational and environmental constraints.",
    "link": "http://arxiv.org/abs/2601.23200v1",
    "date": "2026-01-30",
    "fetched_at": "2026-02-02"
  },
  {
    "id": "http://arxiv.org/abs/2601.23184v1",
    "title": "ReGuLaR: Variational Latent Reasoning Guided by Rendered Chain-of-Thought",
    "authors": [
      "Fanmeng Wang",
      "Haotian Liu",
      "Guojiang Zhao",
      "Hongteng Xu",
      "Zhifeng Gao"
    ],
    "summary": "While Chain-of-Thought (CoT) significantly enhances the performance of Large Language Models (LLMs), explicit reasoning chains introduce substantial computational redundancy. Recent latent reasoning methods attempt to mitigate this by compressing reasoning processes into latent space, but often suffer from severe performance degradation due to the lack of appropriate compression guidance. In this study, we propose Rendered CoT-Guided variational Latent Reasoning (ReGuLaR), a simple yet novel latent learning paradigm resolving this issue. Fundamentally, we formulate latent reasoning within the Variational Auto-Encoding (VAE) framework, sampling the current latent reasoning state from the posterior distribution conditioned on previous ones. Specifically, when learning this variational latent reasoning model, we render explicit reasoning chains as images, from which we extract dense visual-semantic representations to regularize the posterior distribution, thereby achieving efficient compression with minimal information loss. Extensive experiments demonstrate that ReGuLaR significantly outperforms existing latent reasoning methods across both computational efficiency and reasoning effectiveness, and even surpasses CoT through multi-modal reasoning, providing a new and insightful solution to latent reasoning. Code: https://github.com/FanmengWang/ReGuLaR.",
    "link": "http://arxiv.org/abs/2601.23184v1",
    "date": "2026-01-30",
    "fetched_at": "2026-02-02"
  },
  {
    "id": "http://arxiv.org/abs/2601.23183v1",
    "title": "JobResQA: A Benchmark for LLM Machine Reading Comprehension on Multilingual R\u00e9sum\u00e9s and JDs",
    "authors": [
      "Casimiro Pio Carrino",
      "Paula Estrella",
      "Rabih Zbib",
      "Carlos Escolano",
      "Jos\u00e9 A. R. Fonollosa"
    ],
    "summary": "We introduce JobResQA, a multilingual Question Answering benchmark for evaluating Machine Reading Comprehension (MRC) capabilities of LLMs on HR-specific tasks involving r\u00e9sum\u00e9s and job descriptions. The dataset comprises 581 QA pairs across 105 synthetic r\u00e9sum\u00e9-job description pairs in five languages (English, Spanish, Italian, German, and Chinese), with questions spanning three complexity levels from basic factual extraction to complex cross-document reasoning. We propose a data generation pipeline derived from real-world sources through de-identification and data synthesis to ensure both realism and privacy, while controlled demographic and professional attributes (implemented via placeholders) enable systematic bias and fairness studies. We also present a cost-effective, human-in-the-loop translation pipeline based on the TEaR methodology, incorporating MQM error annotations and selective post-editing to ensure an high-quality multi-way parallel benchmark. We provide a baseline evaluations across multiple open-weight LLM families using an LLM-as-judge approach revealing higher performances on English and Spanish but substantial degradation for other languages, highlighting critical gaps in multilingual MRC capabilities for HR applications. JobResQA provides a reproducible benchmark for advancing fair and reliable LLM-based HR systems. The benchmark is publicly available at: https://github.com/Avature/jobresqa-benchmark",
    "link": "http://arxiv.org/abs/2601.23183v1",
    "date": "2026-01-30",
    "fetched_at": "2026-02-02"
  },
  {
    "id": "http://arxiv.org/abs/2601.23180v1",
    "title": "TriSpec: Ternary Speculative Decoding via Lightweight Proxy Verification",
    "authors": [
      "Haoyun Jiang",
      "Junqi He",
      "Feng Hong",
      "Xinlong Yang",
      "Jianwei Zhang",
      "Zheng Li",
      "Zhengyang Zhuge",
      "Zhiyong Chen",
      "Bo Han",
      "Junyang Lin",
      "Jiangchao Yao"
    ],
    "summary": "Inference efficiency in Large Language Models (LLMs) is fundamentally limited by their serial, autoregressive generation, especially as reasoning becomes a key capability and response sequences grow longer. Speculative decoding (SD) offers a powerful solution, providing significant speed-ups through its lightweight drafting and parallel verification mechanism. While existing work has nearly saturated improvements in draft effectiveness and efficiency, this paper advances SD from a new yet critical perspective: the verification cost. We propose TriSpec, a novel ternary SD framework that, at its core, introduces a lightweight proxy to significantly reduce computational cost by approving easily verifiable draft sequences and engaging the full target model only when encountering uncertain tokens. TriSpec can be integrated with state-of-the-art SD methods like EAGLE-3 to further reduce verification costs, achieving greater acceleration. Extensive experiments on the Qwen3 and DeepSeek-R1-Distill-Qwen/LLaMA families show that TriSpec achieves up to 35\\% speedup over standard SD, with up to 50\\% fewer target model invocations while maintaining comparable accuracy.",
    "link": "http://arxiv.org/abs/2601.23180v1",
    "date": "2026-01-30",
    "fetched_at": "2026-02-02"
  },
  {
    "id": "http://arxiv.org/abs/2601.23174v1",
    "title": "Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization",
    "authors": [
      "Luca Della Libera",
      "Cem Subakan",
      "Mirco Ravanelli"
    ],
    "summary": "Neural audio codecs are at the core of modern conversational speech technologies, converting continuous speech into sequences of discrete tokens that can be processed by LLMs. However, existing codecs typically operate at fixed frame rates, allocating tokens uniformly in time and producing unnecessarily long sequences. In this work, we introduce DyCAST, a Dynamic Character-Aligned Speech Tokenizer that enables variable-frame-rate tokenization through soft character-level alignment and explicit duration modeling. DyCAST learns to associate tokens with character-level linguistic units during training and supports alignment-free inference with direct control over token durations at decoding time. To improve speech resynthesis quality at low frame rates, we further introduce a retrieval-augmented decoding mechanism that enhances reconstruction fidelity without increasing bitrate. Experiments show that DyCAST achieves competitive speech resynthesis quality and downstream performance while using significantly fewer tokens than fixed-frame-rate codecs.",
    "link": "http://arxiv.org/abs/2601.23174v1",
    "date": "2026-01-30",
    "fetched_at": "2026-02-02"
  },
  {
    "id": "http://arxiv.org/abs/2601.23166v1",
    "title": "Monotonic Reference-Free Refinement for Autoformalization",
    "authors": [
      "Lan Zhang",
      "Marco Valentino",
      "Andr\u00e9 Freitas"
    ],
    "summary": "While statement autoformalization has advanced rapidly, full-theorem autoformalization remains largely unexplored. Existing iterative refinement methods in statement autoformalization typicall improve isolated aspects of formalization, such as syntactic correctness, but struggle to jointly optimizing multiple quality dimensions, which is critical for full-theorem autoformalization. We introduce a reference-free iterative monotonic process for full-theorem autoformalization that leverages complementary feedback from theorem provers and LLM-based judges, without access to ground-truth proofs or existing formalizations at inference time. Our approach optimizes a masked composite objective over Formal Validity, Logical Preservation, Mathematical Consistency, and Formal Quality, guided by a responsiveness map that indicates how different LLMs acting as different roles preferentially improve each dimension. We further propose an acceptance policy that guarantees certified monotonic improvement, and provide conditions ensuring convergence and termination. Empirical experiments demonstrate the proposed process enables simultaneous improvement across multiple dimensions, achieving 93.44% formal validity and a 78.22% overall score on miniF2F, and 44.09% formal validity and a 29.79% overall score on ProofNet.",
    "link": "http://arxiv.org/abs/2601.23166v1",
    "date": "2026-01-30",
    "fetched_at": "2026-02-02"
  },
  {
    "id": "http://arxiv.org/abs/2601.23163v1",
    "title": "Probing the Trajectories of Reasoning Traces in Large Language Models",
    "authors": [
      "Marthe Ballon",
      "Brecht Verbeken",
      "Vincent Ginis",
      "Andres Algaba"
    ],
    "summary": "Large language models (LLMs) increasingly solve difficult problems by producing \"reasoning traces\" before emitting a final response. However, it remains unclear how accuracy and decision commitment evolve along a reasoning trajectory, and whether intermediate trace segments provide answer-relevant information beyond generic length or stylistic effects. Here, we propose a protocol to systematically probe the trajectories of reasoning traces in LLMs by 1) generating a model's reasoning trace, 2) truncating it at fixed token-percentiles, and 3) injecting each partial trace back into the model (or a different model) to measure the induced distribution over answer choices via next-token probabilities. We apply this protocol to the open-source Qwen3-4B/-8B/-14B and gpt-oss-20b/-120b models across the multiple-choice GPQA Diamond and MMLU-Pro benchmarks. We find that accuracy and decision commitment consistently increase as the percentage of provided reasoning tokens grows. These gains are primarily driven by relevant content in the model generation rather than context length or generic \"reasoning style\" effects. Stronger models often backtrack successfully from incorrect partial traces, but immediate answers often remain anchored in the weaker model's incorrect response. More broadly, we show that trajectory probing provides diagnostics for efficient and safer deployment of reasoning models as the measurements can inform practical trace-handling and monitoring policies that improve reliability without assuming intermediate tokens are inherently faithful explanations.",
    "link": "http://arxiv.org/abs/2601.23163v1",
    "date": "2026-01-30",
    "fetched_at": "2026-02-02"
  },
  {
    "id": "http://arxiv.org/abs/2601.23153v1",
    "title": "Behemoth: Benchmarking Unlearning in LLMs Using Fully Synthetic Data",
    "authors": [
      "Eugenia Iofinova",
      "Dan Alistarh"
    ],
    "summary": "As artificial neural networks, and specifically large language models, have improved rapidly in capabilities and quality, they have increasingly been deployed in real-world applications, from customer service to Google search, despite the fact that they frequently make factually incorrect or undesirable statements. This trend has inspired practical and academic interest in model editing, that is, in adjusting the weights of the model to modify its likely outputs for queries relating to a specific fact or set of facts. This may be done either to amend a fact or set of facts, for instance, to fix a frequent error in the training data, or to suppress a fact or set of facts entirely, for instance, in case of dangerous knowledge. Multiple methods have been proposed to do such edits. However, at the same time, it has been shown that such model editing can be brittle and incomplete. Moreover the effectiveness of any model editing method necessarily depends on the data on which the model is trained, and, therefore, a good understanding of the interaction of the training data distribution and the way it is stored in the network is necessary and helpful to reliably perform model editing. However, working with large language models trained on real-world data does not allow us to understand this relationship or fully measure the effects of model editing. We therefore propose Behemoth, a fully synthetic data generation framework. To demonstrate the practical insights from the framework, we explore model editing in the context of simple tabular data, demonstrating surprising findings that, in some cases, echo real-world results, for instance, that in some cases restricting the update rank results in a more effective update. The code is available at https://github.com/IST-DASLab/behemoth.git.",
    "link": "http://arxiv.org/abs/2601.23153v1",
    "date": "2026-01-30",
    "fetched_at": "2026-02-02"
  },
  {
    "id": "http://arxiv.org/abs/2601.23133v1",
    "title": "RAudit: A Blind Auditing Protocol for Large Language Model Reasoning",
    "authors": [
      "Edward Y. Chang",
      "Longling Geng"
    ],
    "summary": "Inference-time scaling can amplify reasoning pathologies: sycophancy, rung collapse, and premature certainty. We present RAudit, a diagnostic protocol for auditing LLM reasoning without ground truth access. The key constraint is blindness: the auditor evaluates only whether derivation steps support conclusions, enabling detection of trace-output inconsistency and, when latent competence exists, its recovery. RAudit measures process quality via CRIT-based reasonableness scores and varies critique formulation to study how social framing affects model response. We prove bounded correction and $O(\\log(1/\u03b5))$ termination. Experiments on mathematical reasoning (CAP-GSM8K) and causal judgment (CausalL2) reveal four mechanisms explaining model unreliability: (1) Latent Competence Suppression, where models derive correct answers then overwrite them under social pressure; (2) The False Competence Trap, where weaker judges mask sycophancy that stronger judges expose; (3) The Complexity-Vulnerability Tradeoff, where causal tasks induce more than 10 times higher sycophancy than mathematical tasks; and (4) Iatrogenic Critique, where authoritative correction harms weaker models. These findings challenge assumptions that capability implies robustness and that stronger feedback yields better outputs.",
    "link": "http://arxiv.org/abs/2601.23133v1",
    "date": "2026-01-30",
    "fetched_at": "2026-02-02"
  },
  {
    "id": "http://arxiv.org/abs/2601.23132v1",
    "title": "Secure Tool Manifest and Digital Signing Solution for Verifiable MCP and LLM Pipelines",
    "authors": [
      "Saeid Jamshidi",
      "Kawser Wazed Nafi",
      "Arghavan Moradi Dakhel",
      "Foutse Khomh",
      "Amin Nikanjam",
      "Mohammad Adnan Hamdaqa"
    ],
    "summary": "Large Language Models (LLMs) are increasingly adopted in sensitive domains such as healthcare and financial institutions' data analytics; however, their execution pipelines remain vulnerable to manipulation and unverifiable behavior. Existing control mechanisms, such as the Model Context Protocol (MCP), define compliance policies for tool invocation but lack verifiable enforcement and transparent validation of model actions. To address this gap, we propose a novel Secure Tool Manifest and Digital Signing Framework, a structured and security-aware extension of Model Context Protocols. The framework enforces cryptographically signed manifests, integrates transparent verification logs, and isolates model-internal execution metadata from user-visible components to ensure verifiable execution integrity. Furthermore, the evaluation demonstrates that the framework scales nearly linearly (R-squared = 0.998), achieves near-perfect acceptance of valid executions while consistently rejecting invalid ones, and maintains balanced model utilization across execution pipelines.",
    "link": "http://arxiv.org/abs/2601.23132v1",
    "date": "2026-01-30",
    "fetched_at": "2026-02-02"
  },
  {
    "id": "http://arxiv.org/abs/2601.23129v1",
    "title": "Evaluating the Utility of Grounding Documents with Reference-Free LLM-based Metrics",
    "authors": [
      "Yilun Hua",
      "Giuseppe Castellucci",
      "Peter Schulam",
      "Heba Elfardy",
      "Kevin Small"
    ],
    "summary": "Retrieval Augmented Generation (RAG)'s success depends on the utility the LLM derives from the content used for grounding. Quantifying content utility does not have a definitive specification and existing metrics ignore model-specific capabilities and/or rely on costly annotations. In this paper, we propose Grounding Generation Utility (GroGU), a model-specific and reference-free metric that defines utility as a function of the downstream LLM's generation confidence based on entropy. Despite having no annotation requirements, GroGU is largely faithful in distinguishing ground-truth documents while capturing nuances ignored by LLM-agnostic metrics. We apply GroGU to train a query-rewriter for RAG by identifying high-utility preference data for Direct Preference Optimization. Experiments show improvements by up to 18.2 points in Mean Reciprocal Rank and up to 9.4 points in answer accuracy.",
    "link": "http://arxiv.org/abs/2601.23129v1",
    "date": "2026-01-30",
    "fetched_at": "2026-02-02"
  },
  {
    "id": "http://arxiv.org/abs/2601.23121v1",
    "title": "An Automatic Deep Learning Approach for Trailer Generation through Large Language Models",
    "authors": [
      "Roberto Balestri",
      "Pasquale Cascarano",
      "Mirko Degli Esposti",
      "Guglielmo Pescatore"
    ],
    "summary": "Trailers are short promotional videos designed to provide audiences with a glimpse of a movie. The process of creating a trailer typically involves selecting key scenes, dialogues and action sequences from the main content and editing them together in a way that effectively conveys the tone, theme and overall appeal of the movie. This often includes adding music, sound effects, visual effects and text overlays to enhance the impact of the trailer. In this paper, we present a framework exploiting a comprehensive multimodal strategy for automated trailer production. Also, a Large Language Model (LLM) is adopted across various stages of the trailer creation. First, it selects main key visual sequences that are relevant to the movie's core narrative. Then, it extracts the most appealing quotes from the movie, aligning them with the trailer's narrative. Additionally, the LLM assists in creating music backgrounds and voiceovers to enrich the audience's engagement, thus contributing to make a trailer not just a summary of the movie's content but a narrative experience in itself. Results show that our framework generates trailers that are more visually appealing to viewers compared to those produced by previous state-of-the-art competitors.",
    "link": "http://arxiv.org/abs/2601.23121v1",
    "date": "2026-01-30",
    "fetched_at": "2026-02-02"
  },
  {
    "id": "http://arxiv.org/abs/2601.16979v1",
    "title": "A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs",
    "authors": [
      "Dayal Singh Kalra",
      "Jean-Christophe Gagnon-Audet",
      "Andrey Gromov",
      "Ishita Mediratta",
      "Kelvin Niu",
      "Alexander H Miller",
      "Michael Shvartsman"
    ],
    "summary": "Understanding the curvature evolution of the loss landscape is fundamental to analyzing the training dynamics of neural networks. The most commonly studied measure, Hessian sharpness ($\u03bb_{\\max}^H$) -- the largest eigenvalue of the loss Hessian -- determines local training stability and interacts with the learning rate throughout training. Despite its significance in analyzing training dynamics, direct measurement of Hessian sharpness remains prohibitive for Large Language Models (LLMs) due to high computational cost. We analyze $\\textit{critical sharpness}$ ($\u03bb_c$), a computationally efficient measure requiring fewer than $10$ forward passes given the update direction $\u0394\\mathbf\u03b8$. Critically, this measure captures well-documented Hessian sharpness phenomena, including progressive sharpening and Edge of Stability. Using this measure, we provide the first demonstration of these sharpness phenomena at scale, up to $7$B parameters, spanning both pre-training and mid-training of OLMo-2 models. We further introduce $\\textit{relative critical sharpness}$ ($\u03bb_c^{1\\to 2}$), which quantifies the curvature of one loss landscape while optimizing another, to analyze the transition from pre-training to fine-tuning and guide data mixing strategies. Critical sharpness provides practitioners with a practical tool for diagnosing curvature dynamics and informing data composition choices at scale. More broadly, our work shows that scalable curvature measures can provide actionable insights for large-scale training.",
    "link": "http://arxiv.org/abs/2601.16979v1",
    "date": "2026-01-23",
    "fetched_at": "2026-01-26"
  },
  {
    "id": "http://arxiv.org/abs/2601.16967v1",
    "title": "Empowering Medical Equipment Sustainability in Low-Resource Settings: An AI-Powered Diagnostic and Support Platform for Biomedical Technicians",
    "authors": [
      "Bernes Lorier Atabonfack",
      "Ahmed Tahiru Issah",
      "Mohammed Hardi Abdul Baaki",
      "Clemence Ingabire",
      "Tolulope Olusuyi",
      "Maruf Adewole",
      "Udunna C. Anazodo",
      "Timothy X Brown"
    ],
    "summary": "In low- and middle-income countries (LMICs), a significant proportion of medical diagnostic equipment remains underutilized or non-functional due to a lack of timely maintenance, limited access to technical expertise, and minimal support from manufacturers, particularly for devices acquired through third-party vendors or donations. This challenge contributes to increased equipment downtime, delayed diagnoses, and compromised patient care. This research explores the development and validation of an AI-powered support platform designed to assist biomedical technicians in diagnosing and repairing medical devices in real-time. The system integrates a large language model (LLM) with a user-friendly web interface, enabling imaging technologists/radiographers and biomedical technicians to input error codes or device symptoms and receive accurate, step-by-step troubleshooting guidance. The platform also includes a global peer-to-peer discussion forum to support knowledge exchange and provide additional context for rare or undocumented issues. A proof of concept was developed using the Philips HDI 5000 ultrasound machine, achieving 100% precision in error code interpretation and 80% accuracy in suggesting corrective actions. This study demonstrates the feasibility and potential of AI-driven systems to support medical device maintenance, with the aim of reducing equipment downtime to improve healthcare delivery in resource-constrained environments.",
    "link": "http://arxiv.org/abs/2601.16967v1",
    "date": "2026-01-23",
    "fetched_at": "2026-01-26"
  },
  {
    "id": "http://arxiv.org/abs/2601.16965v1",
    "title": "Spatial-Agent: Agentic Geo-spatial Reasoning with Scientific Core Concepts",
    "authors": [
      "Riyang Bao",
      "Cheng Yang",
      "Dazhou Yu",
      "Zhexiang Tang",
      "Gengchen Mai",
      "Liang Zhao"
    ],
    "summary": "Geospatial reasoning is essential for real-world applications such as urban analytics, transportation planning, and disaster response. However, existing LLM-based agents often fail at genuine geospatial computation, relying instead on web search or pattern matching while hallucinating spatial relationships. We present Spatial-Agent, an AI agent grounded in foundational theories of spatial information science. Our approach formalizes geo-analytical question answering as a concept transformation problem, where natural-language questions are parsed into executable workflows represented as GeoFlow Graphs -- directed acyclic graphs with nodes corresponding to spatial concepts and edges representing transformations. Drawing on spatial information theory, Spatial-Agent extracts spatial concepts, assigns functional roles with principled ordering constraints, and composes transformation sequences through template-based generation. Extensive experiments on MapEval-API and MapQA benchmarks demonstrate that Spatial-Agent significantly outperforms existing baselines including ReAct and Reflexion, while producing interpretable and executable geospatial workflows.",
    "link": "http://arxiv.org/abs/2601.16965v1",
    "date": "2026-01-23",
    "fetched_at": "2026-01-26"
  },
  {
    "id": "http://arxiv.org/abs/2601.16964v1",
    "title": "AgentDrive: An Open Benchmark Dataset for Agentic AI Reasoning with LLM-Generated Scenarios in Autonomous Systems",
    "authors": [
      "Mohamed Amine Ferrag",
      "Abderrahmane Lakas",
      "Merouane Debbah"
    ],
    "summary": "The rapid advancement of large language models (LLMs) has sparked growing interest in their integration into autonomous systems for reasoning-driven perception, planning, and decision-making. However, evaluating and training such agentic AI models remains challenging due to the lack of large-scale, structured, and safety-critical benchmarks. This paper introduces AgentDrive, an open benchmark dataset containing 300,000 LLM-generated driving scenarios designed for training, fine-tuning, and evaluating autonomous agents under diverse conditions. AgentDrive formalizes a factorized scenario space across seven orthogonal axes: scenario type, driver behavior, environment, road layout, objective, difficulty, and traffic density. An LLM-driven prompt-to-JSON pipeline generates semantically rich, simulation-ready specifications that are validated against physical and schema constraints. Each scenario undergoes simulation rollouts, surrogate safety metric computation, and rule-based outcome labeling. To complement simulation-based evaluation, we introduce AgentDrive-MCQ, a 100,000-question multiple-choice benchmark spanning five reasoning dimensions: physics, policy, hybrid, scenario, and comparative reasoning. We conduct a large-scale evaluation of fifty leading LLMs on AgentDrive-MCQ. Results show that while proprietary frontier models perform best in contextual and policy reasoning, advanced open models are rapidly closing the gap in structured and physics-grounded reasoning. We release the AgentDrive dataset, AgentDrive-MCQ benchmark, evaluation code, and related materials at https://github.com/maferrag/AgentDrive",
    "link": "http://arxiv.org/abs/2601.16964v1",
    "date": "2026-01-23",
    "fetched_at": "2026-01-26"
  },
  {
    "id": "http://arxiv.org/abs/2601.16956v1",
    "title": "DataStates-LLM: Scalable Checkpointing for Transformer Models Using Composable State Providers",
    "authors": [
      "Avinash Maurya",
      "M. Mustafa Rafique",
      "Franck Cappello",
      "Bogdan Nicolae"
    ],
    "summary": "The rapid growth of Large Transformer-based models, specifically Large Language Models (LLMs), now scaling to trillions of parameters, has necessitated training across thousands of GPUs using complex hybrid parallelism strategies (e.g., data, tensor, and pipeline parallelism). Checkpointing this massive, distributed state is critical for a wide range of use cases, such as resilience, suspend-resume, investigating undesirable training trajectories, and explaining model evolution. However, existing checkpointing solutions typically treat model state as opaque binary blobs, ignoring the ``3D heterogeneity'' of the underlying data structures--varying by memory location (GPU vs. Host), number of ``logical'' objects sharded and split across multiple files, data types (tensors vs. Python objects), and their serialization requirements. This results in significant runtime overheads due to blocking device-to-host transfers, data-oblivious serialization, and storage I/O contention. In this paper, we introduce DataStates-LLM, a novel checkpointing architecture that leverages State Providers to decouple state abstraction from data movement. DataStates-LLM exploits the immutability of model parameters during the forward and backward passes to perform ``lazy'', non-blocking asynchronous snapshots. By introducing State Providers, we efficiently coalesce fragmented, heterogeneous shards and overlap the serialization of metadata with bulk tensor I/O. We evaluate DataStates-LLM on models up to 70B parameters on 256 A100-40GB GPUs. Our results demonstrate that DataStates-LLM achieves up to 4$\\times$ higher checkpointing throughput and reduces end-to-end training time by up to 2.2$\\times$ compared to state-of-the-art solutions, effectively mitigating the serialization and heterogeneity bottlenecks in extreme-scale LLM training.",
    "link": "http://arxiv.org/abs/2601.16956v1",
    "date": "2026-01-23",
    "fetched_at": "2026-01-26"
  },
  {
    "id": "http://arxiv.org/abs/2601.16946v1",
    "title": "Strategies for Span Labeling with Large Language Models",
    "authors": [
      "Danil Semin",
      "Ond\u0159ej Du\u0161ek",
      "Zden\u011bk Kasner"
    ],
    "summary": "Large language models (LLMs) are increasingly used for text analysis tasks, such as named entity recognition or error detection. Unlike encoder-based models, however, generative architectures lack an explicit mechanism to refer to specific parts of their input. This leads to a variety of ad-hoc prompting strategies for span labeling, often with inconsistent results. In this paper, we categorize these strategies into three families: tagging the input text, indexing numerical positions of spans, and matching span content. To address the limitations of content matching, we introduce LogitMatch, a new constrained decoding method that forces the model's output to align with valid input spans. We evaluate all methods across four diverse tasks. We find that while tagging remains a robust baseline, LogitMatch improves upon competitive matching-based methods by eliminating span matching issues and outperforms other strategies in some setups.",
    "link": "http://arxiv.org/abs/2601.16946v1",
    "date": "2026-01-23",
    "fetched_at": "2026-01-26"
  },
  {
    "id": "http://arxiv.org/abs/2601.16890v1",
    "title": "LLM-Based Adversarial Persuasion Attacks on Fact-Checking Systems",
    "authors": [
      "Jo\u00e3o A. Leite",
      "Olesya Razuvayevskaya",
      "Kalina Bontcheva",
      "Carolina Scarton"
    ],
    "summary": "Automated fact-checking (AFC) systems are susceptible to adversarial attacks, enabling false claims to evade detection. Existing adversarial frameworks typically rely on injecting noise or altering semantics, yet no existing framework exploits the adversarial potential of persuasion techniques, which are widely used in disinformation campaigns to manipulate audiences. In this paper, we introduce a novel class of persuasive adversarial attacks on AFCs by employing a generative LLM to rephrase claims using persuasion techniques. Considering 15 techniques grouped into 6 categories, we study the effects of persuasion on both claim verification and evidence retrieval using a decoupled evaluation strategy. Experiments on the FEVER and FEVEROUS benchmarks show that persuasion attacks can substantially degrade both verification performance and evidence retrieval. Our analysis identifies persuasion techniques as a potent class of adversarial attacks, highlighting the need for more robust AFC systems.",
    "link": "http://arxiv.org/abs/2601.16890v1",
    "date": "2026-01-23",
    "fetched_at": "2026-01-26"
  },
  {
    "id": "http://arxiv.org/abs/2601.16872v1",
    "title": "From Atom to Community: Structured and Evolving Agent Memory for User Behavior Modeling",
    "authors": [
      "Yuxin Liao",
      "Le Wu",
      "Min Hou",
      "Yu Wang",
      "Han Wu",
      "Meng Wang"
    ],
    "summary": "User behavior modeling lies at the heart of personalized applications like recommender systems. With LLM-based agents, user preference representation has evolved from latent embeddings to semantic memory. While existing memory mechanisms show promise in textual dialogues, modeling non-textual behaviors remains challenging, as preferences must be inferred from implicit signals like clicks without ground truth supervision. Current approaches rely on a single unstructured summary, updated through simple overwriting. However, this is suboptimal: users exhibit multi-faceted interests that get conflated, preferences evolve yet naive overwriting causes forgetting, and sparse individual interactions necessitate collaborative signals. We present STEAM (\\textit{\\textbf{ST}ructured and \\textbf{E}volving \\textbf{A}gent \\textbf{M}emory}), a novel framework that reimagines how agent memory is organized and updated. STEAM decomposes preferences into atomic memory units, each capturing a distinct interest dimension with explicit links to observed behaviors. To exploit collaborative patterns, STEAM organizes similar memories across users into communities and generates prototype memories for signal propagation. The framework further incorporates adaptive evolution mechanisms, including consolidation for refining memories and formation for capturing emerging interests. Experiments on three real-world datasets demonstrate that STEAM substantially outperforms state-of-the-art baselines in recommendation accuracy, simulation fidelity, and diversity.",
    "link": "http://arxiv.org/abs/2601.16872v1",
    "date": "2026-01-23",
    "fetched_at": "2026-01-26"
  },
  {
    "id": "http://arxiv.org/abs/2601.16858v1",
    "title": "Navigating the Shift: A Comparative Analysis of Web Search and Generative AI Response Generation",
    "authors": [
      "Mahe Chen",
      "Xiaoxuan Wang",
      "Kaiwen Chen",
      "Nick Koudas"
    ],
    "summary": "The rise of generative AI as a primary information source presents a paradigm shift from traditional web search. This paper presents a large-scale empirical study quantifying the fundamental differences between the results returned by Google Search and leading generative AI services. We analyze multiple dimensions, demonstrating that AI-generated answers and web search results diverge significantly in their consulted source domains, the typology of these domains (e.g., earned media vs. owned, social), query intent and the freshness of the information provided. We then investigate the role of LLM pre-training as a key factor shaping these differences, analyzing how this intrinsic knowledge base interacts with and influences real-time web search when enabled. Our findings reveal the distinct mechanics of these two information ecosystems, leading to critical observations on the emergent field of Answer Engine Optimization (AEO) and its contrast with traditional Search Engine Optimization (SEO).",
    "link": "http://arxiv.org/abs/2601.16858v1",
    "date": "2026-01-23",
    "fetched_at": "2026-01-26"
  },
  {
    "id": "http://arxiv.org/abs/2601.16853v1",
    "title": "Reasoning Promotes Robustness in Theory of Mind Tasks",
    "authors": [
      "Ian B. de Haan",
      "Peter van der Putten",
      "Max van Duijn"
    ],
    "summary": "Large language models (LLMs) have recently shown strong performance on Theory of Mind (ToM) tests, prompting debate about the nature and true performance of the underlying capabilities. At the same time, reasoning-oriented LLMs trained via reinforcement learning with verifiable rewards (RLVR) have achieved notable improvements across a range of benchmarks. This paper examines the behavior of such reasoning models in ToM tasks, using novel adaptations of machine psychological experiments and results from established benchmarks. We observe that reasoning models consistently exhibit increased robustness to prompt variations and task perturbations. Our analysis indicates that the observed gains are more plausibly attributed to increased robustness in finding the correct solution, rather than to fundamentally new forms of ToM reasoning. We discuss the implications of this interpretation for evaluating social-cognitive behavior in LLMs.",
    "link": "http://arxiv.org/abs/2601.16853v1",
    "date": "2026-01-23",
    "fetched_at": "2026-01-26"
  },
  {
    "id": "http://arxiv.org/abs/2601.16849v1",
    "title": "The Art of Being Difficult: Combining Human and AI Strengths to Find Adversarial Instances for Heuristics",
    "authors": [
      "Henri Nikoleit",
      "Ankit Anand",
      "Anurag Murty Naredla",
      "Heiko R\u00f6glin"
    ],
    "summary": "We demonstrate the power of human-LLM collaboration in tackling open problems in theoretical computer science. Focusing on combinatorial optimization, we refine outputs from the FunSearch algorithm [Romera-Paredes et al., Nature 2023] to derive state-of-the-art lower bounds for standard heuristics. Specifically, we target the generation of adversarial instances where these heuristics perform poorly. By iterating on FunSearch's outputs, we identify improved constructions for hierarchical $k$-median clustering, bin packing, the knapsack problem, and a generalization of Lov\u00e1sz's gasoline problem - some of these have not seen much improvement for over a decade, despite intermittent attention. These results illustrate how expert oversight can effectively extrapolate algorithmic insights from LLM-based evolutionary methods to break long-standing barriers.   Our findings demonstrate that while LLMs provide critical initial patterns, human expertise is essential for transforming these patterns into mathematically rigorous and insightful constructions. This work highlights that LLMs are a strong collaborative tool in mathematics and computer science research.",
    "link": "http://arxiv.org/abs/2601.16849v1",
    "date": "2026-01-23",
    "fetched_at": "2026-01-26"
  },
  {
    "id": "http://arxiv.org/abs/2601.16824v1",
    "title": "Privacy in Human-AI Romantic Relationships: Concerns, Boundaries, and Agency",
    "authors": [
      "Rongjun Ma",
      "Shijing He",
      "Jose Luis Martin-Navarro",
      "Xiao Zhan",
      "Jose Such"
    ],
    "summary": "An increasing number of LLM-based applications are being developed to facilitate romantic relationships with AI partners, yet the safety and privacy risks in these partnerships remain largely underexplored. In this work, we investigate privacy in human-AI romantic relationships through an interview study (N=17), examining participants' experiences and privacy perceptions across stages of exploration, intimacy, and dissolution, alongside platforms they used. We found that these relationships took varied forms, from one-to-one to one-to-many, and were shaped by multiple actors, including creators, platforms, and moderators. AI partners were perceived as having agency, actively negotiating privacy boundaries with participants and sometimes encouraging disclosure of personal details. As intimacy deepened, these boundaries became more permeable, though some participants voiced concerns such as conversation exposure and sought to preserve anonymity. Overall, platform affordances and diverse romantic dynamics expand the privacy landscape, underscoring the need to rethink how privacy is constructed in human-AI intimacy.",
    "link": "http://arxiv.org/abs/2601.16824v1",
    "date": "2026-01-23",
    "fetched_at": "2026-01-26"
  },
  {
    "id": "http://arxiv.org/abs/2601.16823v1",
    "title": "Trapped in the past? Disentangling fluid and crystallized intelligence of large language models using chess",
    "authors": [
      "Leonard S. Pleiss",
      "Maximilian Schiffer",
      "Robert K. von Weizs\u00e4cker"
    ],
    "summary": "Large Language Models (LLMs) exhibit remarkable capabilities, yet it remains unclear to what extent these reflect sophisticated recall (crystallized intelligence) or reasoning ability (fluid intelligence). We introduce chess as a controlled testbed for disentangling these faculties. Leveraging the game's structure and scalable engine evaluations, we construct a taxonomy of positions varying in training corpus proximity--ranging from common states solvable by memorization to novel ones requiring first-principles reasoning. We systematically evaluate multiple GPT generations under varying reasoning intensities. Our analysis reveals a clear gradient: performance consistently degrades as fluid intelligence demands increase. Notably, in out-of-distribution tasks, performance collapses to random levels. While newer models improve, progress slows significantly for tasks outside the training distribution. Furthermore, while reasoning-augmented inference improves performance, its marginal benefit per token decreases with distributional proximity. These results suggest current architectures remain limited in systematic generalization, highlighting the need for mechanisms beyond scale to achieve robust fluid intelligence.",
    "link": "http://arxiv.org/abs/2601.16823v1",
    "date": "2026-01-23",
    "fetched_at": "2026-01-26"
  },
  {
    "id": "http://arxiv.org/abs/2601.16800v1",
    "title": "Large Language Models as Automatic Annotators and Annotation Adjudicators for Fine-Grained Opinion Analysis",
    "authors": [
      "Gaurav Negi",
      "MA Waskow",
      "Paul Buitelaar"
    ],
    "summary": "Fine-grained opinion analysis of text provides a detailed understanding of expressed sentiments, including the addressed entity. Although this level of detail is sound, it requires considerable human effort and substantial cost to annotate opinions in datasets for training models, especially across diverse domains and real-world applications. We explore the feasibility of LLMs as automatic annotators for fine-grained opinion analysis, addressing the shortage of domain-specific labelled datasets. In this work, we use a declarative annotation pipeline. This approach reduces the variability of manual prompt engineering when using LLMs to identify fine-grained opinion spans in text. We also present a novel methodology for an LLM to adjudicate multiple labels and produce final annotations. After trialling the pipeline with models of different sizes for the Aspect Sentiment Triplet Extraction (ASTE) and Aspect-Category-Opinion-Sentiment (ACOS) analysis tasks, we show that LLMs can serve as automatic annotators and adjudicators, achieving high Inter-Annotator Agreement across individual LLM-based annotators. This reduces the cost and human effort needed to create these fine-grained opinion-annotated datasets.",
    "link": "http://arxiv.org/abs/2601.16800v1",
    "date": "2026-01-23",
    "fetched_at": "2026-01-26"
  },
  {
    "id": "http://arxiv.org/abs/2601.16781v1",
    "title": "Persuasion Tokens for Editing Factual Knowledge in LLMs",
    "authors": [
      "Paul Youssef",
      "J\u00f6rg Schl\u00f6tterer",
      "Christin Seifert"
    ],
    "summary": "In-context knowledge editing (IKE) is a promising technique for updating Large Language Models (LLMs) with new information. However, IKE relies on lengthy, fact-specific demonstrations which are costly to create and consume significant context window space. In this paper, we introduce persuasion tokens (P-Tokens) -- special tokens trained to replicate the effect of IKE demonstrations, enabling efficient knowledge editing without requiring fact-specific demonstrations. We evaluate P-Tokens across two editing datasets and three LLMs, demonstrating performance comparable to, and often exceeding, IKE. We further find that editing performance is robust to distractors with small negative effects to neighboring facts, and that increasing the number of P-Tokens improves performance. Our work addresses key limitations of IKE and provides a more practical and scalable alternative for editing LLMs.",
    "link": "http://arxiv.org/abs/2601.16781v1",
    "date": "2026-01-23",
    "fetched_at": "2026-01-26"
  },
  {
    "id": "http://arxiv.org/abs/2601.16778v1",
    "title": "GTA: Generative Traffic Agents for Simulating Realistic Mobility Behavior",
    "authors": [
      "Simon L\u00e4mmer",
      "Mark Colley",
      "Patrick Ebel"
    ],
    "summary": "People's transportation choices reflect complex trade-offs shaped by personal preferences, social norms, and technology acceptance. Predicting such behavior at scale is a critical challenge with major implications for urban planning and sustainable transport. Traditional methods use handcrafted assumptions and costly data collection, making them impractical for early-stage evaluations of new technologies or policies. We introduce Generative Traffic Agents (GTA) for simulating large-scale, context-sensitive transportation choices using LLM-powered, persona-based agents. GTA generates artificial populations from census-based sociodemographic data. It simulates activity schedules and mode choices, enabling scalable, human-like simulations without handcrafted rules. We evaluate GTA in Berlin-scale experiments, comparing simulation results against empirical data. While agents replicate patterns, such as modal split by socioeconomic status, they show systematic biases in trip length and mode preference. GTA offers new opportunities for modeling how future innovations, from bike lanes to transit apps, shape mobility decisions.",
    "link": "http://arxiv.org/abs/2601.16778v1",
    "date": "2026-01-23",
    "fetched_at": "2026-01-26"
  },
  {
    "id": "http://arxiv.org/abs/2601.16775v1",
    "title": "LLM-powered Real-time Patent Citation Recommendation for Financial Technologies",
    "authors": [
      "Tianang Deng",
      "Yu Deng",
      "Tianchen Gao",
      "Yonghong Hu",
      "Rui Pan"
    ],
    "summary": "Rapid financial innovation has been accompanied by a sharp increase in patenting activity, making timely and comprehensive prior-art discovery more difficult. This problem is especially evident in financial technologies, where innovations develop quickly, patent collections grow continuously, and citation recommendation systems must be updated as new applications arrive. Existing patent retrieval and citation recommendation methods typically rely on static indexes or periodic retraining, which limits their ability to operate effectively in such dynamic settings. In this study, we propose a real-time patent citation recommendation framework designed for large and fast-changing financial patent corpora. Using a dataset of 428,843 financial patents granted by the China National Intellectual Property Administration (CNIPA) between 2000 and 2024, we build a three-stage recommendation pipeline. The pipeline uses large language model (LLM) embeddings to represent the semantic content of patent abstracts, applies efficient approximate nearest-neighbor search to construct a manageable candidate set, and ranks candidates by semantic similarity to produce top-k citation recommendations. In addition to improving recommendation accuracy, the proposed framework directly addresses the dynamic nature of patent systems. By using an incremental indexing strategy based on hierarchical navigable small-world (HNSW) graphs, newly issued patents can be added without rebuilding the entire index. A rolling day-by-day update experiment shows that incremental updating improves recall while substantially reducing computational cost compared with rebuild-based indexing. The proposed method also consistently outperforms traditional text-based baselines and alternative nearest-neighbor retrieval approaches.",
    "link": "http://arxiv.org/abs/2601.16775v1",
    "date": "2026-01-23",
    "fetched_at": "2026-01-26"
  },
  {
    "id": "http://arxiv.org/abs/2601.16766v1",
    "title": "Do LLM hallucination detectors suffer from low-resource effect?",
    "authors": [
      "Debtanu Datta",
      "Mohan Kishore Chilukuri",
      "Yash Kumar",
      "Saptarshi Ghosh",
      "Muhammad Bilal Zafar"
    ],
    "summary": "LLMs, while outperforming humans in a wide range of tasks, can still fail in unanticipated ways. We focus on two pervasive failure modes: (i) hallucinations, where models produce incorrect information about the world, and (ii) the low-resource effect, where the models show impressive performance in high-resource languages like English but the performance degrades significantly in low-resource languages like Bengali. We study the intersection of these issues and ask: do hallucination detectors suffer from the low-resource effect? We conduct experiments on five tasks across three domains (factual recall, STEM, and Humanities). Experiments with four LLMs and three hallucination detectors reveal a curious finding: As expected, the task accuracies in low-resource languages experience large drops (compared to English). However, the drop in detectors' accuracy is often several times smaller than the drop in task accuracy. Our findings suggest that even in low-resource languages, the internal mechanisms of LLMs might encode signals about their uncertainty. Further, the detectors are robust within language (even for non-English) and in multilingual setups, but not in cross-lingual settings without in-language supervision.",
    "link": "http://arxiv.org/abs/2601.16766v1",
    "date": "2026-01-23",
    "fetched_at": "2026-01-26"
  },
  {
    "id": "http://arxiv.org/abs/2601.16753v1",
    "title": "Standardizing Longitudinal Radiology Report Evaluation via Large Language Model Annotation",
    "authors": [
      "Xinyi Wang",
      "Grazziela Figueredo",
      "Ruizhe Li",
      "Xin Chen"
    ],
    "summary": "Longitudinal information in radiology reports refers to the sequential tracking of findings across multiple examinations over time, which is crucial for monitoring disease progression and guiding clinical decisions. Many recent automated radiology report generation methods are designed to capture longitudinal information; however, validating their performance is challenging. There is no proper tool to consistently label temporal changes in both ground-truth and model-generated texts for meaningful comparisons. Existing annotation methods are typically labor-intensive, relying on the use of manual lexicons and rules. Complex rules are closed-source, domain specific and hard to adapt, whereas overly simple ones tend to miss essential specialised information. Large language models (LLMs) offer a promising annotation alternative, as they are capable of capturing nuanced linguistic patterns and semantic similarities without extensive manual intervention. They also adapt well to new contexts. In this study, we therefore propose an LLM-based pipeline to automatically annotate longitudinal information in radiology reports. The pipeline first identifies sentences containing relevant information and then extracts the progression of diseases. We evaluate and compare five mainstream LLMs on these two tasks using 500 manually annotated reports. Considering both efficiency and performance, Qwen2.5-32B was subsequently selected and used to annotate another 95,169 reports from the public MIMIC-CXR dataset. Our Qwen2.5-32B-annotated dataset provided us with a standardized benchmark for evaluating report generation models. Using this new benchmark, we assessed seven state-of-the-art report generation models. Our LLM-based annotation method outperforms existing annotation solutions, achieving 11.3\\% and 5.3\\% higher F1-scores for longitudinal information detection and disease tracking, respectively.",
    "link": "http://arxiv.org/abs/2601.16753v1",
    "date": "2026-01-23",
    "fetched_at": "2026-01-26"
  },
  {
    "id": "http://arxiv.org/abs/2601.16746v1",
    "title": "SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents",
    "authors": [
      "Yuhang Wang",
      "Yuling Shi",
      "Mo Yang",
      "Rongrui Zhang",
      "Shilin He",
      "Heng Lian",
      "Yuting Chen",
      "Siyu Ye",
      "Kai Cai",
      "Xiaodong Gu"
    ],
    "summary": "LLM agents have demonstrated remarkable capabilities in software development, but their performance is hampered by long interaction contexts, which incur high API costs and latency. While various context compression approaches such as LongLLMLingua have emerged to tackle this challenge, they typically rely on fixed metrics such as PPL, ignoring the task-specific nature of code understanding. As a result, they frequently disrupt syntactic and logical structure and fail to retain critical implementation details. In this paper, we propose SWE-Pruner, a self-adaptive context pruning framework tailored for coding agents. Drawing inspiration from how human programmers \"selectively skim\" source code during development and debugging, SWE-Pruner performs task-aware adaptive pruning for long contexts. Given the current task, the agent formulates an explicit goal (e.g., \"focus on error handling\") as a hint to guide the pruning targets. A lightweight neural skimmer (0.6B parameters) is trained to dynamically select relevant lines from the surrounding context given the goal. Evaluations across four benchmarks and multiple models validate SWE-Pruner's effectiveness in various scenarios, achieving 23-54% token reduction on agent tasks like SWE-Bench Verified and up to 14.84x compression on single-turn tasks like LongCodeQA with minimal performance impact.",
    "link": "http://arxiv.org/abs/2601.16746v1",
    "date": "2026-01-23",
    "fetched_at": "2026-01-26"
  },
  {
    "id": "http://arxiv.org/abs/2601.11522v1",
    "title": "UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation",
    "authors": [
      "Ruiheng Zhang",
      "Jingfeng Yao",
      "Huangxuan Zhao",
      "Hao Yan",
      "Xiao He",
      "Lei Chen",
      "Zhou Wei",
      "Yong Luo",
      "Zengmao Wang",
      "Lefei Zhang",
      "Dacheng Tao",
      "Bo Du"
    ],
    "summary": "Despite recent progress, medical foundation models still struggle to unify visual understanding and generation, as these tasks have inherently conflicting goals: semantic abstraction versus pixel-level reconstruction. Existing approaches, typically based on parameter-shared autoregressive architectures, frequently lead to compromised performance in one or both tasks. To address this, we present UniX, a next-generation unified medical foundation model for chest X-ray understanding and generation. UniX decouples the two tasks into an autoregressive branch for understanding and a diffusion branch for high-fidelity generation. Crucially, a cross-modal self-attention mechanism is introduced to dynamically guide the generation process with understanding features. Coupled with a rigorous data cleaning pipeline and a multi-stage training strategy, this architecture enables synergistic collaboration between tasks while leveraging the strengths of diffusion models for superior generation. On two representative benchmarks, UniX achieves a 46.1% improvement in understanding performance (Micro-F1) and a 24.2% gain in generation quality (FD-RadDino), using only a quarter of the parameters of LLM-CXR. By achieving performance on par with task-specific models, our work establishes a scalable paradigm for synergistic medical image understanding and generation. Codes and models are available at https://github.com/ZrH42/UniX.",
    "link": "http://arxiv.org/abs/2601.11522v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11518v1",
    "title": "How Long Is a Piece of String? A Brief Empirical Analysis of Tokenizers",
    "authors": [
      "Jonathan Roberts",
      "Kai Han",
      "Samuel Albanie"
    ],
    "summary": "Frontier LLMs are increasingly utilised across academia, society and industry. A commonly used unit for comparing models, their inputs and outputs, and estimating inference pricing is the token. In general, tokens are used as a stable currency, assumed to be broadly consistent across tokenizers and contexts, enabling direct comparisons. However, tokenization varies significantly across models and domains of text, making naive interpretation of token counts problematic. We quantify this variation by providing a comprehensive empirical analysis of tokenization, exploring the compression of sequences to tokens across different distributions of textual data. Our analysis challenges commonly held heuristics about token lengths, finding them to be overly simplistic. We hope the insights of our study add clarity and intuition toward tokenization in contemporary LLMs.",
    "link": "http://arxiv.org/abs/2601.11518v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11488v1",
    "title": "CTest-Metric: A Unified Framework to Assess Clinical Validity of Metrics for CT Report Generation",
    "authors": [
      "Vanshali Sharma",
      "Andrea Mia Bejar",
      "Gorkem Durak",
      "Ulas Bagci"
    ],
    "summary": "In the generative AI era, where even critical medical tasks are increasingly automated, radiology report generation (RRG) continues to rely on suboptimal metrics for quality assessment. Developing domain-specific metrics has therefore been an active area of research, yet it remains challenging due to the lack of a unified, well-defined framework to assess their robustness and applicability in clinical contexts. To address this, we present CTest-Metric, a first unified metric assessment framework with three modules determining the clinical feasibility of metrics for CT RRG. The modules test: (i) Writing Style Generalizability (WSG) via LLM-based rephrasing; (ii) Synthetic Error Injection (SEI) at graded severities; and (iii) Metrics-vs-Expert correlation (MvE) using clinician ratings on 175 \"disagreement\" cases. Eight widely used metrics (BLEU, ROUGE, METEOR, BERTScore-F1, F1-RadGraph, RaTEScore, GREEN Score, CRG) are studied across seven LLMs built on a CT-CLIP encoder. Using our novel framework, we found that lexical NLG metrics are highly sensitive to stylistic variations; GREEN Score aligns best with expert judgments (Spearman~0.70), while CRG shows negative correlation; and BERTScore-F1 is least sensitive to factual error injection. We will release the framework, code, and allowable portion of the anonymized evaluation data (rephrased/error-injected CT reports), to facilitate reproducible benchmarking and future metric development.",
    "link": "http://arxiv.org/abs/2601.11488v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11479v1",
    "title": "Health Facility Location in Ethiopia: Leveraging LLMs to Integrate Expert Knowledge into Algorithmic Planning",
    "authors": [
      "Yohai Trabelsi",
      "Guojun Xiong",
      "Fentabil Getnet",
      "St\u00e9phane Verguet",
      "Milind Tambe"
    ],
    "summary": "Ethiopia's Ministry of Health is upgrading health posts to improve access to essential services, particularly in rural areas. Limited resources, however, require careful prioritization of which facilities to upgrade to maximize population coverage while accounting for diverse expert and stakeholder preferences. In collaboration with the Ethiopian Public Health Institute and Ministry of Health, we propose a hybrid framework that systematically integrates expert knowledge with optimization techniques. Classical optimization methods provide theoretical guarantees but require explicit, quantitative objectives, whereas stakeholder criteria are often articulated in natural language and difficult to formalize. To bridge these domains, we develop the Large language model and Extended Greedy (LEG) framework. Our framework combines a provable approximation algorithm for population coverage optimization with LLM-driven iterative refinement that incorporates human-AI alignment to ensure solutions reflect expert qualitative guidance while preserving coverage guarantees. Experiments on real-world data from three Ethiopian regions demonstrate the framework's effectiveness and its potential to inform equitable, data-driven health system planning.",
    "link": "http://arxiv.org/abs/2601.11479v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11468v1",
    "title": "Exploring LLM Features in Predictive Process Monitoring for Small-Scale Event-Logs",
    "authors": [
      "Alessandro Padella",
      "Massimiliano de Leoni",
      "Marlon Dumas"
    ],
    "summary": "Predictive Process Monitoring is a branch of process mining that aims to predict the outcome of an ongoing process. Recently, it leveraged machine-and-deep learning architectures. In this paper, we extend our prior LLM-based Predictive Process Monitoring framework, which was initially focused on total time prediction via prompting. The extension consists of comprehensively evaluating its generality, semantic leverage, and reasoning mechanisms, also across multiple Key Performance Indicators. Empirical evaluations conducted on three distinct event logs and across the Key Performance Indicators of Total Time and Activity Occurrence prediction indicate that, in data-scarce settings with only 100 traces, the LLM surpasses the benchmark methods. Furthermore, the experiments also show that the LLM exploits both its embodied prior knowledge and the internal correlations among training traces. Finally, we examine the reasoning strategies employed by the model, demonstrating that the LLM does not merely replicate existing predictive methods but performs higher-order reasoning to generate the predictions.",
    "link": "http://arxiv.org/abs/2601.11468v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11441v1",
    "title": "Hierarchical Orthogonal Residual Spread for Precise Massive Editing in Large Language Models",
    "authors": [
      "Xiaojie Gu",
      "Guangxu Chen",
      "Yuheng Yang",
      "Jingxin Han",
      "Andi Zhang"
    ],
    "summary": "Large language models (LLMs) exhibit exceptional performance across various domains, yet they face critical safety concerns. Model editing has emerged as an effective approach to mitigate these issues. Existing model editing methods often focus on optimizing an information matrix that blends new and old knowledge. While effective, these approaches can be computationally expensive and may cause conflicts. In contrast, we shift our attention to Hierarchical Orthogonal Residual SprEad of the information matrix, which reduces noisy gradients and enables more stable edits from a different perspective. We demonstrate the effectiveness of our method HORSE through a clear theoretical comparison with several popular methods and extensive experiments conducted on two datasets across multiple LLMs. The results show that HORSE maintains precise massive editing across diverse scenarios. The code is available at https://github.com/XiaojieGu/HORSE",
    "link": "http://arxiv.org/abs/2601.11441v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11432v1",
    "title": "The unreasonable effectiveness of pattern matching",
    "authors": [
      "Gary Lupyan",
      "Blaise Ag\u00fcera y Arcas"
    ],
    "summary": "We report on an astonishing ability of large language models (LLMs) to make sense of \"Jabberwocky\" language in which most or all content words have been randomly replaced by nonsense strings, e.g., translating \"He dwushed a ghanc zawk\" to \"He dragged a spare chair\". This result addresses ongoing controversies regarding how to best think of what LLMs are doing: are they a language mimic, a database, a blurry version of the Web? The ability of LLMs to recover meaning from structural patterns speaks to the unreasonable effectiveness of pattern-matching. Pattern-matching is not an alternative to \"real\" intelligence, but rather a key ingredient.",
    "link": "http://arxiv.org/abs/2601.11432v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11429v1",
    "title": "Relational Linearity is a Predictor of Hallucinations",
    "authors": [
      "Yuetian Lu",
      "Yihong Liu",
      "Hinrich Sch\u00fctze"
    ],
    "summary": "Hallucination is a central failure mode in large language models (LLMs). We focus on hallucinations of answers to questions like: \"Which instrument did Glenn Gould play?\", but we ask these questions for synthetic entities that are unknown to the model. Surprisingly, we find that medium-size models like Gemma-7B-IT frequently hallucinate, i.e., they have difficulty recognizing that the hallucinated fact is not part of their knowledge. We hypothesize that an important factor in causing these hallucinations is the linearity of the relation: linear relations tend to be stored more abstractly, making it difficult for the LLM to assess its knowledge; the facts of nonlinear relations tend to be stored more directly, making knowledge assessment easier. To investigate this hypothesis, we create SyntHal, a dataset of 6000 synthetic entities for six relations. In our experiments with four models, we determine, for each relation, the hallucination rate on SyntHal and also measure its linearity, using $\u0394\\cos$. We find a strong correlation ($r \\in [.78,.82]$) between relational linearity and hallucination rate, providing evidence for our hypothesis that the underlying storage of triples of a relation is a factor in how well a model can self-assess its knowledge. This finding has implications for how to manage hallucination behavior and suggests new research directions for improving the representation of factual knowledge in LLMs.",
    "link": "http://arxiv.org/abs/2601.11429v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11398v1",
    "title": "Understanding Help Seeking for Digital Privacy, Safety, and Security",
    "authors": [
      "Kurt Thomas",
      "Sai Teja Peddinti",
      "Sarah Meiklejohn",
      "Tara Matthews",
      "Amelia Hassoun",
      "Animesh Srivastava",
      "Jessica McClearn",
      "Patrick Gage Kelley",
      "Sunny Consolvo",
      "Nina Taft"
    ],
    "summary": "The complexity of navigating digital privacy, safety, and security threats often falls directly on users. This leads to users seeking help from family and peers, platforms and advice guides, dedicated communities, and even large language models (LLMs). As a precursor to improving resources across this ecosystem, our community needs to understand what help seeking looks like in the wild. To that end, we blend qualitative coding with LLM fine-tuning to sift through over one billion Reddit posts from the last four years to identify where and for what users seek digital privacy, safety, or security help. We isolate three million relevant posts with 93% precision and recall and automatically annotate each with the topics discussed (e.g., security tools, privacy configurations, scams, account compromise, content moderation, and more). We use this dataset to understand the scope and scale of help seeking, the communities that provide help, and the types of help sought. Our work informs the development of better resources for users (e.g., user guides or LLM help-giving agents) while underscoring the inherent challenges of supporting users through complex combinations of threats, platforms, mitigations, context, and emotions.",
    "link": "http://arxiv.org/abs/2601.11398v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11379v1",
    "title": "Evaluating LLM Behavior in Hiring: Implicit Weights, Fairness Across Groups, and Alignment with Human Preferences",
    "authors": [
      "Morgane Hoffmann",
      "Emma Jouffroy",
      "Warren Jouanneau",
      "Marc Palyart",
      "Charles Pebereau"
    ],
    "summary": "General-purpose Large Language Models (LLMs) show significant potential in recruitment applications, where decisions require reasoning over unstructured text, balancing multiple criteria, and inferring fit and competence from indirect productivity signals. Yet, it is still uncertain how LLMs assign importance to each attribute and whether such assignments are in line with economic principles, recruiter preferences or broader societal norms. We propose a framework to evaluate an LLM's decision logic in recruitment, by drawing on established economic methodologies for analyzing human hiring behavior. We build synthetic datasets from real freelancer profiles and project descriptions from a major European online freelance marketplace and apply a full factorial design to estimate how a LLM weighs different match-relevant criteria when evaluating freelancer-project fit. We identify which attributes the LLM prioritizes and analyze how these weights vary across project contexts and demographic subgroups. Finally, we explain how a comparable experimental setup could be implemented with human recruiters to assess alignment between model and human decisions. Our findings reveal that the LLM weighs core productivity signals, such as skills and experience, but interprets certain features beyond their explicit matching value. While showing minimal average discrimination against minority groups, intersectional effects reveal that productivity signals carry different weights between demographic groups.",
    "link": "http://arxiv.org/abs/2601.11379v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11374v1",
    "title": "Reward Modeling for Scientific Writing Evaluation",
    "authors": [
      "Furkan \u015eahinu\u00e7",
      "Subhabrata Dutta",
      "Iryna Gurevych"
    ],
    "summary": "Scientific writing is an expert-domain task that demands deep domain knowledge, task-specific requirements and reasoning capabilities that leverage the domain knowledge to satisfy the task specifications. While scientific text generation has been widely studied, its evaluation remains a challenging and open problem. It is critical to develop models that can be reliably deployed for evaluating diverse open-ended scientific writing tasks while adhering to their distinct requirements. However, existing LLM-based judges and reward models are primarily optimized for general-purpose benchmarks with fixed scoring rubrics and evaluation criteria. Consequently, they often fail to reason over sparse knowledge of scientific domains when interpreting task-dependent and multi-faceted criteria. Moreover, fine-tuning for each individual task is costly and impractical for low-resource settings. To bridge these gaps, we propose cost-efficient, open-source reward models tailored for scientific writing evaluation. We introduce a two-stage training framework that initially optimizes scientific evaluation preferences and then refines reasoning capabilities. Our multi-aspect evaluation design and joint training across diverse tasks enable fine-grained assessment and robustness to dynamic criteria and scoring rubrics. Experimental analysis shows that our training regime strongly improves LLM-based scientific writing evaluation. Our models generalize effectively across tasks and to previously unseen scientific writing evaluation settings, allowing a single trained evaluator to be reused without task-specific retraining.",
    "link": "http://arxiv.org/abs/2601.11374v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11369v1",
    "title": "Institutional AI: Governing LLM Collusion in Multi-Agent Cournot Markets via Public Governance Graphs",
    "authors": [
      "Marcantonio Bracale Syrnikov",
      "Federico Pierucci",
      "Marcello Galisai",
      "Matteo Prandi",
      "Piercosma Bisconti",
      "Francesco Giarrusso",
      "Olga Sorokoletova",
      "Vincenzo Suriani",
      "Daniele Nardi"
    ],
    "summary": "Multi-agent LLM ensembles can converge on coordinated, socially harmful equilibria. This paper advances an experimental framework for evaluating Institutional AI, our system-level approach to AI alignment that reframes alignment from preference engineering in agent-space to mechanism design in institution-space. Central to this approach is the governance graph, a public, immutable manifest that declares legal states, transitions, sanctions, and restorative paths; an Oracle/Controller runtime interprets this manifest, attaching enforceable consequences to evidence of coordination while recording a cryptographically keyed, append-only governance log for audit and provenance. We apply the Institutional AI framework to govern the Cournot collusion case documented by prior work and compare three regimes: Ungoverned (baseline incentives from the structure of the Cournot market), Constitutional (a prompt-only policy-as-prompt prohibition implemented as a fixed written anti-collusion constitution, and Institutional (governance-graph-based). Across six model configurations including cross-provider pairs (N=90 runs/condition), the Institutional regime produces large reductions in collusion: mean tier falls from 3.1 to 1.8 (Cohen's d=1.28), and severe-collusion incidence drops from 50% to 5.6%. The prompt-only Constitutional baseline yields no reliable improvement, illustrating that declarative prohibitions do not bind under optimisation pressure. These results suggest that multi-agent alignment may benefit from being framed as an institutional design problem, where governance graphs can provide a tractable abstraction for alignment-relevant collective behavior.",
    "link": "http://arxiv.org/abs/2601.11369v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11362v1",
    "title": "RITA: A Tool for Automated Requirements Classification and Specification from Online User Feedback",
    "authors": [
      "Manjeshwar Aniruddh Mallya",
      "Alessio Ferrari",
      "Mohammad Amin Zadenoori",
      "Jacek D\u0105browski"
    ],
    "summary": "Context and motivation. Online user feedback is a valuable resource for requirements engineering, but its volume and noise make analysis difficult. Existing tools support individual feedback analysis tasks, but their capabilities are rarely integrated into end-to-end support. Problem. The lack of end-to-end integration limits the practical adoption of existing RE tools and makes it difficult to assess their real-world usefulness. Solution. To address this challenge, we present RITA, a tool that integrates lightweight open-source large language models into a unified workflow for feedback-driven RE. RITA supports automated request classification, non-functional requirement identification, and natural-language requirements specification generation from online feedback via a user-friendly interface, and integrates with Jira for seamless transfer of requirements specifications to development tools. Results and conclusions. RITA exploits previously evaluated LLM-based RE techniques to efficiently transform raw user feedback into requirements artefacts, helping bridge the gap between research and practice. A demonstration is available at: https://youtu.be/8meCLpwQWV8.",
    "link": "http://arxiv.org/abs/2601.11362v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11354v1",
    "title": "AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems",
    "authors": [
      "Weiyi Wang",
      "Xinchi Chen",
      "Jingjing Gong",
      "Xuanjing Huang",
      "Xipeng Qiu"
    ],
    "summary": "Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.",
    "link": "http://arxiv.org/abs/2601.11354v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11344v1",
    "title": "How Much Would a Clinician Edit This Draft? Evaluating LLM Alignment for Patient Message Response Drafting",
    "authors": [
      "Parker Seegmiller",
      "Joseph Gatto",
      "Sarah E. Greer",
      "Ganza Belise Isingizwe",
      "Rohan Ray",
      "Timothy E. Burdick",
      "Sarah Masud Preum"
    ],
    "summary": "Large language models (LLMs) show promise in drafting responses to patient portal messages, yet their integration into clinical workflows raises various concerns, including whether they would actually save clinicians time and effort in their portal workload. We investigate LLM alignment with individual clinicians through a comprehensive evaluation of the patient message response drafting task. We develop a novel taxonomy of thematic elements in clinician responses and propose a novel evaluation framework for assessing clinician editing load of LLM-drafted responses at both content and theme levels. We release an expert-annotated dataset and conduct large-scale evaluations of local and commercial LLMs using various adaptation techniques including thematic prompting, retrieval-augmented generation, supervised fine-tuning, and direct preference optimization. Our results reveal substantial epistemic uncertainty in aligning LLM drafts with clinician responses. While LLMs demonstrate capability in drafting certain thematic elements, they struggle with clinician-aligned generation in other themes, particularly question asking to elicit further information from patients. Theme-driven adaptation strategies yield improvements across most themes. Our findings underscore the necessity of adapting LLMs to individual clinician preferences to enable reliable and responsible use in patient-clinician communication workflows.",
    "link": "http://arxiv.org/abs/2601.11344v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11342v1",
    "title": "Unlocking the Potentials of Retrieval-Augmented Generation for Diffusion Language Models",
    "authors": [
      "Chuanyue Yu",
      "Jiahui Wang",
      "Yuhan Li",
      "Heng Chang",
      "Ge Lan",
      "Qingyun Sun",
      "Jia Li",
      "Jianxin Li",
      "Ziwei Zhang"
    ],
    "summary": "Diffusion Language Models (DLMs) have recently demonstrated remarkable capabilities in natural language processing tasks. However, the potential of Retrieval-Augmented Generation (RAG), which shows great successes for enhancing large language models (LLMs), has not been well explored, due to the fundamental difference between LLM and DLM decoding. To fill this critical gap, we systematically test the performance of DLMs within the RAG framework. Our findings reveal that DLMs coupled with RAG show promising potentials with stronger dependency on contextual information, but suffer from limited generation precision. We identify a key underlying issue: Response Semantic Drift (RSD), where the generated answer progressively deviates from the query's original semantics, leading to low precision content. We trace this problem to the denoising strategies in DLMs, which fail to maintain semantic alignment with the query throughout the iterative denoising process. To address this, we propose Semantic-Preserving REtrieval-Augmented Diffusion (SPREAD), a novel framework that introduces a query-relevance-guided denoising strategy. By actively guiding the denoising trajectory, SPREAD ensures the generation remains anchored to the query's semantics and effectively suppresses drift. Experimental results demonstrate that SPREAD significantly enhances the precision and effectively mitigates RSD of generated answers within the RAG framework.",
    "link": "http://arxiv.org/abs/2601.11342v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11332v1",
    "title": "Idea First, Code Later: Disentangling Problem Solving from Code Generation in Evaluating LLMs for Competitive Programming",
    "authors": [
      "Sama Hadhoud",
      "Alaa Elsetohy",
      "Frederikus Hudi",
      "Jan Christian Blaise Cruz",
      "Steven Halim",
      "Alham Fikri Aji"
    ],
    "summary": "Large Language Models (LLMs) increasingly succeed on competitive programming problems, yet existing evaluations conflate algorithmic reasoning with code-level implementation. We argue that competitive programming is fundamentally a problem-solving task and propose centering natural-language editorials in both solution generation and evaluation. Generating an editorial prior to code improves solve rates for some LLMs, with substantially larger gains when using expertly written gold editorials. However, even with gold editorials, models continue to struggle with implementation, while the gap between generated and gold editorials reveals a persistent problem-solving bottleneck in specifying correct and complete algorithms. Beyond pass/fail metrics, we diagnose reasoning errors by comparing model-generated editorials to gold standards using expert annotations and validate an LLM-as-a-judge protocol for scalable evaluation. We introduce a dataset of 83 ICPC-style problems with gold editorials and full test suites, and evaluate 19 LLMs, arguing that future benchmarks should explicitly separate problem solving from implementation.",
    "link": "http://arxiv.org/abs/2601.11332v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11327v1",
    "title": "Can Small Agent Collaboration Beat a Single Big LLM?",
    "authors": [
      "Agata \u017bywot",
      "Xinyi Chen",
      "Maarten de Rijke"
    ],
    "summary": "This report studies whether small, tool-augmented agents can match or outperform larger monolithic models on the GAIA benchmark. Using Qwen3 models (4B-32B) within an adapted Agentic-Reasoning framework, we isolate the effects of model scale, explicit thinking (no thinking, planner-only, or full), and tool use (search, code, mind-map). Tool augmentation provides the largest and most consistent gains. Using tools, 4B models can outperform 32B models without tool access on GAIA in our experimental setup. In contrast, explicit thinking is highly configuration- and difficulty-dependent: planner-only thinking can improve decomposition and constraint tracking, while unrestricted full thinking often degrades performance by destabilizing tool orchestration, leading to skipped verification steps, excessive tool calls, non-termination, and output-format drift.",
    "link": "http://arxiv.org/abs/2601.11327v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11314v1",
    "title": "Membership Inference on LLMs in the Wild",
    "authors": [
      "Jiatong Yi",
      "Yanyang Li"
    ],
    "summary": "Membership Inference Attacks (MIAs) act as a crucial auditing tool for the opaque training data of Large Language Models (LLMs). However, existing techniques predominantly rely on inaccessible model internals (e.g., logits) or suffer from poor generalization across domains in strict black-box settings where only generated text is available. In this work, we propose SimMIA, a robust MIA framework tailored for this text-only regime by leveraging an advanced sampling strategy and scoring mechanism. Furthermore, we present WikiMIA-25, a new benchmark curated to evaluate MIA performance on modern proprietary LLMs. Experiments demonstrate that SimMIA achieves state-of-the-art results in the black-box setting, rivaling baselines that exploit internal model information.",
    "link": "http://arxiv.org/abs/2601.11314v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.11311v1",
    "title": "FORESTLLM: Large Language Models Make Random Forest Great on Few-shot Tabular Learning",
    "authors": [
      "Zhihan Yang",
      "Jiaqi Wei",
      "Xiang Zhang",
      "Haoyu Dong",
      "Yiwen Wang",
      "Xiaoke Guo",
      "Pengkun Zhang",
      "Yiwei Xu",
      "Chenyu You"
    ],
    "summary": "Tabular data high-stakes critical decision-making in domains such as finance, healthcare, and scientific discovery. Yet, learning effectively from tabular data in few-shot settings, where labeled examples are scarce, remains a fundamental challenge. Traditional tree-based methods often falter in these regimes due to their reliance on statistical purity metrics, which become unstable and prone to overfitting with limited supervision. At the same time, direct applications of large language models (LLMs) often overlook its inherent structure, leading to suboptimal performance. To overcome these limitations, we propose FORESTLLM, a novel framework that unifies the structural inductive biases of decision forests with the semantic reasoning capabilities of LLMs. Crucially, FORESTLLM leverages the LLM only during training, treating it as an offline model designer that encodes rich, contextual knowledge into a lightweight, interpretable forest model, eliminating the need for LLM inference at test time. Our method is two-fold. First, we introduce a semantic splitting criterion in which the LLM evaluates candidate partitions based on their coherence over both labeled and unlabeled data, enabling the induction of more robust and generalizable tree structures under few-shot supervision. Second, we propose a one-time in-context inference mechanism for leaf node stabilization, where the LLM distills the decision path and its supporting examples into a concise, deterministic prediction, replacing noisy empirical estimates with semantically informed outputs. Across a diverse suite of few-shot classification and regression benchmarks, FORESTLLM achieves state-of-the-art performance.",
    "link": "http://arxiv.org/abs/2601.11311v1",
    "date": "2026-01-16",
    "fetched_at": "2026-01-19"
  },
  {
    "id": "http://arxiv.org/abs/2601.06022v1",
    "title": "AdaFuse: Adaptive Ensemble Decoding with Test-Time Scaling for LLMs",
    "authors": [
      "Chengming Cui",
      "Tianxin Wei",
      "Ziyi Chen",
      "Ruizhong Qiu",
      "Zhichen Zeng",
      "Zhining Liu",
      "Xuying Ning",
      "Duo Zhou",
      "Jingrui He"
    ],
    "summary": "Large language models (LLMs) exhibit complementary strengths arising from differences in pretraining data, model architectures, and decoding behaviors. Inference-time ensembling provides a practical way to combine these capabilities without retraining. However, existing ensemble approaches suffer from fundamental limitations. Most rely on fixed fusion granularity, which lacks the flexibility required for mid-generation adaptation and fails to adapt to different generation characteristics across tasks. To address these challenges, we propose AdaFuse, an adaptive ensemble decoding framework that dynamically selects semantically appropriate fusion units during generation. Rather than committing to a fixed granularity, AdaFuse adjusts fusion behavior on the fly based on the decoding context, with words serving as basic building blocks for alignment. To be specific, we introduce an uncertainty-based criterion to decide whether to apply ensembling at each decoding step. Under confident decoding states, the model continues generation directly. In less certain states, AdaFuse invokes a diversity-aware scaling strategy to explore alternative candidate continuations and inform ensemble decisions. This design establishes a synergistic interaction between adaptive ensembling and test-time scaling, where ensemble decisions guide targeted exploration, and the resulting diversity in turn strengthens ensemble quality. Experiments on open-domain question answering, arithmetic reasoning, and machine translation demonstrate that AdaFuse consistently outperforms strong ensemble baselines, achieving an average relative improvement of 6.88%. The code is available at https://github.com/CCM0111/AdaFuse.",
    "link": "http://arxiv.org/abs/2601.06022v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.06021v1",
    "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
    "authors": [
      "Jiajie Zhang",
      "Xin Lv",
      "Ling Feng",
      "Lei Hou",
      "Juanzi Li"
    ],
    "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose \\textbf{Citation-aware Rubric Rewards (CaRR)}, a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce \\textbf{Citation-aware Group Relative Policy Optimization (C-GRPO)}, which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
    "link": "http://arxiv.org/abs/2601.06021v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.06007v1",
    "title": "Don't Break the Cache: An Evaluation of Prompt Caching for Long-Horizon Agentic Tasks",
    "authors": [
      "Elias Lumer",
      "Faheem Nizar",
      "Akshaya Jangiti",
      "Kevin Frank",
      "Anmol Gulati",
      "Mandar Phadate",
      "Vamse Kumar Subbiah"
    ],
    "summary": "Recent advancements in Large Language Model (LLM) agents have enabled complex multi-turn agentic tasks requiring extensive tool calling, where conversations can span dozens of API calls with increasingly large context windows. However, although major LLM providers offer prompt caching to reduce cost and latency, its benefits for agentic workloads remain underexplored in the research literature. To our knowledge, no prior work quantifies these cost savings or compares caching strategies for multi-turn agentic tasks. We present a comprehensive evaluation of prompt caching across three major LLM providers (OpenAI, Anthropic, and Google) and compare three caching strategies, including full context caching, system prompt only caching, and caching that excludes dynamic tool results. We evaluate on DeepResearchBench, a multi-turn agentic benchmark where agents autonomously execute real-world web search tool calls to answer complex research questions, measuring both API cost and time to first token (TTFT) across over 500 agent sessions with 10,000-token system prompts. Our results demonstrate that prompt caching reduces API costs by 45-80% and improves time to first token by 13-31% across providers. We find that strategic prompt cache block control, such as placing dynamic content at the end of the system prompt, avoiding dynamic traditional function calling, and excluding dynamic tool results, provides more consistent benefits than naive full-context caching, which can paradoxically increase latency. Our analysis reveals nuanced variations in caching behavior across providers, and we provide practical guidance for implementing prompt caching in production agentic systems.",
    "link": "http://arxiv.org/abs/2601.06007v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.06002v1",
    "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
    "authors": [
      "Qiguang Chen",
      "Yantao Du",
      "Ziniu Li",
      "Jinhao Liu",
      "Songyao Duan",
      "Jiarui Guo",
      "Minghao Liu",
      "Jiaheng Liu",
      "Tong Yang",
      "Ge Zhang",
      "Libo Qin",
      "Wanxiang Che",
      "Wenhao Huang"
    ],
    "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
    "link": "http://arxiv.org/abs/2601.06002v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05991v1",
    "title": "Open-Vocabulary 3D Instruction Ambiguity Detection",
    "authors": [
      "Jiayu Ding",
      "Haoran Tang",
      "Ge Li"
    ],
    "summary": "In safety-critical domains, linguistic ambiguity can have severe consequences; a vague command like \"Pass me the vial\" in a surgical setting could lead to catastrophic errors. Yet, most embodied AI research overlooks this, assuming instructions are clear and focusing on execution rather than confirmation. To address this critical safety gap, we are the first to define Open-Vocabulary 3D Instruction Ambiguity Detection, a fundamental new task where a model must determine if a command has a single, unambiguous meaning within a given 3D scene. To support this research, we build Ambi3D, the large-scale benchmark for this task, featuring over 700 diverse 3D scenes and around 22k instructions. Our analysis reveals a surprising limitation: state-of-the-art 3D Large Language Models (LLMs) struggle to reliably determine if an instruction is ambiguous. To address this challenge, we propose AmbiVer, a two-stage framework that collects explicit visual evidence from multiple views and uses it to guide an vision-language model (VLM) in judging instruction ambiguity. Extensive experiments demonstrate the challenge of our task and the effectiveness of AmbiVer, paving the way for safer and more trustworthy embodied AI. Code and dataset available at https://jiayuding031020.github.io/ambi3d/.",
    "link": "http://arxiv.org/abs/2601.05991v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05960v1",
    "title": "Distilling Feedback into Memory-as-a-Tool",
    "authors": [
      "V\u00edctor Gallego"
    ],
    "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.",
    "link": "http://arxiv.org/abs/2601.05960v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05943v1",
    "title": "Global Optimization for Combinatorial Geometry Problems",
    "authors": [
      "Timo Berthold",
      "Dominik Kamp",
      "Gioni Mexi",
      "Sebastian Pokutta",
      "Imre P\u00f3lik"
    ],
    "summary": "Recent progress in LLM-driven algorithm discovery, exemplified by DeepMind's AlphaEvolve, has produced new best-known solutions for a range of hard geometric and combinatorial problems. This raises a natural question: to what extent can modern off-the-shelf global optimization solvers match such results when the problems are formulated directly as nonlinear optimization problems (NLPs)?   We revisit a subset of problems from the AlphaEvolve benchmark suite and evaluate straightforward NLP formulations with two state-of-the-art solvers, the commercial FICO Xpress and the open-source SCIP. Without any solver modifications, both solvers reproduce, and in several cases improve upon, the best solutions previously reported in the literature, including the recent LLM-driven discoveries. Our results not only highlight the maturity of generic NLP technology and its ability to tackle nonlinear mathematical problems that were out of reach for general-purpose solvers only a decade ago, but also position global NLP solvers as powerful tools that may be exploited within LLM-driven algorithm discovery.",
    "link": "http://arxiv.org/abs/2601.05943v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05930v1",
    "title": "Can We Predict Before Executing Machine Learning Agents?",
    "authors": [
      "Jingsheng Zheng",
      "Jintian Zhang",
      "Yujie Luo",
      "Yuren Mao",
      "Yunjun Gao",
      "Lun Du",
      "Huajun Chen",
      "Ningyu Zhang"
    ],
    "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
    "link": "http://arxiv.org/abs/2601.05930v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05918v1",
    "title": "Agentic LLMs as Powerful Deanonymizers: Re-identification of Participants in the Anthropic Interviewer Dataset",
    "authors": [
      "Tianshi Li"
    ],
    "summary": "On December 4, 2025, Anthropic released Anthropic Interviewer, an AI tool for running qualitative interviews at scale, along with a public dataset of 1,250 interviews with professionals, including 125 scientists, about their use of AI for research. Focusing on the scientist subset, I show that widely available LLMs with web search and agentic capabilities can link six out of twenty-four interviews to specific scientific works, recovering associated authors and, in some cases, uniquely identifying the interviewees. My contribution is to show that modern LLM-based agents make such re-identification attacks easy and low-effort: off-the-shelf tools can, with a few natural-language prompts, search the web, cross-reference details, and propose likely matches, effectively lowering the technical barrier. Existing safeguards can be bypassed by breaking down the re-identification into benign tasks. I outline the attack at a high level, discuss implications for releasing rich qualitative data in the age of LLM agents, and propose mitigation recommendations and open problems. I have notified Anthropic of my findings.",
    "link": "http://arxiv.org/abs/2601.05918v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05905v1",
    "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
    "authors": [
      "Haoming Xu",
      "Ningyuan Zhao",
      "Yunzhi Yao",
      "Weihong Xu",
      "Hongru Wang",
      "Xinle Deng",
      "Shumin Deng",
      "Jeff Z. Pan",
      "Huajun Chen",
      "Ningyu Zhang"
    ],
    "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
    "link": "http://arxiv.org/abs/2601.05905v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05904v1",
    "title": "Can AI mediation improve democratic deliberation?",
    "authors": [
      "Michael Henry Tessler",
      "Georgina Evans",
      "Michiel A. Bakker",
      "Iason Gabriel",
      "Sophie Bridgers",
      "Rishub Jain",
      "Raphael Koster",
      "Verena Rieser",
      "Anca Dragan",
      "Matthew Botvinick",
      "Christopher Summerfield"
    ],
    "summary": "The strength of democracy lies in the free and equal exchange of diverse viewpoints. Living up to this ideal at scale faces inherent tensions: broad participation, meaningful deliberation, and political equality often trade off with one another (Fishkin, 2011). We ask whether and how artificial intelligence (AI) could help navigate this \"trilemma\" by engaging with a recent example of a large language model (LLM)-based system designed to help people with diverse viewpoints find common ground (Tessler, Bakker, et al., 2024). Here, we explore the implications of the introduction of LLMs into deliberation augmentation tools, examining their potential to enhance participation through scalability, improve political equality via fair mediation, and foster meaningful deliberation by, for example, surfacing trustworthy information. We also point to key challenges that remain. Ultimately, a range of empirical, technical, and theoretical advancements are needed to fully realize the promise of AI-mediated deliberation for enhancing citizen engagement and strengthening democratic deliberation.",
    "link": "http://arxiv.org/abs/2601.05904v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05903v1",
    "title": "HAPS: Hierarchical LLM Routing with Joint Architecture and Parameter Search",
    "authors": [
      "Zihang Tian",
      "Rui Li",
      "Jingsen Zhang",
      "Xiaohe Bo",
      "Wei Huo",
      "Xu Chen"
    ],
    "summary": "Large language model (LLM) routing aims to exploit the specialized strengths of different LLMs for diverse tasks. However, existing approaches typically focus on selecting LLM architectures while overlooking parameter settings, which are critical for task performance. In this paper, we introduce HAPS, a hierarchical LLM routing framework that jointly searches over model architectures and parameters. Specifically, we use a high-level router to select among candidate LLM architectures, and then search for the optimal parameters for the selected architectures based on a low-level router. We design a parameter generation network to share parameters between the two routers to mutually enhance their capabilities. In the training process, we design a reward-augmented objective to effectively optimize our framework. Experiments on two commonly used benchmarks show that HAPS consistently outperforms strong routing baselines. We have released our code at https://github.com/zihangtian/HAPS.",
    "link": "http://arxiv.org/abs/2601.05903v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05899v1",
    "title": "TowerMind: A Tower Defence Game Learning Environment and Benchmark for LLM as Agents",
    "authors": [
      "Dawei Wang",
      "Chengming Zhou",
      "Di Zhao",
      "Xinyuan Liu",
      "Marci Chi Ma",
      "Gary Ushaw",
      "Richard Davison"
    ],
    "summary": "Recent breakthroughs in Large Language Models (LLMs) have positioned them as a promising paradigm for agents, with long-term planning and decision-making emerging as core general-purpose capabilities for adapting to diverse scenarios and tasks. Real-time strategy (RTS) games serve as an ideal testbed for evaluating these two capabilities, as their inherent gameplay requires both macro-level strategic planning and micro-level tactical adaptation and action execution. Existing RTS game-based environments either suffer from relatively high computational demands or lack support for textual observations, which has constrained the use of RTS games for LLM evaluation. Motivated by this, we present TowerMind, a novel environment grounded in the tower defense (TD) subgenre of RTS games. TowerMind preserves the key evaluation strengths of RTS games for assessing LLMs, while featuring low computational demands and a multimodal observation space, including pixel-based, textual, and structured game-state representations. In addition, TowerMind supports the evaluation of model hallucination and provides a high degree of customizability. We design five benchmark levels to evaluate several widely used LLMs under different multimodal input settings. The results reveal a clear performance gap between LLMs and human experts across both capability and hallucination dimensions. The experiments further highlight key limitations in LLM behavior, such as inadequate planning validation, a lack of multifinality in decision-making, and inefficient action use. We also evaluate two classic reinforcement learning algorithms: Ape-X DQN and PPO. By offering a lightweight and multimodal design, TowerMind complements the existing RTS game-based environment landscape and introduces a new benchmark for the AI agent field. The source code is publicly available on GitHub(https://github.com/tb6147877/TowerMind).",
    "link": "http://arxiv.org/abs/2601.05899v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05887v1",
    "title": "Cybersecurity AI: A Game-Theoretic AI for Guiding Attack and Defense",
    "authors": [
      "V\u00edctor Mayoral-Vilches",
      "Mar\u00eda Sanz-G\u00f3mez",
      "Francesco Balassone",
      "Stefan Rass",
      "Lidia Salas-Espejo",
      "Benjamin Jablonski",
      "Luis Javier Navarrete-Lozano",
      "Maite del Mundo de Torres",
      "Crist\u00f3bal R. J. Veas Chavez"
    ],
    "summary": "AI-driven penetration testing now executes thousands of actions per hour but still lacks the strategic intuition humans apply in competitive security. To build cybersecurity superintelligence --Cybersecurity AI exceeding best human capability-such strategic intuition must be embedded into agentic reasoning processes. We present Generative Cut-the-Rope (G-CTR), a game-theoretic guidance layer that extracts attack graphs from agent's context, computes Nash equilibria with effort-aware scoring, and feeds a concise digest back into the LLM loop \\emph{guiding} the agent's actions. Across five real-world exercises, G-CTR matches 70--90% of expert graph structure while running 60--245x faster and over 140x cheaper than manual analysis. In a 44-run cyber-range, adding the digest lifts success from 20.0% to 42.9%, cuts cost-per-success by 2.7x, and reduces behavioral variance by 5.2x. In Attack-and-Defense exercises, a shared digest produces the Purple agent, winning roughly 2:1 over the LLM-only baseline and 3.7:1 over independently guided teams. This closed-loop guidance is what produces the breakthrough: it reduces ambiguity, collapses the LLM's search space, suppresses hallucinations, and keeps the model anchored to the most relevant parts of the problem, yielding large gains in success rate, consistency, and reliability.",
    "link": "http://arxiv.org/abs/2601.05887v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05879v1",
    "title": "Gender Bias in LLMs: Preliminary Evidence from Shared Parenting Scenario in Czech Family Law",
    "authors": [
      "Jakub Harasta",
      "Matej Vasina",
      "Martin Kornel",
      "Tomas Foltynek"
    ],
    "summary": "Access to justice remains limited for many people, leading laypersons to increasingly rely on Large Language Models (LLMs) for legal self-help. Laypeople use these tools intuitively, which may lead them to form expectations based on incomplete, incorrect, or biased outputs. This study examines whether leading LLMs exhibit gender bias in their responses to a realistic family law scenario. We present an expert-designed divorce scenario grounded in Czech family law and evaluate four state-of-the-art LLMs GPT-5 nano, Claude Haiku 4.5, Gemini 2.5 Flash, and Llama 3.3 in a fully zero-shot interaction. We deploy two versions of the scenario, one with gendered names and one with neutral labels, to establish a baseline for comparison. We further introduce nine legally relevant factors that vary the factual circumstances of the case and test whether these variations influence the models' proposed shared-parenting ratios. Our preliminary results highlight differences across models and suggest gender-dependent patterns in the outcomes generated by some systems. The findings underscore both the risks associated with laypeople's reliance on LLMs for legal guidance and the need for more robust evaluation of model behavior in sensitive legal contexts. We present exploratory and descriptive evidence intended to identify systematic asymmetries rather than to establish causal effects.",
    "link": "http://arxiv.org/abs/2601.05879v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05874v1",
    "title": "Continual-learning for Modelling Low-Resource Languages from Large Language Models",
    "authors": [
      "Santosh Srinath K",
      "Mudit Somani",
      "Varun Reddy Padala",
      "Prajna Devi Upadhyay",
      "Abhijit Das"
    ],
    "summary": "Modelling a language model for a multi-lingual scenario includes several potential challenges, among which catastrophic forgetting is the major challenge. For example, small language models (SLM) built for low-resource languages by adapting large language models (LLMs) pose the challenge of catastrophic forgetting. This work proposes to employ a continual learning strategy using parts-of-speech (POS)-based code-switching along with a replay adapter strategy to mitigate the identified gap of catastrophic forgetting while training SLM from LLM. Experiments conducted on vision language tasks such as visual question answering and language modelling task exhibits the success of the proposed architecture.",
    "link": "http://arxiv.org/abs/2601.05874v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05870v1",
    "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
    "authors": [
      "Huilin Deng",
      "Hongchen Luo",
      "Yue Zhu",
      "Long Li",
      "Zhuoyue Chen",
      "Xinghao Zhao",
      "Ming Li",
      "Jihai Zhang",
      "Mengchang Wang",
      "Yang Cao",
      "Yu Kang"
    ],
    "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
    "link": "http://arxiv.org/abs/2601.05870v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05858v1",
    "title": "CLewR: Curriculum Learning with Restarts for Machine Translation Preference Learning",
    "authors": [
      "Alexandra Dragomir",
      "Florin Brad",
      "Radu Tudor Ionescu"
    ],
    "summary": "Large language models (LLMs) have demonstrated competitive performance in zero-shot multilingual machine translation (MT). Some follow-up works further improved MT performance via preference optimization, but they leave a key aspect largely underexplored: the order in which data samples are given during training. We address this topic by integrating curriculum learning into various state-of-the-art preference optimization algorithms to boost MT performance. We introduce a novel curriculum learning strategy with restarts (CLewR), which reiterates easy-to-hard curriculum multiple times during training to effectively mitigate the catastrophic forgetting of easy examples. We demonstrate consistent gains across several model families (Gemma2, Qwen2.5, Llama3.1) and preference optimization techniques. We publicly release our code at https://github.com/alexandra-dragomir/CLewR.",
    "link": "http://arxiv.org/abs/2601.05858v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05835v1",
    "title": "Left, Right, or Center? Evaluating LLM Framing in News Classification and Generation",
    "authors": [
      "Molly Kennedy",
      "Ali Parker",
      "Yihong Liu",
      "Hinrich Sch\u00fctze"
    ],
    "summary": "Large Language Model (LLM) based summarization and text generation are increasingly used for producing and rewriting text, raising concerns about political framing in journalism where subtle wording choices can shape interpretation. Across nine state-of-the-art LLMs, we study political framing by testing whether LLMs' classification-based bias signals align with framing behavior in their generated summaries. We first compare few-shot ideology predictions against LEFT/CENTER/RIGHT labels. We then generate \"steered\" summaries under FAITHFUL, CENTRIST, LEFT, and RIGHT prompts, and score all outputs using a single fixed ideology evaluator. We find pervasive ideological center-collapse in both article-level ratings and generated text, indicating a systematic tendency toward centrist framing. Among evaluated models, Grok 4 is by far the most ideologically expressive generator, while Claude Sonnet 4.5 and Llama 3.1 achieve the strongest bias-rating performance among commercial and open-weight models, respectively.",
    "link": "http://arxiv.org/abs/2601.05835v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05827v1",
    "title": "SSR: Safeguarding Staking Rewards by Defining and Detecting Logical Defects in DeFi Staking",
    "authors": [
      "Zewei Lin",
      "Jiachi Chen",
      "Jingwen Zhang",
      "Zexu Wang",
      "Yuming Feng",
      "Weizhe Zhang",
      "Zibin Zheng"
    ],
    "summary": "Decentralized Finance (DeFi) staking is one of the most prominent applications within the DeFi ecosystem, where DeFi projects enable users to stake tokens on the platform and reward participants with additional tokens. However, logical defects in DeFi staking could enable attackers to claim unwarranted rewards by manipulating reward amounts, repeatedly claiming rewards, or engaging in other malicious actions. To mitigate these threats, we conducted the first study focused on defining and detecting logical defects in DeFi staking. Through the analysis of 64 security incidents and 144 audit reports, we identified six distinct types of logical defects, each accompanied by detailed descriptions and code examples. Building on this empirical research, we developed SSR (Safeguarding Staking Reward), a static analysis tool designed to detect logical defects in DeFi staking contracts. SSR utilizes a large language model (LLM) to extract fundamental information about staking logic and constructs a DeFi staking model. It then identifies logical defects by analyzing the model and the associated semantic features. We constructed a ground truth dataset based on known security incidents and audit reports to evaluate the effectiveness of SSR. The results indicate that SSR achieves an overall precision of 92.31%, a recall of 87.92%, and an F1-score of 88.85%. Additionally, to assess the prevalence of logical defects in real-world smart contracts, we compiled a large-scale dataset of 15,992 DeFi staking contracts. SSR detected that 3,557 (22.24%) of these contracts contained at least one logical defect.",
    "link": "http://arxiv.org/abs/2601.05827v1",
    "date": "2026-01-09",
    "fetched_at": "2026-01-12"
  },
  {
    "id": "http://arxiv.org/abs/2601.05215v1",
    "title": "MineNPC-Task: Task Suite for Memory-Aware Minecraft Agents",
    "authors": [
      "Tamil Sudaravan Mohan Doss",
      "Michael Xu",
      "Sudha Rao",
      "Andrew D. Wilson",
      "Balasaravanan Thoravi Kumaravel"
    ],
    "summary": "We present \\textsc{MineNPC-Task}, a user-authored benchmark and evaluation harness for testing memory-aware, mixed-initiative LLM agents in open-world \\emph{Minecraft}. Rather than relying on synthetic prompts, tasks are elicited from formative and summative co-play with expert players, normalized into parametric templates with explicit preconditions and dependency structure, and paired with machine-checkable validators under a bounded-knowledge policy that forbids out-of-world shortcuts. The harness captures plan/act/memory events-including plan previews, targeted clarifications, memory reads and writes, precondition checks, and repair attempts and reports outcomes relative to the total number of attempted subtasks, derived from in-world evidence.   As an initial snapshot, we instantiate the framework with GPT-4o and evaluate \\textbf{216} subtasks across \\textbf{8} experienced players. We observe recurring breakdown patterns in code execution, inventory/tool handling, referencing, and navigation, alongside recoveries supported by mixed-initiative clarifications and lightweight memory. Participants rated interaction quality and interface usability positively, while highlighting the need for stronger memory persistence across tasks. We release the complete task suite, validators, logs, and harness to support transparent, reproducible evaluation of future memory-aware embodied agents.",
    "link": "http://arxiv.org/abs/2601.05215v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05214v1",
    "title": "Internal Representations as Indicators of Hallucinations in Agent Tool Selection",
    "authors": [
      "Kait Healy",
      "Bharathi Srinivasan",
      "Visakh Madathil",
      "Jing Wu"
    ],
    "summary": "Large Language Models (LLMs) have shown remarkable capabilities in tool calling and tool usage, but suffer from hallucinations where they choose incorrect tools, provide malformed parameters and exhibit 'tool bypass' behavior by performing simulations and generating outputs instead of invoking specialized tools or external systems. This undermines the reliability of LLM based agents in production systems as it leads to inconsistent results, and bypasses security and audit controls. Such hallucinations in agent tool selection require early detection and error handling. Unlike existing hallucination detection methods that require multiple forward passes or external validation, we present a computationally efficient framework that detects tool-calling hallucinations in real-time by leveraging LLMs' internal representations during the same forward pass used for generation. We evaluate this approach on reasoning tasks across multiple domains, demonstrating strong detection performance (up to 86.4\\% accuracy) while maintaining real-time inference capabilities with minimal computational overhead, particularly excelling at detecting parameter-level hallucinations and inappropriate tool selections, critical for reliable agent deployment.",
    "link": "http://arxiv.org/abs/2601.05214v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05192v1",
    "title": "LELA: an LLM-based Entity Linking Approach with Zero-Shot Domain Adaptation",
    "authors": [
      "Samy Haffoudhi",
      "Fabian M. Suchanek",
      "Nils Holzenberger"
    ],
    "summary": "Entity linking (mapping ambiguous mentions in text to entities in a knowledge base) is a foundational step in tasks such as knowledge graph construction, question-answering, and information extraction. Our method, LELA, is a modular coarse-to-fine approach that leverages the capabilities of large language models (LLMs), and works with different target domains, knowledge bases and LLMs, without any fine-tuning phase. Our experiments across various entity linking settings show that LELA is highly competitive with fine-tuned approaches, and substantially outperforms the non-fine-tuned ones.",
    "link": "http://arxiv.org/abs/2601.05192v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05187v1",
    "title": "SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning",
    "authors": [
      "Yanchang Liang",
      "Xiaowei Zhao"
    ],
    "summary": "Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.",
    "link": "http://arxiv.org/abs/2601.05187v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05184v1",
    "title": "Observations and Remedies for Large Language Model Bias in Self-Consuming Performative Loop",
    "authors": [
      "Yaxuan Wang",
      "Zhongteng Cai",
      "Yujia Bao",
      "Xueru Zhang",
      "Yang Liu"
    ],
    "summary": "The rapid advancement of large language models (LLMs) has led to growing interest in using synthetic data to train future models. However, this creates a self-consuming retraining loop, where models are trained on their own outputs and may cause performance drops and induce emerging biases. In real-world applications, previously deployed LLMs may influence the data they generate, leading to a dynamic system driven by user feedback. For example, if a model continues to underserve users from a group, less query data will be collected from this particular demographic of users. In this study, we introduce the concept of \\textbf{S}elf-\\textbf{C}onsuming \\textbf{P}erformative \\textbf{L}oop (\\textbf{SCPL}) and investigate the role of synthetic data in shaping bias during these dynamic iterative training processes under controlled performative feedback. This controlled setting is motivated by the inaccessibility of real-world user preference data from dynamic production systems, and enables us to isolate and analyze feedback-driven bias evolution in a principled manner. We focus on two types of loops, including the typical retraining setting and the incremental fine-tuning setting, which is largely underexplored. Through experiments on three real-world tasks, we find that the performative loop increases preference bias and decreases disparate bias. We design a reward-based rejection sampling strategy to mitigate the bias, moving towards more trustworthy self-improving systems.",
    "link": "http://arxiv.org/abs/2601.05184v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05172v1",
    "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
    "authors": [
      "Haoyu Zhao",
      "Akide Liu",
      "Zeyu Zhang",
      "Weijie Wang",
      "Feng Chen",
      "Ruihan Zhu",
      "Gholamreza Haffari",
      "Bohan Zhuang"
    ],
    "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.   We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
    "link": "http://arxiv.org/abs/2601.05172v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05170v1",
    "title": "Reverse-engineering NLI: A study of the meta-inferential properties of Natural Language Inference",
    "authors": [
      "Rasmus Blanck",
      "Bill Noble",
      "Stergios Chatzikyriakidis"
    ],
    "summary": "Natural Language Inference (NLI) has been an important task for evaluating language models for Natural Language Understanding, but the logical properties of the task are poorly understood and often mischaracterized. Understanding the notion of inference captured by NLI is key to interpreting model performance on the task. In this paper we formulate three possible readings of the NLI label set and perform a comprehensive analysis of the meta-inferential properties they entail. Focusing on the SNLI dataset, we exploit (1) NLI items with shared premises and (2) items generated by LLMs to evaluate models trained on SNLI for meta-inferential consistency and derive insights into which reading of the logical relations is encoded by the dataset.",
    "link": "http://arxiv.org/abs/2601.05170v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05167v1",
    "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
    "authors": [
      "Chengsong Huang",
      "Tong Zheng",
      "Langlin Huang",
      "Jinyuan Li",
      "Haolin Liu",
      "Jiaxin Huang"
    ],
    "summary": "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.",
    "link": "http://arxiv.org/abs/2601.05167v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05162v1",
    "title": "GenAI-DrawIO-Creator: A Framework for Automated Diagram Generation",
    "authors": [
      "Jinze Yu",
      "Dayuan Jiang"
    ],
    "summary": "Diagrams are crucial for communicating complex information, yet creating and modifying them remains a labor-intensive task. We present GenAI-DrawIO-Creator, a novel framework that leverages Large Language Models (LLMs) to automate diagram generation and manipulation in the structured XML format used by draw.io. Our system integrates Claude 3.7 to reason about structured visual data and produce valid diagram representations. Key contributions include a high-level system design enabling real-time diagram updates, specialized prompt engineering and error-checking to ensure well-formed XML outputs. We demonstrate a working prototype capable of generating accurate diagrams (such as network architectures and flowcharts) from natural language or code, and even replicating diagrams from images. Simulated evaluations show that our approach significantly reduces diagram creation time and produces outputs with high structural fidelity. Our results highlight the promise of Claude 3.7 in handling structured visual reasoning tasks and lay the groundwork for future research in AI-assisted diagramming applications.",
    "link": "http://arxiv.org/abs/2601.05162v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05144v1",
    "title": "Distilling the Thought, Watermarking the Answer: A Principle Semantic Guided Watermark for Large Reasoning Models",
    "authors": [
      "Shuliang Liu",
      "Xingyu Li",
      "Hongyi Liu",
      "Yibo Yan",
      "Bingchen Duan",
      "Qi Zheng",
      "Dong Fang",
      "Lingfeng Su",
      "Xuming Hu"
    ],
    "summary": "Reasoning Large Language Models (RLLMs) excelling in complex tasks present unique challenges for digital watermarking, as existing methods often disrupt logical coherence or incur high computational costs. Token-based watermarking techniques can corrupt the reasoning flow by applying pseudo-random biases, while semantic-aware approaches improve quality but introduce significant latency or require auxiliary models. This paper introduces ReasonMark, a novel watermarking framework specifically designed for reasoning-intensive LLMs. Our approach decouples generation into an undisturbed Thinking Phase and a watermarked Answering Phase. We propose a Criticality Score to identify semantically pivotal tokens from the reasoning trace, which are distilled into a Principal Semantic Vector (PSV). The PSV then guides a semantically-adaptive mechanism that modulates watermark strength based on token-PSV alignment, ensuring robustness without compromising logical integrity. Extensive experiments show ReasonMark surpasses state-of-the-art methods by reducing text Perplexity by 0.35, increasing translation BLEU score by 0.164, and raising mathematical accuracy by 0.67 points. These advancements are achieved alongside a 0.34% higher watermark detection AUC and stronger robustness to attacks, all with a negligible increase in latency. This work enables the traceable and trustworthy deployment of reasoning LLMs in real-world applications.",
    "link": "http://arxiv.org/abs/2601.05144v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05114v1",
    "title": "Evaluative Fingerprints: Stable and Systematic Differences in LLM Evaluator Behavior",
    "authors": [
      "Wajid Nasser"
    ],
    "summary": "LLM-as-judge systems promise scalable, consistent evaluation. We find the opposite: judges are consistent, but not with each other; they are consistent with themselves. Across 3,240 evaluations (9 judges x 120 unique video x pack items x 3 independent runs), inter-judge agreement is near-zero (Krippendorff's \u03b1 = 0.042). On two dimensions, judges disagree more than random noise would predict (\u03b1 < 0). Yet this disagreement isn't chaos; it's structured. A classifier identifies which judge produced an evaluation with 77.1% accuracy from rubric scores alone, rising to 89.9% with disposition features. Within model families, the signal is even stronger: GPT-4.1 and GPT-5.2 are distinguishable with 99.6% accuracy. We call this the reliability paradox: judges cannot agree on what constitutes quality, yet their disagreement patterns are so stable they function as fingerprints. Each judge implements a distinct, stable theory of quality: an \"evaluative disposition\" that shapes how it interprets any rubric. We characterize these dispositions along multiple axes: harshness/leniency, dimension emphasis, within-judge stability (ICC), and evidence behavior (receipt validity, semantic linkage via NLI, and shotgun index). The implication is stark: LLM judges are not interchangeable instruments measuring a shared construct. They are distinct measurement devices, each encoding its own implicit theory of quality. Averaging their scores produces a synthetic verdict that corresponds to no judge's actual values.",
    "link": "http://arxiv.org/abs/2601.05114v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05111v1",
    "title": "Agent-as-a-Judge",
    "authors": [
      "Runyang You",
      "Hongru Cai",
      "Caiqi Zhang",
      "Qiancheng Xu",
      "Meng Liu",
      "Tiezheng Yu",
      "Yongqi Li",
      "Wenjie Li"
    ],
    "summary": "LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.",
    "link": "http://arxiv.org/abs/2601.05111v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05109v1",
    "title": "Nalar: An agent serving framework",
    "authors": [
      "Marco Laju",
      "Donghyun Son",
      "Saurabh Agarwal",
      "Nitin Kedia",
      "Myungjin Lee",
      "Jayanth Srinivasa",
      "Aditya Akella"
    ],
    "summary": "LLM-driven agentic applications increasingly automate complex, multi-step tasks, but serving them efficiently remains challenging due to heterogeneous components, dynamic and model-driven control flow, long-running state, and unpredictable latencies. Nalar is a ground-up agent-serving framework that cleanly separates workflow specification from execution while providing the runtime visibility and control needed for robust performance. Nalar preserves full Python expressiveness, using lightweight auto-generated stubs that turn agent and tool invocations into futures carrying dependency and context metadata. A managed state layer decouples logical state from physical placement, enabling safe reuse, migration, and consistent retry behavior. A two-level control architecture combines global policy computation with local event-driven enforcement to support adaptive routing, scheduling, and resource management across evolving workflows. Together, these mechanisms allow Nalar to deliver scalable, efficient, and policy-driven serving of heterogeneous agentic applications without burdening developers with orchestration logic. Across three agentic workloads, Nalar cuts tail latency by 34--74\\%, achieves up to $2.9\\times$ speedups, sustains 80 RPS where baselines fail, and scales to 130K futures with sub-500 ms control overhead.",
    "link": "http://arxiv.org/abs/2601.05109v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05107v1",
    "title": "Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction",
    "authors": [
      "Muzhao Tian",
      "Zisu Huang",
      "Xiaohua Wang",
      "Jingwen Xu",
      "Zhengkang Guo",
      "Qi Qian",
      "Yuanzhe Shen",
      "Kaitao Song",
      "Jiakang Yuan",
      "Changze Lv",
      "Xiaoqing Zheng"
    ],
    "summary": "As LLM-based agents are increasingly used in long-term interactions, cumulative memory is critical for enabling personalization and maintaining stylistic consistency. However, most existing systems adopt an ``all-or-nothing'' approach to memory usage: incorporating all relevant past information can lead to \\textit{Memory Anchoring}, where the agent is trapped by past interactions, while excluding memory entirely results in under-utilization and the loss of important interaction history. We show that an agent's reliance on memory can be modeled as an explicit and user-controllable dimension. We first introduce a behavioral metric of memory dependence to quantify the influence of past interactions on current outputs. We then propose \\textbf{Stee}rable \\textbf{M}emory Agent, \\texttt{SteeM}, a framework that allows users to dynamically regulate memory reliance, ranging from a fresh-start mode that promotes innovation to a high-fidelity mode that closely follows interaction history. Experiments across different scenarios demonstrate that our approach consistently outperforms conventional prompting and rigid memory masking strategies, yielding a more nuanced and effective control for personalized human-agent collaboration.",
    "link": "http://arxiv.org/abs/2601.05107v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05106v1",
    "title": "Token-Level LLM Collaboration via FusionRoute",
    "authors": [
      "Nuoya Xiong",
      "Yuhang Zhou",
      "Hanqing Zeng",
      "Zhaorun Chen",
      "Furong Huang",
      "Shuchao Bi",
      "Lizhu Zhang",
      "Zhuokai Zhao"
    ],
    "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
    "link": "http://arxiv.org/abs/2601.05106v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05103v1",
    "title": "Semantically Orthogonal Framework for Citation Classification: Disentangling Intent and Content",
    "authors": [
      "Changxu Duan",
      "Zhiyin Tan"
    ],
    "summary": "Understanding the role of citations is essential for research assessment and citation-aware digital libraries. However, existing citation classification frameworks often conflate citation intent (why a work is cited) with cited content type (what part is cited), limiting their effectiveness in auto classification due to a dilemma between fine-grained type distinctions and practical classification reliability. We introduce SOFT, a Semantically Orthogonal Framework with Two dimensions that explicitly separates citation intent from cited content type, drawing inspiration from semantic role theory. We systematically re-annotate the ACL-ARC dataset using SOFT and release a cross-disciplinary test set sampled from ACT2. Evaluation with both zero-shot and fine-tuned Large Language Models demonstrates that SOFT enables higher agreement between human annotators and LLMs, and supports stronger classification performance and robust cross-domain generalization compared to ACL-ARC and SciCite annotation frameworks. These results confirm SOFT's value as a clear, reusable annotation standard, improving clarity, consistency, and generalizability for digital libraries and scholarly communication infrastructures. All code and data are publicly available on GitHub https://github.com/zhiyintan/SOFT.",
    "link": "http://arxiv.org/abs/2601.05103v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05101v1",
    "title": "Arabic Prompts with English Tools: A Benchmark",
    "authors": [
      "Konstantin Kubrak",
      "Ahmed El-Moselhy",
      "Ammar Alsulami",
      "Remaz Altuwaim",
      "Hassan Ismail Fawaz",
      "Faisal Alsaby"
    ],
    "summary": "Large Language Models (LLMs) are now integral to numerous industries, increasingly serving as the core reasoning engine for autonomous agents that perform complex tasks through tool-use. While the development of Arabic-native LLMs is accelerating, the benchmarks for evaluating their capabilities lag behind, with most existing frameworks focusing on English. A critical and overlooked area is tool-calling, where the performance of models prompted in non-English languages like Arabic is poorly understood, especially since these models are often pretrained on predominantly English data. This paper addresses this critical gap by introducing the first dedicated benchmark for evaluating the tool-calling and agentic capabilities of LLMs in the Arabic language. Our work provides a standardized framework to measure the functional accuracy and robustness of models in Arabic agentic workflows. Our findings reveal a huge performance gap: when users interact in Arabic, tool-calling accuracy drops by an average of 5-10\\%, regardless of whether the tool descriptions themselves are in Arabic or English. By shedding light on these critical challenges, this benchmark aims to foster the development of more reliable and linguistically equitable AI agents for Arabic-speaking users.",
    "link": "http://arxiv.org/abs/2601.05101v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05075v1",
    "title": "SemPA: Improving Sentence Embeddings of Large Language Models through Semantic Preference Alignment",
    "authors": [
      "Ziyang Chen",
      "Zhenxuan Huang",
      "Yile Wang",
      "Weiqin Wang",
      "Lu Yin",
      "Hui Huang"
    ],
    "summary": "Traditional sentence embedding methods employ token-level contrastive learning on non-generative pre-trained models. Recently, there have emerged embedding methods based on generative large language models (LLMs). These methods either rely on fixed prompt templates or involve modifications to the model architecture. The former lacks further optimization of the model and results in limited performance, while the latter alters the internal computational mechanisms of the model, thereby compromising its generative capabilities. We propose SemPA, a novel approach that boosts the sentence representations while preserving the generative ability of LLMs via semantic preference alignment. We leverage sentence-level Direct Preference Optimization (DPO) to efficiently optimize LLMs on a paraphrase generation task, where the model learns to discriminate semantically equivalent sentences while preserving inherent generative capacity. Theoretically, we establish a formal connection between DPO and contrastive learning under the Plackett-Luce model framework. Empirically, experimental results on both semantic textual similarity tasks and various benchmarks for LLMs show that SemPA achieves better semantic representations without sacrificing the inherent generation capability of LLMs.",
    "link": "http://arxiv.org/abs/2601.05075v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05062v1",
    "title": "Compositional Steering of Large Language Models with Steering Tokens",
    "authors": [
      "Gorjan Radevski",
      "Kiril Gashteovski",
      "Giwon Hong",
      "Carolin Lawrence",
      "Goran Glava\u0161"
    ],
    "summary": "Deploying LLMs in real-world applications requires controllable output that satisfies multiple desiderata at the same time. While existing work extensively addresses LLM steering for a single behavior, \\textit{compositional steering} -- i.e., steering LLMs simultaneously towards multiple behaviors -- remains an underexplored problem. In this work, we propose \\emph{compositional steering tokens} for multi-behavior steering. We first embed individual behaviors, expressed as natural language instructions, into dedicated tokens via self-distillation. Contrary to most prior work, which operates in the activation space, our behavior steers live in the space of input tokens, enabling more effective zero-shot composition. We then train a dedicated \\textit{composition token} on pairs of behaviors and show that it successfully captures the notion of composition: it generalizes well to \\textit{unseen} compositions, including those with unseen behaviors as well as those with an unseen \\textit{number} of behaviors. Our experiments across different LLM architectures show that steering tokens lead to superior multi-behavior control compared to competing approaches (instructions, activation steering, and LoRA merging). Moreover, we show that steering tokens complement natural language instructions, with their combination resulting in further gains.",
    "link": "http://arxiv.org/abs/2601.05062v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  },
  {
    "id": "http://arxiv.org/abs/2601.05053v1",
    "title": "Reinforced Efficient Reasoning via Semantically Diverse Exploration",
    "authors": [
      "Ziqi Zhao",
      "Zhaochun Ren",
      "Jiahong Zou",
      "Liu Yang",
      "Zhiwei Xu",
      "Xuri Ge",
      "Zhumin Chen",
      "Xinyu Ma",
      "Daiting Shi",
      "Shuaiqiang Wang",
      "Dawei Yin",
      "Xin Xin"
    ],
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has proven effective in enhancing the reasoning of large language models (LLMs). Monte Carlo Tree Search (MCTS)-based extensions improve upon vanilla RLVR (e.g., GRPO) by providing tree-based reasoning rollouts that enable fine-grained and segment-level credit assignment. However, existing methods still suffer from limited exploration diversity and inefficient reasoning. To address the above challenges, we propose reinforced efficient reasoning via semantically diverse explorations, i.e., ROSE, for LLMs. To encourage more diverse reasoning exploration, our method incorporates a semantic-entropy-based branching strategy and an $\\varepsilon$-exploration mechanism. The former operates on already sampled reasoning rollouts to capture semantic uncertainty and select branching points with high semantic divergence to generate new successive reasoning paths, whereas the latter stochastically initiates reasoning rollouts from the root, preventing the search process from becoming overly local. To improve efficiency, we design a length-aware segment-level advantage estimator that rewards concise and correct reasoning while penalizing unnecessarily long reasoning chains. Extensive experiments on various mathematical reasoning benchmarks with Qwen and Llama models validate the effectiveness and efficiency of ROSE. Codes are available at https://github.com/ZiqiZhao1/ROSE-rl.",
    "link": "http://arxiv.org/abs/2601.05053v1",
    "date": "2026-01-08",
    "fetched_at": "2026-01-11"
  }
]